Log in to your Hugging Face account:
https://huggingface.co/login

Go to Settings â†’ Access Tokens or directly here:
https://huggingface.co/settings/tokens

Click "New token"

Name: Give it a name like chatty-ai-download

Role: Select Read (this is required to download models)

Leave scopes as default unless you're doing API dev

Click "Generate"

Youâ€™ll see a token that starts with hf_ â€” copy this immediately
(you wonâ€™t be able to see it again!)

ğŸ§ª Optional: Test the Token with curl
To make sure it works before using wget, test it like this:

bash
Copy
Edit
curl -L -H "Authorization: Bearer hf_your_token_here" \
https://huggingface.co/api/models/cmp-nct/tinyllama-gguf
If you get JSON metadata about the model, you're good to go âœ…
If you get a 401 error, the token is incorrect or lacks permission âŒ

ğŸš€ Now Download the Model
Once verified, run:

bash
Copy
Edit
wget --header="Authorization: Bearer hf_your_token_here" \
https://huggingface.co/cmp-nct/tinyllama-gguf/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
-O tinyllama.gguf
Let me know when the model finishes downloading and Iâ€™ll help you:

Add the local LLM container to docker-compose.yml

Connect it to Chatty using Wyoming LLM

Letâ€™s get Chatty working fully offline! ğŸ§ ğŸ”Š

