Log in to your Hugging Face account:
https://huggingface.co/login

Go to Settings → Access Tokens or directly here:
https://huggingface.co/settings/tokens

Click "New token"

Name: Give it a name like chatty-ai-download

Role: Select Read (this is required to download models)

Leave scopes as default unless you're doing API dev

Click "Generate"

You’ll see a token that starts with hf_ — copy this immediately
(you won’t be able to see it again!)

🧪 Optional: Test the Token with curl
To make sure it works before using wget, test it like this:

bash
Copy
Edit
curl -L -H "Authorization: Bearer hf_your_token_here" \
https://huggingface.co/api/models/cmp-nct/tinyllama-gguf
If you get JSON metadata about the model, you're good to go ✅
If you get a 401 error, the token is incorrect or lacks permission ❌

🚀 Now Download the Model
Once verified, run:

bash
Copy
Edit
wget --header="Authorization: Bearer hf_your_token_here" \
https://huggingface.co/cmp-nct/tinyllama-gguf/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
-O tinyllama.gguf
Let me know when the model finishes downloading and I’ll help you:

Add the local LLM container to docker-compose.yml

Connect it to Chatty using Wyoming LLM

Let’s get Chatty working fully offline! 🧠🔊

