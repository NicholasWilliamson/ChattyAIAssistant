1. Phi-2 Model Download Fails Midway
You're seeing a 403 Forbidden after a long download. This happens because Hugging Face presigned URLs expire after ~1 hour, which breaks large downloads on slower networks like Raspberry Pi Wi-Fi.

âœ… Fix:
Instead of using wget, use aria2c which supports auto-resume and parallel segments:

ðŸ”§ Install aria2:
bash
Copy
Edit
sudo apt update
sudo apt install aria2
ðŸš€ Re-download phi-2 with resume:
bash
Copy
Edit
aria2c -c -x 4 -s 4 https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf -o phi-2.Q4_K_M.gguf
-c: Continue if file exists

-x 4: 4 concurrent connections

-s 4: 4 segments

This is much more reliable than wget for large GGUF models.

âœ… 2. TinyMistral Download Asking for Login
You tried:

bash
Copy
Edit
wget https://huggingface.co/TheBloke/TinyMistral-248M-Chat-GGUF/resolve/main/TinyMistral-248M-Chat
But thatâ€™s not a complete file name â€” Hugging Face interpreted it as private content.

âœ… Fix:
Hereâ€™s the full correct GGUF filename:


aria2c -c -x 4 -s 4 https://huggingface.co/TheBloke/TinyMistral-248M-Chat-GGUF/resolve/main/TinyMistral-248M-Chat.Q4_K_M.gguf -o TinyMistral-248M-Chat.Q4_K_M.gguf
This file is public and requires no login, as long as the filename is fully correct.

âœ… After Downloading
Once the .gguf file is in place, update models.yaml like this:

- name: tinymistral
  model: TinyMistral-248M-Chat.Q4_K_M.gguf
  backend: llama-cpp
  parameters:
    temperature: 0.7
    top_p: 0.9
    threads: 4
    stopwords: ["</s>"]
  template:
    chat: mistral-instruct
Then restart:

sudo docker compose down
sudo docker compose up -d
âœ… Test the Assistant

curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinymistral",
    "messages": [
      {"role": "system", "content": "You are a friendly Australian voice assistant."},
      {"role": "user", "content": "Hey Chatty, what can you help with today?"}
    ]
  }' | jq '.choices[0].message.content'
Would you like me to regenerate the full models.yaml with both TinyMistral and Phi-2 loaded â€” or just preload the fastest one (TinyMistral)?