Thank you ChatGPT. You are great.

I ran nickspi5@raspberrypi1:~/ha-voice $ sudo docker exec -it localai ls -lh /models | grep tinyllama
-rw-r--r-- 1 1000 1000  225 Jun 28 23:49 modelstinyllama.yaml
-rw-r--r-- 1 1000 1000 609M Jun 28 02:44 tinyllama.gguf
-rw-r--r-- 1 1000 1000  139 Jun 28 03:20 tinyllama.yaml
nickspi5@raspberrypi1:~/ha-voice $ curl -v --max-time 20 http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "tinyllama",
        "messages": [
          {"role": "user", "content": "Hey chatty, why is the sky coloured blue?"}
        ]
      }'
*   Trying 127.0.0.1:8080...
* Connected to localhost (127.0.0.1) port 8080 (#0)
> POST /v1/chat/completions HTTP/1.1
> Host: localhost:8080
> User-Agent: curl/7.88.1
> Accept: */*
> Content-Type: application/json
> Content-Length: 154
> 
< HTTP/1.1 200 OK
< Date: Sun, 29 Jun 2025 03:35:05 GMT
< Content-Type: application/json
< Content-Length: 313
< X-Correlation-Id: 2764b92d-b0fe-4b4f-b88f-fe8b072a79c6
< 
* Connection #0 to host localhost left intact
{"created":1751168105,"object":"chat.completion","id":"5108dfe8-900c-4adb-a3dd-57f354b20fe0","model":"tinyllama","choices":[{"index":0,"finish_reason":"stop","message":{"role":"assistant","content":", y los que se quedan estÃ¡n perdidos.\n"}}],"usage":{"prompt_tokens":5,"completion_tokens":13,"total_tokens":18}}nickspi5@raspberrypi1:~/ha-voice $ 
