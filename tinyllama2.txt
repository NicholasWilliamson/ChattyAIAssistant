Most Proven LLaMA 3.1 / 3.2-Based GGUF Chat Models for Raspberry Pi 5 (Q4_K_M)
Since LLaMA 3.2 GGUF models are still rolling out slowly, the best option currently fully available and proven is:

ðŸ§  NeuralHermes-2.5-Mistral-7B.Q4_K_M.gguf
âœ… Based on Mistral 7B (super fast & efficient)

âœ… Excellent instruction/chat performance

âœ… Available now in GGUF (from TheBloke)

âœ… Works well on Raspberry Pi 5 (needs ~4.5GB RAM)

âœ… Works with LocalAI

ðŸ”— Download:

bash
Copy
Edit
wget https://huggingface.co/TheBloke/NeuralHermes-2.5-Mistral-7B-GGUF/resolve/main/NeuralHermes-2.5-Mistral-7B.Q4_K_M.gguf -O models/neuralhermes.gguf
ðŸ”„ YAML for LocalAI (models/neuralhermes.yaml)
yaml
Copy
Edit
name: neuralhermes
backend: llama-cpp
parameters:
  model: /models/neuralhermes.gguf
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  n_predict: 400
  stopwords:
    - "<|im_end|>"
    - "<|endoftext|>"
  repeat_penalty: 1.1
  threads: 4
  batch: 64
  context_size: 4096
  f16: true
  gpu_layers: 35
Then restart:

bash
Copy
Edit
sudo docker compose down
sudo docker compose up -d
And test it:

bash
Copy
Edit
curl -s http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "neuralhermes",
    "messages": [
      {"role": "user", "content": "Hey Chatty, how do I cook a steak medium rare?"}
    ]
  }' | jq