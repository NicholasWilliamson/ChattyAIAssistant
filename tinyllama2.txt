✅ (B) Update models.yaml
- name: phi2
  model: phi-2/phi-2.Q4_K_M.gguf
  backend: llama-cpp
  parameters:
    temperature: 0.7
    top_p: 0.9
    threads: 4
  template:
    chat: chatml

✅ (C) Restart Docker
sudo docker compose down
sudo docker compose up -d

✅ (D) Test It
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi2",
    "messages": [
      {"role": "system", "content": "You are a helpful Australian voice assistant."},
      {"role": "user", "content": "Hey Chatty, what do you do?"}
    ]
  }' | jq '.choices[0].message.content'
You should see a normal chat response like:

"I'm your friendly local voice assistant, mate! I can help answer questions, give tips, and more. Just ask!"
✅ Summary