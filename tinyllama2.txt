Thank you ChatGPT. You are fantastic.

I did as you said.

I ran nickspi5@raspberrypi1:~/ha-voice $ sudo docker exec -it localai ls -lh /models/
total 4.3G
-rw-r--r-- 1 root root  263 Jun 25 08:24 MODEL_CARD
-rw-r--r-- 1 root root  61M Jun 25 08:24 en-us-amy-low.onnx
-rw-r--r-- 1 root root 5.7K Jun 25 08:24 en-us-amy-low.onnx.json
-rw-r--r-- 1 1000 1000   12 Jun 29 08:40 models.yaml
-rw-r--r-- 1 1000 1000  27K Jun 28 00:31 mpt-7b-chat-q4_0.gguf
-rw-r--r-- 1 1000 1000  343 Jun 29 12:45 phi-2-chat.yaml
-rw-r--r-- 1 1000 1000 1.7G Jun 28 13:35 phi-2.Q4_K_M.gguf
-rw-r--r-- 1 root root 2.2M Jun 23 11:47 silero-vad.onnx
-rw-r--r-- 1 root root 1.5G Jun 27 07:03 stable-diffusion-v1-5-pruned-emaonly-Q4_0.gguf
-rw-r--r-- 1 1000 1000 1.1G Jun 29 05:56 tinyllama-1.1b-chat-v1.0.Q8_0.gguf
drwxr-xr-x 2 1000 1000 4.0K Jun 29 10:30 unused
nickspi5@raspberrypi1:~/ha-voice $ sudo docker compose down
[+] Running 6/6
 ✔ Container localai            Removed                                                                                  0.6s 
 ✔ Container whisper            Removed                                                                                 10.6s 
 ✔ Container piper              Removed                                                                                  0.5s 
 ✔ Container wyoming-satellite  Removed                                                                                 10.6s 
 ✔ Container homeassistant      Removed                                                                                  4.8s 
 ✔ Network ha-voice_default     Removed                                                                                  0.2s 
nickspi5@raspberrypi1:~/ha-voice $ sudo docker compose up -d
[+] Running 6/6
 ✔ Network ha-voice_default     Created                                                                                  0.1s 
 ✔ Container piper              Started                                                                                  1.3s 
 ✔ Container wyoming-satellite  Started                                                                                  1.6s 
 ✔ Container homeassistant      Started                                                                                  0.6s 
 ✔ Container whisper            Started                                                                                  1.6s 
 ✔ Container localai            Started                                                                                  1.2s 
nickspi5@raspberrypi1:~/ha-voice $ sudo docker logs -f localai
CPU info:
CPU: no AVX    found
CPU: no AVX2   found
CPU: no AVX512 found
12:52PM DBG Setting logging to debug
12:52PM INF Starting LocalAI using 4 threads, with models path: /models
12:52PM INF LocalAI version: v3.1.0 (6a650e68cb37487615887e27039f5b85fe0d418d)
12:52PM DBG CPU capabilities: [aes asimd asimddp asimdhp asimdrdm atomics cpuid crc32 dcpop evtstrm fp fphp lrcpc pmull sha1 sha2]
WARNING: failed to determine memory area for node: open /sys/devices/system/node/node0/hugepages: no such file or directory
WARNING: failed to read int from file: open /sys/class/drm/card0/device/numa_node: no such file or directory
WARNING: failed to read int from file: open /sys/class/drm/card1/device/numa_node: no such file or directory
WARNING: failed to determine memory area for node: open /sys/devices/system/node/node0/hugepages: no such file or directory
12:52PM DBG GPU count: 2
12:52PM DBG GPU: card #0 @1002000000.v3d
12:52PM DBG GPU: card #1 @axi:gpu
12:52PM WRN [startup] failed resolving model '/usr/bin/local-ai'
12:52PM ERR error installing models error="failed resolving model '/usr/bin/local-ai'"
12:52PM DBG GPU vendor gpuVendor=
12:52PM DBG guessDefaultsFromFile: NGPULayers set NGPULayers=99999999
12:52PM DBG guessDefaultsFromFile: template already set name=phi-2-chat
12:52PM INF Preloading models from /models

  Model name: phi-2-chat                                                      


12:52PM DBG Model:  (config: {PredictionOptions:{BasicModelRequest:{Model:} Language: Translate:false N:0 TopP:0x400d896a00 TopK:0x400d896a08 Temperature:0x400d896a10 Maxtokens:0x400d896a40 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 RepeatLastN:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0x400d896a38 TypicalP:0x400d896a30 Seed:0x400d896a50 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 ClipSkip:0 Tokenizer:} Name: F16:0x400d8969f8 Threads:0x400d8969f0 Debug:0x400d896a48 Roles:map[] Embeddings:0x400d896a49 Backend: TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions: UseTokenizerTemplate:false JoinChatMessagesByCharacter:<nil> Multimodal: JinjaTemplate:false ReplyPrefix:} KnownUsecaseStrings:[FLAG_ANY] KnownUsecases:<nil> Pipeline:{TTS: LLM: Transcription: VAD:} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: ResponseFormat: ResponseFormatMap:map[] FunctionsConfig:{DisableNoAction:false GrammarConfig:{ParallelCalls:false DisableParallelNewLines:false MixedMode:false NoMixedFreeString:false NoGrammar:false Prefix: ExpectStringsAfterJSON:false PropOrder: SchemaType: GrammarTriggers:[]} NoActionFunctionName: NoActionDescriptionName: ResponseRegex:[] JSONRegexMatch:[] ArgumentRegex:[] ArgumentRegexKey: ArgumentRegexValue: ReplaceFunctionResults:[] ReplaceLLMResult:[] CaptureLLMResult:[] FunctionNameKey: FunctionArgumentsKey:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0x400d896a28 MirostatTAU:0x400d896a20 Mirostat:0x400d896a18 NGPULayers:<nil> MMap:0x400d896a48 MMlock:0x400d896a49 LowVRAM:0x400d896a49 Reranking:0x400d896a49 Grammar: StopWords:[] Cutstrings:[] ExtractRegex:[] TrimSpace:[] TrimSuffix:[] ContextSize:0x400d896a58 NUMA:false LoraAdapter: LoraBase: LoraAdapters:[] LoraScales:[] LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: LoadFormat: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 DisableLogStatus:false DType: LimitMMPerPrompt:{LimitImagePerPrompt:0 LimitVideoPerPrompt:0 LimitAudioPerPrompt:0} MMProj: FlashAttention:false NoKVOffloading:false CacheTypeK: CacheTypeV: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0 CFGScale:0} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} TTSConfig:{Voice: AudioPath:} CUDA:false DownloadFiles:[] Description: Usage: Options:[]})
12:52PM DBG Model: phi-2-chat (config: {PredictionOptions:{BasicModelRequest:{Model:phi-2.Q4_K_M.gguf} Language: Translate:false N:0 TopP:0x400d896cf8 TopK:0x400d896d90 Temperature:0x400d896cd8 Maxtokens:0x400d896dc8 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 RepeatLastN:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0x400d896dc0 TypicalP:0x400d896db8 Seed:0x400d896dd8 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 ClipSkip:0 Tokenizer:} Name:phi-2-chat F16:0x400d896d80 Threads:0x400d896d68 Debug:0x400d896dd0 Roles:map[] Embeddings:0x400d896dd1 Backend:llama-cpp TemplateConfig:{Chat:llama-2 ChatMessage: Completion: Edit: Functions: UseTokenizerTemplate:false JoinChatMessagesByCharacter:<nil> Multimodal: JinjaTemplate:false ReplyPrefix:} KnownUsecaseStrings:[FLAG_CHAT FLAG_ANY] KnownUsecases:<nil> Pipeline:{TTS: LLM: Transcription: VAD:} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: ResponseFormat: ResponseFormatMap:map[] FunctionsConfig:{DisableNoAction:false GrammarConfig:{ParallelCalls:false DisableParallelNewLines:false MixedMode:false NoMixedFreeString:false NoGrammar:false Prefix: ExpectStringsAfterJSON:false PropOrder: SchemaType: GrammarTriggers:[]} NoActionFunctionName: NoActionDescriptionName: ResponseRegex:[] JSONRegexMatch:[] ArgumentRegex:[] ArgumentRegexKey: ArgumentRegexValue: ReplaceFunctionResults:[] ReplaceLLMResult:[] CaptureLLMResult:[] FunctionNameKey: FunctionArgumentsKey:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0x400d896db0 MirostatTAU:0x400d896da8 Mirostat:0x400d896da0 NGPULayers:0x400e36e5c8 MMap:0x400d896dd0 MMlock:0x400d896dd1 LowVRAM:0x400d896dd1 Reranking:0x400d896dd1 Grammar: StopWords:[] Cutstrings:[] ExtractRegex:[] TrimSpace:[] TrimSuffix:[] ContextSize:0x400e36e5c0 NUMA:false LoraAdapter: LoraBase: LoraAdapters:[] LoraScales:[] LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: LoadFormat: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 DisableLogStatus:false DType: LimitMMPerPrompt:{LimitImagePerPrompt:0 LimitVideoPerPrompt:0 LimitAudioPerPrompt:0} MMProj: FlashAttention:false NoKVOffloading:false CacheTypeK: CacheTypeV: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0 CFGScale:0} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} TTSConfig:{Voice: AudioPath:} CUDA:false DownloadFiles:[] Description: Usage: Options:[]})
12:52PM DBG Extracting backend assets files to /tmp/localai/backend_data
12:52PM DBG processing api keys runtime update
12:52PM DBG processing external_backends.json
12:52PM DBG external backends loaded from external_backends.json
12:52PM INF core/startup process completed!
12:52PM DBG No configuration file found at /tmp/localai/upload/uploadedFiles.json
12:52PM DBG No configuration file found at /tmp/localai/config/assistants.json
12:52PM DBG No configuration file found at /tmp/localai/config/assistantsFile.json
12:52PM DBG GPU vendor gpuVendor=
12:52PM INF LocalAI API is listening! Please connect to the endpoint for API documentation. endpoint=http://0.0.0.0:8080
12:53PM INF Success ip=127.0.0.1 latency="220.145µs" method=GET status=200 url=/readyz


