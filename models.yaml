models:
  - name: tinyllama.gguf
    backend: llama-cpp
    path: ./tinyllama.gguf
    parameters:
      n_ctx: 2048
      n_threads: 4