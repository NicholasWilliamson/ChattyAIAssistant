Thank you ChatGPT. You are great.

I ran sudo docker compose down
[+] Running 6/6
 ✔ Container piper              Removed                                                                                  0.5s 
 ✔ Container localai            Removed                                                                                  0.5s 
 ✔ Container homeassistant      Removed                                                                                  4.8s 
 ✔ Container whisper            Removed                                                                                 10.5s 
 ✔ Container wyoming-satellite  Removed                                                                                 10.6s 
 ✔ Network ha-voice_default     Removed                                                                                  0.3s 
nickspi5@raspberrypi1:~/ha-voice $ sudo docker compose up -d
[+] Running 6/6
 ✔ Network ha-voice_default     Created                                                                                  0.1s 
 ✔ Container localai            Started                                                                                  1.5s 
 ✔ Container homeassistant      Started                                                                                  0.7s 
 ✔ Container whisper            Started                                                                                  1.1s 
 ✔ Container piper              Started                                                                                  1.6s 
 ✔ Container wyoming-satellite  Started                                                                                  1.8s 
nickspi5@raspberrypi1:~/ha-voice $ sudo docker logs -f localai
===> LocalAI All-in-One (AIO) container starting...
GPU acceleration is not enabled or supported. Defaulting to CPU.
===> Starting LocalAI[cpu] with the following models: /aio/cpu/embeddings.yaml,/aio/cpu/rerank.yaml,/aio/cpu/text-to-speech.yaml,/aio/cpu/image-gen.yaml,/aio/cpu/text-to-text.yaml,/aio/cpu/speech-to-text.yaml,/aio/cpu/vad.yaml,/aio/cpu/vision.yaml
@@@@@
Skipping rebuild
@@@@@
If you are experiencing issues with the pre-compiled builds, try setting REBUILD=true
If you are still experiencing issues with the build, try setting CMAKE_ARGS and disable the instructions set as needed:
CMAKE_ARGS="-DGGML_F16C=OFF -DGGML_AVX512=OFF -DGGML_AVX2=OFF -DGGML_FMA=OFF"
see the documentation at: https://localai.io/basics/build/index.html
Note: See also https://github.com/go-skynet/LocalAI/issues/288
@@@@@
CPU info:
CPU: no AVX    found
CPU: no AVX2   found
CPU: no AVX512 found
@@@@@
11:06AM INF env file found, loading environment variables from file envFile=.env
11:06AM DBG Setting logging to debug
11:06AM INF Starting LocalAI using 4 threads, with models path: /models
11:06AM INF LocalAI version: v3.0.0 (f9b968e19d7cbc556d59dceb2e0e450b828a3fda)
11:06AM DBG CPU capabilities: [aes asimd asimddp asimdhp asimdrdm atomics cpuid crc32 dcpop evtstrm fp fphp lrcpc pmull sha1 sha2]
WARNING: failed to determine memory area for node: open /sys/devices/system/node/node0/hugepages: no such file or directory
WARNING: failed to read int from file: open /sys/class/drm/card0/device/numa_node: no such file or directory
WARNING: failed to read int from file: open /sys/class/drm/card1/device/numa_node: no such file or directory
WARNING: failed to determine memory area for node: open /sys/devices/system/node/node0/hugepages: no such file or directory
WARNING: error parsing the pci address "1002000000.v3d"
WARNING: error parsing the pci address "axi:gpu"
11:06AM DBG GPU count: 2
11:06AM DBG GPU: card #0 @1002000000.v3d
11:06AM DBG GPU: card #1 @axi:gpu
11:06AM DBG [startup] resolved local model: /aio/cpu/embeddings.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/rerank.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/text-to-speech.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/image-gen.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/text-to-text.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/speech-to-text.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/vad.yaml
11:06AM DBG [startup] resolved local model: /aio/cpu/vision.yaml
11:07AM WRN [startup] failed resolving model '/usr/bin/local-ai'
11:07AM ERR error installing models error="failed resolving model '/usr/bin/local-ai'"
11:07AM ERR guessDefaultsFromFile: panic while parsing gguf file
11:07AM ERR config is not valid Name=tinyllama
11:07AM INF Preloading models from /models
11:07AM INF Downloading "https://huggingface.co/bartowski/granite-embedding-107m-multilingual-GGUF/resolve/main/granite-embedding-107m-multilingual-f16.gguf"
11:07AM ERR error downloading models error="failed to check if uri server supports range header: Head \"https://huggingface.co/bartowski/granite-embedding-107m-multilingual-GGUF/resolve/main/granite-embedding-107m-multilingual-f16.gguf\": dial tcp: lookup huggingface.co on 127.0.0.11:53: read udp 127.0.0.1:40749->127.0.0.11:53: i/o timeout"
11:07AM DBG Model:  (config: {PredictionOptions:{BasicModelRequest:{Model:} Language: Translate:false N:0 TopP:0x400ce31ca0 TopK:0x400ce31ca8 Temperature:0x400ce31cb0 Maxtokens:0x400ce31ce0 Echo:false Batch:0 IgnoreEOS:false RepeatPenalty:0 RepeatLastN:0 Keep:0 FrequencyPenalty:0 PresencePenalty:0 TFZ:0x400ce31cd8 TypicalP:0x400ce31cd0 Seed:0x400ce31cf0 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 ClipSkip:0 Tokenizer:} Name: F16:0x400ce31c98 Threads:0x400ce31c90 Debug:0x400ce31ce8 Roles:map[] Embeddings:0x400ce31ce9 Backend: TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions: UseTokenizerTemplate:false JoinChatMessagesByCharacter:<nil> Multimodal: JinjaTemplate:false ReplyPrefix:} KnownUsecaseStrings:[FLAG_ANY] KnownUsecases:<nil> Pipeline:{TTS: LLM: Transcription: VAD:} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: ResponseFormat: ResponseFormatMap:map[] FunctionsConfig:{DisableNoAction:false GrammarConfig:{ParallelCalls:false DisableParallelNewLines:false MixedMode:false NoMixedFreeString:false NoGrammar:false Prefix: ExpectStringsAfterJSON:false PropOrder: SchemaType: GrammarTriggers:[]} NoActionFunctionName: NoActionDescriptionName: ResponseRegex:[] JSONRegexMatch:[] ArgumentRegex:[] ArgumentRegexKey: ArgumentRegexValue: ReplaceFunctionResults:[] ReplaceLLMResult:[] CaptureLLMResult:[] FunctionNameKey: FunctionArgumentsKey:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0x400ce31cc8 MirostatTAU:0x400ce31cc0 Mirostat:0x400ce31cb8 NGPULayers:<nil> MMap:0x400ce31ce8 MMlock:0x400ce31ce9 LowVRAM:0x400ce31ce9 Reranking:0x400ce31ce9 Grammar: StopWords:[] Cutstrings:[] ExtractRegex:[] TrimSpace:[] TrimSuffix:[] ContextSize:0x400ce31cf8 NUMA:false LoraAdapter: LoraBase: LoraAdapters:[] LoraScales:[] LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: LoadFormat: GPUMemoryUtilization:0 TrustRemoteCode:false EnforceEager:false SwapSpace:0 MaxModelLen:0 TensorParallelSize:0 DisableLogStatus:false DType: LimitMMPerPrompt:{LimitImagePerPrompt:0 LimitVideoPerPrompt:0 LimitAudioPerPrompt:0} MMProj: FlashAttention:false NoKVOffloading:false CacheTypeK: CacheTypeV: RopeScaling: ModelType: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0 CFGScale:0} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} TTSConfig:{Voice: AudioPath:} CUDA:false DownloadFiles:[] Description: Usage: Options:[]})
