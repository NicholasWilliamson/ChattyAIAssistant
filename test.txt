# CHATTY AI FIXES
# Apply these changes to fix wake word detection and improve LLM responses

# 1. LOWER SILENCE THRESHOLD (Line 94)
# Change from: SILENCE_THRESHOLD = 0.035
# Change to:   SILENCE_THRESHOLD = 0.015

# 2. REPLACE record_wake_word_check METHOD (Starting at line 1193)
def record_wake_word_check(self):
    """Record short audio clip for wake word detection with debugging"""
    try:
        if not self.audio_recording_lock.acquire(blocking=False):
            return False  # Audio system is busy
        
        try:
            # Record 5 seconds of audio for wake word detection
            audio_data = sd.rec(int(5 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
            sd.wait()
            
            # Check if audio contains sound above threshold with debugging
            rms = np.sqrt(np.mean(audio_data**2))
            self.emit_log(f"Wake word audio RMS: {rms:.4f} (threshold: {SILENCE_THRESHOLD * 2:.4f})", 'debug')
            
            if rms > SILENCE_THRESHOLD * 1.5:  # Lower threshold for more sensitivity
                sf.write(WAKE_WORD_AUDIO, audio_data, SAMPLE_RATE)
                self.emit_log(f"Wake word audio saved - RMS {rms:.4f} exceeded threshold", 'info')
                return True
            else:
                self.emit_log(f"Wake word audio too quiet - RMS {rms:.4f} below threshold", 'debug')
                return False
                
        finally:
            self.audio_recording_lock.release()
            
    except Exception as e:
        self.emit_log(f"Wake word recording error: {e}", 'error')
        return False

# 3. REPLACE get_llm_joke METHOD (Starting at line ~1020)
def get_llm_joke(self):
    """Ask the local LLM for a joke with better prompting"""
    try:
        # Randomized joke prompts for variety
        joke_prompts = [
            "Tell me a short, funny joke that will make someone laugh. No setup needed, just the punchline style joke.",
            "Give me a clever one-liner joke. Make it witty and brief.",
            "Share a funny joke - something silly and lighthearted that's family friendly.",
            "Tell me a quick joke that's actually funny. Keep it short and punchy.",
            "Give me a dad joke or pun that's genuinely amusing."
        ]
        
        prompt = random.choice(joke_prompts)
        formatted_prompt = f"You are a comedian. Tell jokes that are actually funny.\nUser: {prompt}\nAssistant: "
        
        # Better LLM parameters for more varied responses
        result = self.llama_model(
            formatted_prompt, 
            max_tokens=80,
            temperature=0.8,  # More randomness
            top_p=0.9,
            repeat_penalty=1.2
        )
        
        if "choices" in result and result["choices"]:
            joke = result["choices"][0]["text"].strip()
            # Clean up the response more thoroughly
            joke = re.sub(r"\(.*?\)", "", joke)
            joke = re.sub(r"(User:|Assistant:|Here's|Here is)", "", joke, flags=re.IGNORECASE)
            joke = joke.strip()
            
            # Take first sentence if multiple sentences
            if '.' in joke:
                first_sentence = joke.split('.')[0].strip()
                if len(first_sentence) > 10:  # Make sure it's substantial
                    joke = first_sentence + '.'
            
            return joke if len(joke) > 10 else "Why don't scientists trust atoms? Because they make up everything!"
        else:
            return "Why don't scientists trust atoms? Because they make up everything!"
    except Exception as e:
        self.emit_log(f"LLM joke error: {e}", 'error')
        return "Why don't scientists trust atoms? Because they make up everything!"

# 4. REPLACE get_llm_fun_fact METHOD (Starting at line ~1045)
def get_llm_fun_fact(self):
    """Ask the local LLM for a fun fact with better prompting"""
    try:
        # Randomized fact prompts for variety
        fact_prompts = [
            "Share a fascinating scientific fact that most people don't know. Keep it brief but mind-blowing.",
            "Tell me an amazing fact about animals, space, or nature. Make it interesting and surprising.",
            "Give me a cool historical fact that sounds unbelievable but is true.",
            "Share an interesting fact about the human body or psychology that's genuinely surprising.",
            "Tell me a fun fact about technology, science, or the world that will amaze someone."
        ]
        
        prompt = random.choice(fact_prompts)
        formatted_prompt = f"You are an encyclopedia of amazing facts. Share fascinating knowledge.\nUser: {prompt}\nAssistant: "
        
        # Better LLM parameters for more varied responses
        result = self.llama_model(
            formatted_prompt, 
            max_tokens=120,
            temperature=0.7,  # Some randomness but still factual
            top_p=0.8,
            repeat_penalty=1.3
        )
        
        if "choices" in result and result["choices"]:
            fact = result["choices"][0]["text"].strip()
            # Clean up the response
            fact = re.sub(r"\(.*?\)", "", fact)
            fact = re.sub(r"(User:|Assistant:|Did you know|Here's|Here is)", "", fact, flags=re.IGNORECASE)
            fact = fact.strip()
            
            # Take only the first 2-3 sentences to keep it concise
            sentences = fact.split('.')
            if len(sentences) > 3:
                fact = '. '.join(sentences[:2]) + '.'
            
            return fact if len(fact) > 15 else "Did you know that octopuses have three hearts and blue blood?"
        else:
            return "Did you know that octopuses have three hearts and blue blood?"
    except Exception as e:
        self.emit_log(f"LLM fun fact error: {e}", 'error')
        return "Did you know that octopuses have three hearts and blue blood?"

# 5. REPLACE check_for_bored_response METHOD (Starting at line 1071)
def check_for_bored_response(self, name):
    """Check if it's time to give a bored response with joke or fun fact from LLM"""
    if not self.wake_word_active or not self.last_bored_response_time:
        return False
    
    current_time = time.time()
    time_since_bored = current_time - self.last_bored_response_time
    
    if time_since_bored >= BORED_RESPONSE_INTERVAL:
        # Add 1-second delay before response
        time.sleep(1.0)
        
        if self.bored_cycle == 0:
            # Give bored response + joke from LLM
            if self.bored_responses:
                bored_template = random.choice(self.bored_responses)
                bored_msg = bored_template.replace("{name}", name)
            else:
                bored_msg = f"Yo {name}, still hanging around here waiting for you, dude!"
            
            # Get LLM joke
            joke = self.get_llm_joke()
            
            # Speak the bored message first
            self.speak_text(bored_msg)
            self.emit_conversation(f"üò¥ Bored response for {name}: {bored_msg}", 'response')
            
            # Add 1-second pause between messages
            time.sleep(1.0)
            
            # Then tell the joke
            joke_intro = "Let me tell you a joke!"
            self.speak_text(joke_intro)
            time.sleep(0.5)  # Short pause
            self.speak_text(joke)
            self.emit_conversation(f"üòÑ Joke for {name}: {joke}", 'response')
            
            self.bored_cycle = 1
            self.emit_log(f"Gave {name} a bored response with LLM joke", 'info')
        else:
            # Give waiting response + fun fact from LLM
            if self.waiting_responses:
                waiting_template = random.choice(self.waiting_responses)
                waiting_msg = waiting_template.replace("{name}", name)
            else:
                waiting_msg = f"I am still around if you need me, {name}"
            
            # Get LLM fun fact
            fun_fact = self.get_llm_fun_fact()
            
            # Speak the waiting message first
            self.speak_text(waiting_msg)
            self.emit_conversation(f"‚è≥ Waiting response for {name}: {waiting_msg}", 'response')
            
            # Add 1-second pause between messages
            time.sleep(1.0)
            
            # Then share the fun fact
            fact_intro = "Here's a fun fact for you!"
            self.speak_text(fact_intro)
            time.sleep(0.5)  # Short pause
            self.speak_text(fun_fact)
            self.emit_conversation(f"üí° Fun fact for {name}: {fun_fact}", 'response')
            
            self.bored_cycle = 0
            self.emit_log(f"Gave {name} a waiting response with LLM fun fact", 'info')
        
        self.last_bored_response_time = current_time
        return True
    
    return False

# ADDITIONAL IMPORT NEEDED
# Add this import at the top with other imports if not already present:
# import random

# SUMMARY OF CHANGES:
print("FIXES APPLIED:")
print("1. ‚úÖ Lowered SILENCE_THRESHOLD from 0.035 to 0.015 for better wake word sensitivity")
print("2. ‚úÖ Enhanced record_wake_word_check with debugging and lower threshold multiplier")
print("3. ‚úÖ Improved get_llm_joke with randomized prompts and better LLM parameters")
print("4. ‚úÖ Enhanced get_llm_fun_fact with varied prompts and better response cleaning")
print("5. ‚úÖ Updated check_for_bored_response with proper delays and separate speech segments")
print("6. ‚úÖ Added temperature and repeat_penalty to LLM calls for more variety")




