

Thank you, Claude,

I ran: nickspi5@raspberrypi:~ $ cd /home/nickspi5/Chatty_AI
nickspi5@raspberrypi:~/Chatty_AI $ source chatty-venv/bin/activate
(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ python3 -m py_compile chatty_ai.py && echo "No syntax errors!"
  File "chatty_ai.py", line 918
    break
    ^^^^^
SyntaxError: 'break' outside loop
(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ systemctl status chatty-ai.service
* chatty-ai.service - Chatty AI Web Server
     Loaded: loaded (/etc/systemd/system/chatty-ai.service; enabled; preset: enabled)
     Active: activating (auto-restart) (Result: exit-code) since Sun 2025-12-28 13:46:37 AEDT; 62ms ago
 Invocation: 6efd21575ef245a9aa5280d6c695a667
    Process: 17680 ExecStart=/home/nickspi5/Chatty_AI/chatty-venv/bin/python /home/nickspi5/Chatty_AI/chatty_ai.py (code=exit>
   Main PID: 17680 (code=exited, status=1/FAILURE)
        CPU: 65ms

Dec 28 13:46:37 raspberrypi systemd[1]: chatty-ai.service: Failed with result 'exit-code'.

(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ grep -A50 "def query_llama_streaming" /home/nickspi5/Chatty_AI/chatty_ai.py | head -60
    def query_llama_streaming(self, prompt, callback=None):
        """Generate LLM response with streaming - speaks as tokens arrive"""
        import time as t
        
        print(f"[TIMING] query_llama_streaming called with prompt: '{prompt[:50]}...'")
        start = t.perf_counter()
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            print(f"[TIMING] Starting streaming LLM inference...")
            llm_start = t.perf_counter()
            
            full_response = ""
            current_sentence = ""
            sentences_spoken = 0
            max_sentences = 99

            # Stop after max sentences
            if sentences_spoken >= max_sentences:
                print(f"[STREAMING] Reached {max_sentences} sentences, stopping")
                break

            # Stream tokens from LLM
            for token_data in self.llama_model(
                formatted_prompt, 
                max_tokens=200,
                stop=["\n", "\n\n", "User:", "Human:", "Assistant:"],
                stream=True
            ):
                token = token_data['choices'][0]['text']
                full_response += token
                current_sentence += token
                
                # Emit each token to web interface for real-time display
                if callback:
                    callback(token)
                
                # Check if we have a complete sentence
                if any(current_sentence.rstrip().endswith(p) for p in ['.', '!', '?']):
                    sentences_spoken += 1

                    # Log time to first speech
                    if sentences_spoken == 1:
                        first_speech_time = t.perf_counter() - llm_start
                        print(f"[TIMING] Time to first speech: {first_speech_time:.2f}s")

                    # Clean up the sentence
                    sentence_to_speak = current_sentence.strip()
                    sentence_to_speak = re.sub(r"\(.*?\)", "", sentence_to_speak)
                    sentence_to_speak = re.sub(r"(User:|Assistant:)", "", sentence_to_speak)
(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ 


















