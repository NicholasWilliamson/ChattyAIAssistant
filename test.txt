Thank you, Claude.

I have my chatty_ai.py file, which is working perfectly.

chatty_ai.py
#!/usr/bin/env python3
"""
chatty_ai.py - Complete AI Assistant with Facial Recognition and Wake Word Detection
Enhanced with personalized greetings and responses for different users
"""

import os
import subprocess
import sounddevice as sd
import soundfile as sf
import numpy as np
import threading
import time
import random
import re
import cv2
import face_recognition
import pickle
import json
import requests
import logging
from datetime import datetime, timedelta
from faster_whisper import WhisperModel
from llama_cpp import Llama
from picamera2 import Picamera2

# -------------------------------
# Configuration
# -------------------------------
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
BEEP_SOUND = "/home/nickspi5/Chatty_AI/audio_files/beep.wav"
LAUGHING_SOUND = "/home/nickspi5/Chatty_AI/audio_files/laughing.wav"
ENCODINGS_FILE = "encodings.pickle"
TELEGRAM_CONFIG_FILE = "telegram_config.json"

# Audio files
WAV_FILENAME = "user_input.wav"
RESPONSE_AUDIO = "output.wav"
WAKE_WORD_AUDIO = "wake_word_check.wav"

# Security directories
SECURITY_PHOTOS_DIR = "/home/nickspi5/Chatty_AI/security_photos"
SECURITY_LOGS_DIR = "/home/nickspi5/Chatty_AI/security_logs"

# Response files
JOKES_FILE = "jokes.txt"
LISTENING_RESPONSES_FILE = "listening_responses.txt"
WAITING_RESPONSES_FILE = "waiting_responses.txt"
WARNING_RESPONSES_FILE = "warning_responses.txt"
GREETING_RESPONSES_FILE = "greeting_responses.txt"
BORED_RESPONSES_FILE = "bored_responses.txt"
VISITOR_GREETING_RESPONSES_FILE = "visitor_greeting_responses.txt"

# Wake word phrases
WAKE_WORDS = [
    "are you awake", "are you alive", "hey chatty", "hello chatty", "sup chatty",
    "sub-chatty", "how's it chatty", "howzit chatty", "hi chatty", "yo chatty",
    "hey chuddy", "hello chuddy", "sup chuddy", "sub-chuddy", "how's it chuddy",
    "howzit chuddy", "hi chuddy", "yo chuddy", "hey cheddy", "hello cheddy",
    "sup cheddy", "sub-cheddy", "how's it cheddy", "howzit cheddy", "hi cheddy",
    "yo cheddy", "hey chetty", "hello chetty", "sup chetty", "sub-chetty",
    "how's it chetty", "howzit chetty", "hi chetty", "yo chetty", "hey cherry",
    "hello cherry", "sup cherry", "sub-cherry", "how's it cherry", "howzit cherry",
    "hi cherry", "yo cherry"
]

# Command keywords
COMMANDS = {
    "flush the toilet": "toilet_flush",
    "turn on the lights": "lights_on",
    "turn off the lights": "lights_off",
    "play music": "play_music",
    "stop music": "stop_music",
    "what time is it": "get_time",
    "shutdown system": "shutdown_system",
    "who is sponsoring this video": "who_is_sponsoring_this_video",
    "how is the weather today": "how_is_the_weather_today",
    "reboot system": "reboot_system"
}

# Audio parameters
SAMPLE_RATE = 16000
CHANNELS = 1
SILENCE_THRESHOLD = 0.035
MIN_SILENCE_DURATION = 1.5
MAX_RECORDING_DURATION = 30

# Timing parameters
GREETING_COOLDOWN = 300  # 5 minutes in seconds
BORED_RESPONSE_INTERVAL = 30  # Configurable duration for bored responses
PERSON_DETECTION_INTERVAL = 0.5  # Check for people every 0.5 seconds
WAKE_WORD_CHECK_INTERVAL = 1.0  # Check for wake words every 1 second

class ChattyAI:
    def __init__(self):
        # AI Models
        self.whisper_model = None
        self.llama_model = None
        
        # Facial Recognition
        self.known_encodings = []
        self.known_names = []
        
        # Camera
        self.picam2 = None
        
        # State variables
        self.is_running = False
        self.current_person = None  # Variable to store detected person's name
        self.last_greeting_time = {}
        self.last_interaction_time = None
        self.person_absent_since = None
        self.last_bored_response_time = None  # Track when last bored response was given
        self.bored_cycle = 0  # 0: joke, 1: fun fact
        self.audio_recording_lock = threading.Lock()
        self.wake_word_active = False

        # Response lists
        self.jokes = []
        self.listening_responses = []
        self.waiting_responses = []
        self.warning_responses = []
        self.greeting_responses = []
        self.bored_responses = []  # Generic bored responses
        self.visitor_greeting_responses = []  # Visitor greetings for daytime

        # Telegram
        self.telegram_token = None
        self.telegram_chat_id = None

        # Threading
        self.camera_thread = None
        self.audio_thread = None
        
        # Initialize everything
        self.setup_directories()
        self.load_response_files()
        self.load_models()
        self.load_encodings()
        self.load_telegram_config()
        self.setup_camera()
        self.setup_logging()
    
    def setup_directories(self):
        """Create necessary directories"""
        os.makedirs(SECURITY_PHOTOS_DIR, exist_ok=True)
        os.makedirs(SECURITY_LOGS_DIR, exist_ok=True)
    
    def setup_logging(self):
        """Setup logging for detections"""
        log_file = os.path.join(SECURITY_LOGS_DIR, "chatty_ai.log")
        self.logger = logging.getLogger('chatty_ai')
        self.logger.setLevel(logging.INFO)
        
        # Remove existing handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
    
    def load_response_files(self):
        """Load response text files"""
        try:
            with open(JOKES_FILE, 'r') as f:
                self.jokes = [line.strip() for line in f if line.strip()]
            
            with open(LISTENING_RESPONSES_FILE, 'r') as f:
                self.listening_responses = [line.strip() for line in f if line.strip()]

            with open(GREETING_RESPONSES_FILE, 'r') as f:
                self.greeting_responses = [line.strip() for line in f if line.strip()]

            with open(WAITING_RESPONSES_FILE, 'r') as f:
                self.waiting_responses = [line.strip() for line in f if line.strip()]
            
            with open(WARNING_RESPONSES_FILE, 'r') as f:
                self.warning_responses = [line.strip() for line in f if line.strip()]
            
            # Load bored responses
            try:
                with open(BORED_RESPONSES_FILE, 'r') as f:
                    self.bored_responses = [line.strip() for line in f if line.strip()]
            except FileNotFoundError:
                print(f"Bored responses file not found. Creating default...")
                self.create_default_bored_responses()
            
            # Load visitor greeting responses
            try:
                with open(VISITOR_GREETING_RESPONSES_FILE, 'r') as f:
                    self.visitor_greeting_responses = [line.strip() for line in f if line.strip()]
            except FileNotFoundError:
                print(f"Visitor greeting responses file not found. Creating default...")
                self.create_default_visitor_responses()
            
            print("Response files loaded successfully")
        except FileNotFoundError as e:
            print(f"Response file not found: {e}")
            # Create default responses if files don't exist
            self.create_default_responses()
    
    def create_default_bored_responses(self):
        """Create default bored responses file"""
        default_bored = [
            "I'm getting a bit bored waiting here",
            "Still hanging around here waiting for you, dude",
            "I'm patiently waiting for your commands",
            "I am feeling restless waiting here",
            "Still here waiting to help you"
        ]
        
        try:
            with open(BORED_RESPONSES_FILE, 'w') as f:
                for response in default_bored:
                    f.write(response + '\n')
            self.bored_responses = default_bored
            print(f"Created default bored responses file: {BORED_RESPONSES_FILE}")
        except Exception as e:
            print(f"Failed to create bored responses file: {e}")
            self.bored_responses = default_bored
    
    def create_default_visitor_responses(self):
        """Create default visitor greeting responses file"""
        default_visitor = [
            "Hello. I do not recognize you. Can I be of assistance?",
            "Good day, visitor. How may I help you today?",
            "Welcome. I do not believe we have met before. What can I do for you?",
            "Hello there. I am an AI assistant. How can I assist you?",
            "It is great to meet you. I do not recognize your face, but I am happy to help."
        ]
        
        try:
            with open(VISITOR_GREETING_RESPONSES_FILE, 'w') as f:
                for response in default_visitor:
                    f.write(response + '\n')
            self.visitor_greeting_responses = default_visitor
            print(f"Created default visitor responses file: {VISITOR_GREETING_RESPONSES_FILE}")
        except Exception as e:
            print(f"Failed to create visitor responses file: {e}")
            self.visitor_greeting_responses = default_visitor
    
    def create_default_responses(self):
        """Create default responses if files are missing"""
        self.jokes = ["Why don't scientists trust atoms? Because they make up everything!"]
        self.greeting_responses = ["Hey {name}! Good to see you, buddy! What's up?"]
        self.listening_responses = ["Yes {name}, I'm listening. What would you like to know?"]
        self.waiting_responses = ["I am still around if you need me, {name}"]
        self.warning_responses = ["Attention unauthorized person, you are not authorized to access this property. Leave immediately. I am contacting the authorities to report your intrusion."]
        self.bored_responses = ["I'm getting a bit bored waiting here"]
        self.visitor_greeting_responses = ["Hello. I do not recognize you. Can I be of assistance?"]
    
    def is_daytime_hours(self):
        """Check if current time is between 6:00AM and 12:00PM (daytime visitor hours)"""
        current_hour = datetime.now().hour
        return 6 <= current_hour <= 12
    
    def get_llm_joke(self):
        """Ask the local LLM for a joke"""
        try:
            prompt = "Tell me a short, clean joke. Just the joke, nothing else."
            formatted_prompt = f"You are a comedian. Tell a brief, funny joke.\nUser: {prompt}\nAssistant: "
            
            result = self.llama_model(formatted_prompt, max_tokens=80)
            if "choices" in result and result["choices"]:
                joke = result["choices"][0]["text"].strip()
                # Clean up the response
                joke = re.sub(r"\(.*?\)", "", joke)
                joke = re.sub(r"(User:|Assistant:)", "", joke)
                joke = joke.strip()
                
                # Take only the first few sentences
                sentences = joke.split('.')
                if len(sentences) > 2:
                    joke = '. '.join(sentences[:2]) + '.'
                
                return joke if joke else "Why did the computer go to therapy? Because it had too many bytes!"
            else:
                return "Why did the computer go to therapy? Because it had too many bytes!"
        except Exception as e:
            print(f"LLM joke error: {e}")
            return "Why did the computer go to therapy? Because it had too many bytes!"
    
    def get_llm_fun_fact(self):
        """Ask the local LLM for a fun fact"""
        try:
            prompt = "Tell me an interesting fun fact. Keep it brief and fascinating."
            formatted_prompt = f"You are a knowledgeable teacher. Share one interesting fact.\nUser: {prompt}\nAssistant: "
            
            result = self.llama_model(formatted_prompt, max_tokens=100)
            if "choices" in result and result["choices"]:
                fact = result["choices"][0]["text"].strip()
                # Clean up the response
                fact = re.sub(r"\(.*?\)", "", fact)
                fact = re.sub(r"(User:|Assistant:)", "", fact)
                fact = fact.strip()
                
                # Take only the first few sentences
                sentences = fact.split('.')
                if len(sentences) > 3:
                    fact = '. '.join(sentences[:3]) + '.'
                
                return fact if fact else "Did you know that octopuses have three hearts and blue blood?"
            else:
                return "Did you know that octopuses have three hearts and blue blood?"
        except Exception as e:
            print(f"LLM fun fact error: {e}")
            return "Did you know that octopuses have three hearts and blue blood?"
    
    def load_models(self):
        """Load AI models"""
        print("Loading AI models...")
        
        try:
            self.whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device="cpu", compute_type="int8")
            print("Whisper model loaded")
        except Exception as e:
            print(f"Failed to load Whisper: {e}")
            return False
        
        try:
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH,
                n_ctx=2048,
                temperature=0.7,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                verbose=False
            )
            print("LLaMA model loaded")
        except Exception as e:
            print(f"Failed to load LLaMA: {e}")
            return False
        
        return True
    
    def load_encodings(self):
        """Load facial recognition encodings"""
        try:
            with open(ENCODINGS_FILE, "rb") as f:
                data = pickle.loads(f.read())
                self.known_encodings = data["encodings"]
                self.known_names = data["names"]
            print(f"Loaded {len(self.known_encodings)} face encodings")
            return True
        except FileNotFoundError:
            print(f"Encodings file '{ENCODINGS_FILE}' not found!")
            return False
        except Exception as e:
            print(f"Failed to load encodings: {e}")
            return False
    
    def load_telegram_config(self):
        """Load Telegram configuration"""
        try:
            with open(TELEGRAM_CONFIG_FILE, 'r') as f:
                config = json.load(f)
                self.telegram_token = config.get('bot_token')
                self.telegram_chat_id = config.get('chat_id')
            print("Telegram configuration loaded")
        except FileNotFoundError:
            print("Telegram config not found - alerts disabled")
        except Exception as e:
            print(f"Failed to load Telegram config: {e}")
    
    def setup_camera(self):
        """Initialize camera"""
        try:
            self.picam2 = Picamera2()
            self.picam2.configure(self.picam2.create_preview_configuration(
                main={"format": 'XRGB8888', "size": (640, 480)}
            ))
            self.picam2.start()
            time.sleep(2)  # Camera warm-up
            print("Camera initialized")
            return True
        except Exception as e:
            print(f"Failed to initialize camera: {e}")
            return False
    
    def speak_text(self, text):
        """Convert text to speech using Piper"""
        try:
            # Use lock to prevent audio conflicts
            with self.audio_recording_lock:
                command = [
                    PIPER_EXECUTABLE,
                    "--model", VOICE_PATH,
                    "--config", CONFIG_PATH,
                    "--output_file", RESPONSE_AUDIO
                ]
                subprocess.run(command, input=text.encode("utf-8"), check=True, capture_output=True)
                subprocess.run(["aplay", RESPONSE_AUDIO], check=True, capture_output=True)
        except subprocess.CalledProcessError as e:
            print(f"TTS failed: {e}")
    
    def play_beep(self):
        """Play beep sound"""
        try:
            with self.audio_recording_lock:
                subprocess.run(["aplay", BEEP_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            pass
    
    def play_laughing(self):
        """Play laughing sound"""
        try:
            with self.audio_recording_lock:
                subprocess.run(["aplay", LAUGHING_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            pass
    
    def detect_faces(self, frame):
        """Detect and recognize faces in frame"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        face_locations = face_recognition.face_locations(rgb_frame, model="hog")
        
        if len(face_locations) == 0:
            return None, None, 0.0
        
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        
        best_name = "Unknown"
        best_confidence = 0.0
        
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(self.known_encodings, face_encoding, tolerance=0.6)
            
            if True in matches:
                face_distances = face_recognition.face_distance(self.known_encodings, face_encoding)
                best_match_index = face_distances.argmin()
                confidence = 1.0 - face_distances[best_match_index]
                
                if matches[best_match_index] and confidence > 0.4:
                    if confidence > best_confidence:
                        best_name = self.known_names[best_match_index]
                        best_confidence = confidence
        
        return best_name, face_locations[0], best_confidence
    
    def save_security_photo(self, frame, person_name, confidence):
        """Save security photo with timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{person_name.lower()}_{timestamp}.jpg"
        filepath = os.path.join(SECURITY_PHOTOS_DIR, filename)
        
        # Add overlay information
        overlay_frame = frame.copy()
        cv2.rectangle(overlay_frame, (10, 10), (500, 100), (0, 0, 0), -1)
        cv2.rectangle(overlay_frame, (10, 10), (500, 100), (255, 255, 255), 2)
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(overlay_frame, f"Person: {person_name}", (20, 35), font, 0.7, (255, 255, 255), 2)
        cv2.putText(overlay_frame, f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", (20, 60), font, 0.6, (255, 255, 255), 2)
        if person_name != "Unknown":
            cv2.putText(overlay_frame, f"Confidence: {confidence:.1%}", (20, 85), font, 0.6, (255, 255, 255), 2)
        
        cv2.imwrite(filepath, overlay_frame)
        self.logger.info(f"Security photo saved: {filename} | Person: {person_name} | Confidence: {confidence:.2f}")
        
        return filepath
    
    def send_telegram_alert(self, person_name, confidence, photo_path):
        """Send Telegram alert"""
        if not self.telegram_token or not self.telegram_chat_id:
            return False
        
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            if person_name == "Unknown":
                message = f"**UNKNOWN PERSON DETECTED**\n\n**Time:** {timestamp}\n**Status:** Unregistered Person\n**Action:** Photo captured for review"
            else:
                message = f"**AUTHORIZED ACCESS**\n\n**Person:** {person_name}\n**Time:** {timestamp}\n**Confidence:** {confidence:.1%}\n**Status:** Registered User"
            
            url = f"https://api.telegram.org/bot{self.telegram_token}/sendPhoto"
            with open(photo_path, 'rb') as photo:
                files = {'photo': photo}
                data = {
                    'chat_id': self.telegram_chat_id,
                    'caption': message,
                    'parse_mode': 'Markdown'
                }
                response = requests.post(url, data=data, files=files, timeout=30)
                return response.status_code == 200
        except Exception as e:
            print(f"Telegram alert failed: {e}")
            return False
    
    def greet_person(self, name):
        """Greet a detected person with generic greeting using their name"""
        current_time = time.time()
        
        # Check if we should greet this person (cooldown check)
        if name in self.last_greeting_time:
            time_since_last = current_time - self.last_greeting_time[name]
            if time_since_last < GREETING_COOLDOWN:
                return False
        
        # Use generic greeting with person's name
        if self.greeting_responses:
            greeting_template = random.choice(self.greeting_responses)
            greeting = greeting_template.replace("{name}", name)
        else:
            greeting = f"Hey {name}! Good to see you, buddy! What's up?"
        
        self.speak_text(greeting)
        self.last_greeting_time[name] = current_time
        self.last_interaction_time = current_time
        
        # Enable wake word detection after greeting
        self.wake_word_active = True
        self.last_bored_response_time = current_time  # Reset bored response timer
        self.bored_cycle = 0  # Reset bored cycle
        print(f"Greeted {name} with generic message - Wake word detection now active")
        return True
    
    def handle_unknown_person(self, frame, confidence):
        """Handle unknown person detection with time-based responses"""
        if self.is_daytime_hours():
            # 6:00AM - 12:00PM: Assume visitor, be friendly
            if self.visitor_greeting_responses:
                visitor_greeting = random.choice(self.visitor_greeting_responses)
            else:
                visitor_greeting = "Hello. I do not recognize you. Can I be of assistance?"
            self.speak_text(visitor_greeting)
            print("Unknown person detected during daytime - treated as visitor")
        else:
            # 12:01PM - 5:59AM: Assume intruder, give warning
            if self.warning_responses:
                warning = random.choice(self.warning_responses)
            else:
                warning = "Attention unauthorized person, you are not authorized to access this property. Leave immediately. I am contacting the authorities to report your intrusion."
            self.speak_text(warning)
            print("Unknown person detected during nighttime/evening - treated as intruder")
        
        photo_path = self.save_security_photo(frame, "Unknown", confidence)
        self.send_telegram_alert("Unknown", confidence, photo_path)
    
    def check_for_bored_response(self, name):
        """Check if it's time to give a bored response with joke or fun fact from LLM"""
        if not self.wake_word_active or not self.last_bored_response_time:
            return False
        
        current_time = time.time()
        time_since_bored = current_time - self.last_bored_response_time
        
        if time_since_bored >= BORED_RESPONSE_INTERVAL:
            if self.bored_cycle == 0:
                # Give bored response + joke from LLM
                if self.bored_responses:
                    bored_template = random.choice(self.bored_responses)
                    bored_msg = bored_template.replace("{name}", name)
                else:
                    bored_msg = f"Yo {name}, still hanging around here waiting for you, dude!"
                
                joke = self.get_llm_joke()
                # Create complete message with 2-second pause represented as spoken text
                full_message = f"{bored_msg} Let me tell you a joke! ... ... {joke}"
                self.speak_text(full_message)
                self.bored_cycle = 1
                print(f"Gave {name} a bored response with LLM joke")
            else:
                # Give waiting response + fun fact from LLM
                if self.waiting_responses:
                    waiting_template = random.choice(self.waiting_responses)
                    waiting_msg = waiting_template.replace("{name}", name)
                else:
                    waiting_msg = f"I am still around if you need me, {name}"
                
                fun_fact = self.get_llm_fun_fact()
                # Create complete message with 2-second pause represented as spoken text
                full_message = f"{waiting_msg} Let me tell you a fun fact! ... ... {fun_fact}"
                self.speak_text(full_message)
                self.bored_cycle = 0
                print(f"Gave {name} a waiting response with LLM fun fact")
            
            self.last_bored_response_time = current_time
            return True
        
        return False
    
    def transcribe_audio(self, filename):
        """Transcribe audio using Whisper"""
        try:
            if not os.path.exists(filename):
                return ""
            
            segments, _ = self.whisper_model.transcribe(filename)
            transcript = " ".join(segment.text for segment in segments).strip()
            print(f"Transcription: '{transcript}'")  # Debug output
            return transcript
        except Exception as e:
            print(f"Transcription error: {e}")
            return ""
    
    def detect_wake_word(self, text):
        """Check if text contains wake word"""
        if not text:
            return False
            
        text_cleaned = text.lower().replace(',', '').replace('.', '').strip()
        
        for wake_word in WAKE_WORDS:
            wake_word_cleaned = wake_word.lower().strip()
            if wake_word_cleaned in text_cleaned:
                print(f"Wake word detected: '{wake_word}' in '{text}'")
                return True
        return False
    
    def record_with_silence_detection(self):
        """Record audio until silence detected"""
        try:
            with self.audio_recording_lock:
                print("Recording audio...")
                audio_data = []
                silence_duration = 0
                recording_duration = 0
                check_interval = 0.2
                samples_per_check = int(SAMPLE_RATE * check_interval)
                
                def audio_callback(indata, frames, time, status):
                    if status:
                        print(f"Audio callback status: {status}")
                    audio_data.extend(indata[:, 0])
                
                with sd.InputStream(callback=audio_callback, 
                                  samplerate=SAMPLE_RATE, 
                                  channels=CHANNELS,
                                  dtype='float32'):
                    
                    while recording_duration < MAX_RECORDING_DURATION:
                        time.sleep(check_interval)
                        recording_duration += check_interval
                        
                        if len(audio_data) >= samples_per_check:
                            recent_audio = np.array(audio_data[-samples_per_check:])
                            rms = np.sqrt(np.mean(recent_audio**2))
                            
                            if rms < SILENCE_THRESHOLD:
                                silence_duration += check_interval
                                if silence_duration >= MIN_SILENCE_DURATION:
                                    print(f"Silence detected after {recording_duration:.1f}s")
                                    break
                            else:
                                silence_duration = 0
                
                if audio_data:
                    audio_array = np.array(audio_data, dtype=np.float32)
                    sf.write(WAV_FILENAME, audio_array, SAMPLE_RATE)
                    print(f"Audio saved: {len(audio_array)/SAMPLE_RATE:.1f}s duration")
                    return True
                
                return False
                
        except Exception as e:
            print(f"Recording error: {e}")
            return False
    
    def record_wake_word_check(self):
        """Record short audio clip for wake word detection"""
        try:
            if not self.audio_recording_lock.acquire(blocking=False):
                return False  # Audio system is busy
            
            try:
                # Record 5 seconds of audio for wake word detection
                print("Listening for wake word...")
                audio_data = sd.rec(int(5 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
                sd.wait()
                
                # Check if audio contains sound above threshold
                rms = np.sqrt(np.mean(audio_data**2))
                if rms > SILENCE_THRESHOLD * 2:  # Higher threshold for wake word
                    sf.write(WAKE_WORD_AUDIO, audio_data, SAMPLE_RATE)
                    print(f"Wake word audio saved, RMS: {rms:.4f}")
                    return True
                else:
                    print(f"Audio too quiet for wake word, RMS: {rms:.4f}")
                    return False
                    
            finally:
                self.audio_recording_lock.release()
                
        except Exception as e:
            print(f"Wake word recording error: {e}")
            if self.audio_recording_lock.locked():
                self.audio_recording_lock.release()
            return False
    
    def is_command(self, text):
        """Check if text is a command"""
        text_lower = text.lower().strip()
        for command in COMMANDS.keys():
            if command in text_lower:
                return command
        return None
    
    def execute_command(self, command):
        """Execute system command"""
        if command == "flush the toilet":
            response = f"Oh {self.current_person}, you know I am a digital assistant. I cannot actually flush toilets! So why dont you haul your lazy butt up off the couch and flush the toilet yourself!"
        elif command == "turn on the lights":
            response = "I would turn on the lights if I was connected to a smart home system."
        elif command == "turn off the lights":
            response = "I would turn off the lights if I was connected to a smart home system."
        elif command == "play music":
            response = "I would start playing music if I had access to a music system."
        elif command == "stop music":
            response = "I would stop the music if any music was playing."
        elif command == "who is sponsoring this video":
            self.play_laughing()
            response = f"You are very funny {self.current_person}. You know you dont have any sponsors for your videos!"
        elif command == "how is the weather today":
            response = f"O M G {self.current_person}! Surely you DO NOT want to waste my valuable resources by asking me what the weather is today. Cant you just look out the window or ask Siri. That is about all Siri is good for!"
        elif command == "what time is it":
            import datetime
            current_time = datetime.datetime.now().strftime("%I:%M %p")
            response = f"The current time is {current_time}"
        elif command == "shutdown system":
            response = "I would shutdown the system, but I will skip that for safety reasons during testing."
        elif command == "reboot system":
            response = "I would reboot the system, but I will skip that for safety reasons during testing."
        else:
            response = f"I understand you want me to {command}, but I dont have that capability yet."
        
        return response
    
    def query_llama(self, prompt):
        """Generate LLM response"""
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            result = self.llama_model(formatted_prompt, max_tokens=100)
            if "choices" in result and result["choices"]:
                reply_text = result["choices"][0]["text"].strip()
                reply_text = re.sub(r"\(.*?\)", "", reply_text)
                reply_text = re.sub(r"(User:|Assistant:)", "", reply_text)
                reply_text = reply_text.strip()
                
                sentences = reply_text.split('.')
                if len(sentences) > 3:
                    reply_text = '. '.join(sentences[:3]) + '.'
                
                return reply_text
            else:
                return "I'm not sure how to answer that."
        except Exception as e:
            print(f"LLM error: {e}")
            return "Sorry, I had trouble processing that question."
    
    def process_user_input(self, text):
        """Process user input"""
        print(f"Processing user input: '{text}'")
        command = self.is_command(text)
        if command:
            print(f"Executing command: {command}")
            response = self.execute_command(command)
        else:
            print("Generating LLM response")
            response = self.query_llama(text)
        
        return response
    
    def listen_for_wake_word(self):
        """Listen for wake words in background"""
        print("Wake word detection thread started")
        
        while self.is_running:
            try:
                # Only listen if someone is present and wake word detection is active
                if self.current_person and self.current_person != "Unknown" and self.wake_word_active:
                    # Check for bored response first
                    if self.check_for_bored_response(self.current_person):
                        # Bored response was given, continue to next iteration
                        time.sleep(WAKE_WORD_CHECK_INTERVAL)
                        continue
                    
                    print("Checking for wake word...")
                    
                    # Record audio for wake word detection
                    if self.record_wake_word_check():
                        # Transcribe and check for wake word
                        transcript = self.transcribe_audio(WAKE_WORD_AUDIO)
                        
                        if transcript and self.detect_wake_word(transcript):
                            print("WAKE WORD DETECTED! Starting conversation...")
                            self.play_beep()
                            
                            # Speak generic listening response with person's name
                            if self.listening_responses:
                                listening_template = random.choice(self.listening_responses)
                                listening_response = listening_template.replace("{name}", self.current_person)
                            else:
                                listening_response = f"Yes {self.current_person}, I'm listening. What would you like to know?"
                            
                            self.speak_text(listening_response)
                            
                            # Record full request
                            print("Please speak your request...")
                            if self.record_with_silence_detection():
                                user_text = self.transcribe_audio(WAV_FILENAME)
                                if user_text and len(user_text.strip()) > 2:
                                    print(f"User said: '{user_text}'")
                                    response = self.process_user_input(user_text)
                                    print(f"Response: '{response}'")
                                    self.speak_text(response)
                                    self.last_interaction_time = time.time()
                                    # Reset bored response timer only after successful interaction
                                    self.last_bored_response_time = time.time()
                                else:
                                    print("No clear speech detected")
                                    self.speak_text("I didn't catch that. Could you repeat your request?")
                            else:
                                print("Failed to record user request")
                                self.speak_text("I'm having trouble hearing you. Please try again.")
                    
                    time.sleep(WAKE_WORD_CHECK_INTERVAL)
                else:
                    # No one present or wake word not active, sleep longer
                    time.sleep(2.0)
                
            except Exception as e:
                print(f"Wake word detection error: {e}")
                time.sleep(2.0)
        
        print("Wake word detection thread stopped")
    
    def camera_monitoring_loop(self):
        """Main camera monitoring loop with OpenCV display"""
        cv2.namedWindow('Chatty AI - Facial Recognition', cv2.WINDOW_AUTOSIZE)
        
        while self.is_running:
            try:
                frame = self.picam2.capture_array()
                
                # Convert from RGB to BGR for OpenCV
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                
                # Process facial recognition
                name, face_location, confidence = self.detect_faces(frame)
                
                current_time = time.time()
                
                # Draw face rectangles and labels on frame
                if name and face_location:
                    top, right, bottom, left = face_location
                    
                    # Choose color based on recognition
                    if name == "Unknown":
                        color = (0, 0, 255)  # Red for unknown
                        label = f"Unknown ({confidence:.2f})"
                    else:
                        color = (0, 255, 0)  # Green for known
                        label = f"{name} ({confidence:.2f})"
                    
                    # Draw rectangle around face
                    cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
                    
                    # Draw label background
                    cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)
                    
                    # Draw label text
                    cv2.putText(frame, label, (left + 6, bottom - 6),
                               cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                
                # Add status information to frame
                status_text = "Chatty AI Active - Press ESC to exit"
                cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # Show current person status
                if self.current_person:
                    person_text = f"Current Person: {self.current_person}"
                    cv2.putText(frame, person_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # Show wake word status
                if self.wake_word_active:
                    wake_word_text = "Wake Word Detection: ACTIVE"
                    cv2.putText(frame, wake_word_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                else:
                    wake_word_text = "Wake Word Detection: INACTIVE"
                    cv2.putText(frame, wake_word_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                
                # Show bored response timer
                if self.last_bored_response_time and self.wake_word_active:
                    time_since_bored = current_time - self.last_bored_response_time
                    timer_text = f"Bored Timer: {int(BORED_RESPONSE_INTERVAL - time_since_bored)}s"
                    cv2.putText(frame, timer_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                # Display the frame
                cv2.imshow('Chatty AI - Facial Recognition', frame)
                
                # Check for key press
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC key
                    print("\n[INFO] ESC key pressed. Shutting down...")
                    self.is_running = False
                    break
                elif key == ord('q'):  # Q key as alternative
                    print("\n[INFO] Q key pressed. Shutting down...")
                    self.is_running = False
                    break
                
                # Process facial recognition logic
                if name and face_location:
                    # Person detected
                    if name != self.current_person:
                        # New person or person changed
                        self.current_person = name
                        self.person_absent_since = None
                        self.wake_word_active = False  # Reset wake word state
                        
                        if name == "Unknown":
                            self.handle_unknown_person(frame, confidence)
                        else:
                            # Save photo and send telegram alert for known person
                            photo_path = self.save_security_photo(frame, name, confidence)
                            self.send_telegram_alert(name, confidence, photo_path)
                            
                            # Greet known person (this will activate wake word detection)
                            self.greet_person(name)
                
                else:
                    # No person detected
                    if self.current_person:
                        if not self.person_absent_since:
                            self.person_absent_since = current_time
                        elif current_time - self.person_absent_since >= GREETING_COOLDOWN:
                            # Person has been absent for 5+ minutes, reset
                            self.current_person = None
                            self.person_absent_since = None
                            self.last_interaction_time = None
                            self.wake_word_active = False
                            self.last_bored_response_time = None  # Reset bored response timer
                            self.bored_cycle = 0  # Reset bored cycle
                            print("Person left - resetting state")
                
                time.sleep(PERSON_DETECTION_INTERVAL)
                
            except Exception as e:
                print(f"Camera loop error: {e}")
                time.sleep(1)
        
        # Clean up OpenCV windows
        cv2.destroyAllWindows()
    
    def run(self):
        """Main run loop"""
        if not self.whisper_model or not self.llama_model or not self.picam2:
            print("Required components not initialized")
            return
        
        print("🚀 Chatty AI Complete System Started!")
        print("=" * 60)
        print("Features active:")
        print("• Facial Recognition with Generic Responses")
        print("• Wake Word Detection")
        print("• AI Assistant (TinyLLaMA)")
        print("• Security Monitoring with Time-Based Responses")
        print("• Telegram Alerts")
        print("• LLM-Generated Entertainment (Jokes & Fun Facts)")
        print("=" * 60)
        print("Press ESC key to exit")
        print("\nDEBUG INFO:")
        print(f"• Wake words: {len(WAKE_WORDS)} phrases loaded")
        print(f"• Bored response interval: {BORED_RESPONSE_INTERVAL} seconds")
        print(f"• Audio sample rate: {SAMPLE_RATE} Hz")
        print(f"• Silence threshold: {SILENCE_THRESHOLD}")
        print(f"• Daytime visitor hours: 6:00AM - 12:00PM")
        print(f"• Intruder detection hours: 12:01PM - 5:59AM")
        print("=" * 60)
        
        self.is_running = True
        
        # Start wake word detection in background thread
        self.audio_thread = threading.Thread(target=self.listen_for_wake_word, daemon=True)
        self.audio_thread.start()
        
        try:
            # Run camera monitoring loop (this will handle the display and ESC key)
            self.camera_monitoring_loop()
                
        except KeyboardInterrupt:
            print("\nShutting down Chatty AI...")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """Clean up resources"""
        print("Cleaning up resources...")
        self.is_running = False
        
        # Wait for audio thread to finish
        if self.audio_thread and self.audio_thread.is_alive():
            print("Waiting for audio thread to stop...")
            self.audio_thread.join(timeout=3)
        
        if self.picam2:
            try:
                self.picam2.stop()
                print("Camera stopped")
            except:
                pass
        
        cv2.destroyAllWindows()
        
        # Clean up audio files
        for audio_file in [WAV_FILENAME, RESPONSE_AUDIO, WAKE_WORD_AUDIO]:
            try:
                if os.path.exists(audio_file):
                    os.remove(audio_file)
            except:
                pass
        
        print("Chatty AI shutdown complete")

def main():
    """Main function"""
    print("Chatty AI - Your smart AI Assistant System")
    print("=" * 60)
    
    # Check audio devices
    try:
        print("Available audio devices:")
        print(sd.query_devices())
        print("=" * 60)
    except Exception as e:
        print(f"Could not query audio devices: {e}")
    
    chatty = ChattyAI()
    chatty.run()

if __name__ == "__main__":
    main()


I also have a previous release version of my chatty_ai.py Python script named app.py to run as a Web UI AI Assistant.

app.py
#!/usr/bin/env python3
"""
app.py - Chatty AI Web Application
Flask web interface for the AI Assistant with video streaming
"""

import os
import subprocess
import sounddevice as sd
import soundfile as sf
import numpy as np
import threading
import time
import random
import re
import cv2
import face_recognition
import pickle
import json
import requests
import logging
from datetime import datetime, timedelta
from faster_whisper import WhisperModel
from llama_cpp import Llama
from flask import Flask, render_template, Response, jsonify, request
from flask_socketio import SocketIO, emit
import base64
import io
from PIL import Image

# Flask application setup
app = Flask(__name__, 
            template_folder='/home/nickspi5/Chatty_AI/templates',
            static_folder='/home/nickspi5/Chatty_AI/templates')
app.config['SECRET_KEY'] = 'chatty_ai_secret_key_2025'
socketio = SocketIO(app, cors_allowed_origins="*", logger=False, engineio_logger=False)

# Configuration
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
BEEP_SOUND = "/home/nickspi5/Chatty_AI/audio_files/beep.wav"
LAUGHING_SOUND = "/home/nickspi5/Chatty_AI/audio_files/laughing.wav"
ENCODINGS_FILE = "encodings.pickle"
TELEGRAM_CONFIG_FILE = "telegram_config.json"

# Audio files
WAV_FILENAME = "user_input.wav"
RESPONSE_AUDIO = "output.wav"
WAKE_WORD_AUDIO = "wake_word_check.wav"

# Security directories
SECURITY_PHOTOS_DIR = "/home/nickspi5/Chatty_AI/security_photos"
SECURITY_LOGS_DIR = "/home/nickspi5/Chatty_AI/security_logs"

# Response files
JOKES_FILE = "jokes.txt"
LISTENING_RESPONSES_FILE = "listening_responses.txt"
WAITING_RESPONSES_FILE = "waiting_responses.txt"
WARNING_RESPONSES_FILE = "warning_responses.txt"
GREETING_RESPONSES_FILE = "greeting_responses.txt"
PERSONALIZED_RESPONSES_FILE = "personalized_responses.json"

# Wake words and commands
WAKE_WORDS = [
    "are you awake", "are you alive", "hey chatty", "hello chatty", "sup chatty",
    "sub-chatty", "how's it chatty", "howzit chatty", "hi chatty", "yo chatty",
    "hey chuddy", "hello chuddy", "sup chuddy", "sub-chuddy", "how's it chuddy",
    "howzit chuddy", "hi chuddy", "yo chuddy", "hey cheddy", "hello cheddy",
    "sup cheddy", "sub-cheddy", "how's it cheddy", "howzit cheddy", "hi cheddy",
    "yo cheddy", "hey chetty", "hello chetty", "sup chetty", "sub-chetty",
    "how's it chetty", "howzit chetty", "hi chetty", "yo chetty", "hey cherry",
    "hello cherry", "sup cherry", "sub-cherry", "how's it cherry", "howzit cherry",
    "hi cherry", "yo cherry"
]

COMMANDS = {
    "flush the toilet": "toilet_flush",
    "turn on the lights": "lights_on",
    "turn off the lights": "lights_off",
    "play music": "play_music",
    "stop music": "stop_music",
    "what time is it": "get_time",
    "shutdown system": "shutdown_system",
    "who is sponsoring this video": "who_is_sponsoring_this_video",
    "how is the weather today": "how_is_the_weather_today",
    "reboot system": "reboot_system"
}

# Audio parameters
SAMPLE_RATE = 16000
CHANNELS = 1
SILENCE_THRESHOLD = 0.035
MIN_SILENCE_DURATION = 1.5
MAX_RECORDING_DURATION = 30

# Timing parameters
GREETING_COOLDOWN = 300
WAITING_INTERVAL = 30
PERSON_DETECTION_INTERVAL = 0.5
WAKE_WORD_CHECK_INTERVAL = 1.0

class ChattyAIWeb:
    def __init__(self):
        print("🔧 Initializing ChattyAI instance...")
        
        # AI Models
        self.whisper_model = None
        self.llama_model = None
        
        # Facial Recognition
        self.known_encodings = []
        self.known_names = []
        
        # Camera
        self.picam2 = None
        self.camera_initialized = False
        
        # State variables
        self.is_running = False
        self.current_person = None
        self.last_greeting_time = {}
        self.last_interaction_time = None
        self.person_absent_since = None
        self.waiting_cycle = 0
        self.last_bored_response_time = None
        self.bored_cycle = 0
        self.audio_recording_lock = threading.Lock()
        self.wake_word_active = False
        
        # Web-specific variables
        self.current_frame = None
        self.current_detected_person = None
        self.current_confidence = 0.0
        self.captured_image = None
        
        # Response lists
        self.jokes = []
        self.listening_responses = []
        self.waiting_responses = []
        self.warning_responses = []
        self.greeting_responses = []
        self.personalized_responses = {}
        
        # Telegram
        self.telegram_token = None
        self.telegram_chat_id = None
        
        # Threading
        self.camera_thread = None
        self.audio_thread = None
        
        # Initialize basic components
        self.setup_directories()
        self.setup_logging()
        self.load_response_files()
        self.load_personalized_responses()
        self.load_telegram_config()
        
        # Try to initialize camera immediately for web display
        self.try_initialize_camera()
        
        print("ChattyAIWeb instance created - models will be loaded when system starts")

    def emit_log(self, message, log_type="info"):
        """Emit log message to web interface"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")
        try:
            socketio.emit('log_update', {
                'timestamp': timestamp,
                'message': message,
                'type': log_type
            })
        except Exception as e:
            print(f"Socket emit error: {e}")

    def emit_conversation(self, message, msg_type="info"):
        """Emit conversation message to web interface"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] CONVERSATION: {message}")
        try:
            socketio.emit('conversation_update', {
                'timestamp': timestamp,
                'message': message,
                'type': msg_type
            })
        except Exception as e:
            print(f"Socket emit error: {e}")

    def setup_directories(self):
        """Create necessary directories"""
        os.makedirs(SECURITY_PHOTOS_DIR, exist_ok=True)
        os.makedirs(SECURITY_LOGS_DIR, exist_ok=True)

    def setup_logging(self):
        """Setup logging for detections"""
        log_file = os.path.join(SECURITY_LOGS_DIR, "chatty_ai.log")
        self.logger = logging.getLogger('chatty_ai')
        self.logger.setLevel(logging.INFO)
        
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
            
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

    def load_response_files(self):
        """Load response text files"""
        try:
            with open(JOKES_FILE, 'r') as f:
                self.jokes = [line.strip() for line in f if line.strip()]
            with open(LISTENING_RESPONSES_FILE, 'r') as f:
                self.listening_responses = [line.strip() for line in f if line.strip()]
            with open(GREETING_RESPONSES_FILE, 'r') as f:
                self.greeting_responses = [line.strip() for line in f if line.strip()]
            with open(WAITING_RESPONSES_FILE, 'r') as f:
                self.waiting_responses = [line.strip() for line in f if line.strip()]
            with open(WARNING_RESPONSES_FILE, 'r') as f:
                self.warning_responses = [line.strip() for line in f if line.strip()]
            self.emit_log("Response files loaded successfully")
        except FileNotFoundError as e:
            self.emit_log(f"Response file not found: {e}", "error")
            self.create_default_responses()

    def load_personalized_responses(self):
        """Load personalized responses from JSON file"""
        try:
            with open(PERSONALIZED_RESPONSES_FILE, 'r') as f:
                self.personalized_responses = json.load(f)
            self.emit_log(f"Personalized responses loaded for {len(self.personalized_responses)} people")
        except FileNotFoundError:
            self.emit_log("Personalized responses file not found. Creating default...", "warning")
            self.create_default_personalized_responses()
        except json.JSONDecodeError as e:
            self.emit_log(f"Error reading personalized responses JSON: {e}", "error")
            self.create_default_personalized_responses()

    def create_default_personalized_responses(self):
        """Create default personalized responses file"""
        default_responses = {
            "Nick": {
                "greetings": [
                    "Hello Nick, my master! It is so lovely to see you again. Thank you for creating me. How may I assist you?",
                    "Welcome back Nick! Your brilliant creation is ready to serve. What can I help you with today?",
                    "Nick! My creator has returned! I've been waiting patiently for your commands."
                ],
                "listening": [
                    "Yes Nick, I'm listening. What would you like to know?",
                    "I'm all ears, Nick. What's on your mind?",
                    "Ready for your instructions, Nick!"
                ],
                "waiting": [
                    "Nick, I'm still here if you need anything",
                    "Your faithful AI assistant is standing by, Nick",
                    "Still waiting to help you, Nick"
                ]
            }
        }
        
        try:
            with open(PERSONALIZED_RESPONSES_FILE, 'w') as f:
                json.dump(default_responses, f, indent=4)
            self.personalized_responses = default_responses
            self.emit_log(f"Created default personalized responses file: {PERSONALIZED_RESPONSES_FILE}")
        except Exception as e:
            self.emit_log(f"Failed to create personalized responses file: {e}", "error")
            self.personalized_responses = default_responses

    def create_default_responses(self):
        """Create default responses if files are missing"""
        self.jokes = ["Why don't scientists trust atoms? Because they make up everything!"]
        self.greeting_responses = ["Hello! How can I help you today?"]
        self.listening_responses = ["I'm listening, what would you like to know?"]
        self.waiting_responses = ["I'm still here if you need anything"]
        self.warning_responses = ["Warning: Unknown person detected. Please identify yourself."]

    def load_models(self):
        """Load AI models"""
        self.emit_log("Loading AI models...")
        try:
            self.whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device="cpu", compute_type="int8")
            self.emit_log("Whisper model loaded")
        except Exception as e:
            self.emit_log(f"Failed to load Whisper: {e}", "error")
            return False

        try:
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH,
                n_ctx=2048,
                temperature=0.7,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                verbose=False
            )
            self.emit_log("LLaMA model loaded")
        except Exception as e:
            self.emit_log(f"Failed to load LLaMA: {e}", "error")
            return False
        return True

    def load_encodings(self):
        """Load facial recognition encodings"""
        try:
            with open(ENCODINGS_FILE, "rb") as f:
                data = pickle.loads(f.read())
            self.known_encodings = data["encodings"]
            self.known_names = data["names"]
            self.emit_log(f"Loaded {len(self.known_encodings)} face encodings")
            return True
        except FileNotFoundError:
            self.emit_log(f"Encodings file '{ENCODINGS_FILE}' not found!", "error")
            return False
        except Exception as e:
            self.emit_log(f"Failed to load encodings: {e}", "error")
            return False

    def load_telegram_config(self):
        """Load Telegram configuration"""
        try:
            with open(TELEGRAM_CONFIG_FILE, 'r') as f:
                config = json.load(f)
            self.telegram_token = config.get('bot_token')
            self.telegram_chat_id = config.get('chat_id')
            self.emit_log("Telegram configuration loaded")
        except FileNotFoundError:
            self.emit_log("Telegram config not found - alerts disabled", "warning")
        except Exception as e:
            self.emit_log(f"Failed to load Telegram config: {e}", "error")

    def try_initialize_camera(self):
        """Try to initialize camera for web display"""
        try:
            from picamera2 import Picamera2
            self.emit_log("Attempting to initialize camera for web display...")
            
            self.picam2 = Picamera2()
            config = self.picam2.create_preview_configuration(
                main={"format": 'XRGB8888', "size": (640, 480)}
            )
            self.picam2.configure(config)
            self.picam2.start()
            time.sleep(2)
            
            # Test capture
            test_frame = self.picam2.capture_array()
            self.camera_initialized = True
            self.emit_log(f"Camera initialized for web display - Frame shape: {test_frame.shape}")
            return True
            
        except Exception as e:
            self.emit_log(f"Camera initialization for web display failed: {e}", "warning")
            self.camera_initialized = False
            if self.picam2:
                try:
                    self.picam2.stop()
                    self.picam2.close()
                except:
                    pass
                self.picam2 = None
            return False

    def setup_camera(self):
        """Initialize camera with retry logic for full system"""
        if self.camera_initialized:
            self.emit_log("Camera already initialized")
            return True
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                from picamera2 import Picamera2
                
                self.emit_log(f"Camera initialization attempt {attempt + 1}/{max_retries}")
                
                # Close any existing camera instance
                if self.picam2:
                    try:
                        self.picam2.stop()
                        self.picam2.close()
                    except:
                        pass
                    self.picam2 = None
                    time.sleep(2)
                
                # Create new camera instance
                self.picam2 = Picamera2()
                config = self.picam2.create_preview_configuration(
                    main={"format": 'XRGB8888', "size": (640, 480)}
                )
                self.picam2.configure(config)
                self.picam2.start()
                time.sleep(2)
                
                # Test capture
                test_frame = self.picam2.capture_array()
                self.camera_initialized = True
                self.emit_log(f"Camera initialized successfully - Frame shape: {test_frame.shape}")
                return True
                
            except Exception as e:
                self.emit_log(f"Camera initialization attempt {attempt + 1} failed: {e}", "error")
                if self.picam2:
                    try:
                        self.picam2.stop()
                        self.picam2.close()
                    except:
                        pass
                    self.picam2 = None
                
                if attempt < max_retries - 1:
                    time.sleep(3)
                else:
                    self.emit_log("All camera initialization attempts failed", "error")
                    self.camera_initialized = False
                    return False
        
        return False

    def detect_faces(self, frame):
        """Detect and recognize faces in frame"""
        try:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            face_locations = face_recognition.face_locations(rgb_frame, model="hog")
            
            if len(face_locations) == 0:
                return None, None, 0.0

            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
            best_name = "Unknown"
            best_confidence = 0.0

            for face_encoding in face_encodings:
                matches = face_recognition.compare_faces(self.known_encodings, face_encoding, tolerance=0.6)
                if True in matches:
                    face_distances = face_recognition.face_distance(self.known_encodings, face_encoding)
                    best_match_index = face_distances.argmin()
                    confidence = 1.0 - face_distances[best_match_index]
                    
                    if matches[best_match_index] and confidence > 0.4:
                        if confidence > best_confidence:
                            best_name = self.known_names[best_match_index]
                            best_confidence = confidence

            return best_name, face_locations[0], best_confidence
        except Exception as e:
            self.emit_log(f"Face detection error: {e}", "error")
            return None, None, 0.0

    def generate_video_feed(self):
        """Generate video frames for streaming"""
        print("Video feed generation started")
        
        frame_count = 0
        while True:  # Keep generating frames continuously
            try:
                frame_count += 1
                
                if self.picam2 and self.camera_initialized:
                    frame = self.picam2.capture_array()
                    
                    # Convert from RGB/RGBA to BGR for OpenCV
                    if len(frame.shape) == 3:
                        if frame.shape[2] == 4:  # RGBA
                            frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)
                        elif frame.shape[2] == 3:  # RGB
                            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                    # Only process facial recognition if system is running
                    if self.is_running and self.known_encodings:
                        name, face_location, confidence = self.detect_faces(frame)
                        
                        # Draw face rectangles and labels
                        if name and face_location:
                            top, right, bottom, left = face_location
                            
                            # Choose color based on recognition
                            if name == "Unknown":
                                color = (0, 0, 255)  # Red
                                label = f"Unknown ({confidence:.2f})"
                            else:
                                color = (0, 255, 0)  # Green
                                label = f"{name} ({confidence:.2f})"
                            
                            # Draw rectangle and label
                            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
                            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)
                            cv2.putText(frame, label, (left + 6, bottom - 6),
                                      cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                            
                            # Capture image for display
                            face_img = frame[top:bottom, left:right].copy()
                            if face_img.size > 0:
                                self.captured_image = cv2.resize(face_img, (200, 200))
                            
                            # Emit person detection update
                            try:
                                socketio.emit('person_detected', {
                                    'name': name,
                                    'confidence': f"{confidence:.1%}",
                                    'timestamp': datetime.now().strftime("%H:%M:%S")
                                })
                            except:
                                pass
                    
                    # Add status overlay to frame
                    if self.is_running:
                        status_text = "Chatty AI Active"
                        cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    else:
                        status_text = "System Ready - Click Start"
                        cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                    
                    if self.current_person:
                        person_text = f"Current: {self.current_person}"
                        cv2.putText(frame, person_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                    if self.wake_word_active:
                        wake_text = "Wake Word: ACTIVE"
                        cv2.putText(frame, wake_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    else:
                        wake_text = "Wake Word: INACTIVE"
                        cv2.putText(frame, wake_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)
                
                else:  # No camera available
                    # Create a black frame with error message
                    frame = np.zeros((480, 640, 3), dtype=np.uint8)
                    cv2.putText(frame, "Camera Not Available", (200, 220), 
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                    cv2.putText(frame, "Check camera connection", (150, 260), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
                
                # Encode frame as JPEG
                ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                
                time.sleep(0.05)  # 20 FPS
                
            except Exception as e:
                print(f"Video feed error: {e}")
                # Create error frame
                error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                cv2.putText(error_frame, f"Video Error", (200, 220), 
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                cv2.putText(error_frame, f"{str(e)[:40]}", (100, 260), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)
                ret, buffer = cv2.imencode('.jpg', error_frame)
                if ret:
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
                time.sleep(1)

    def start_system(self):
        """Start the AI system"""
        self.emit_log("Starting Chatty AI System...")
        
        # Load models first if not already loaded
        if not self.whisper_model or not self.llama_model:
            self.emit_log("Loading AI models...", "info")
            if not self.load_models():
                self.emit_log("Failed to load AI models", "error")
                return False
        
        if not self.camera_initialized:
            self.emit_log("Initializing camera for full system...", "info")
            if not self.setup_camera():
                self.emit_log("Warning: Camera initialization failed - continuing without full AI features", "warning")
        
        if not self.known_encodings:
            self.emit_log("Loading facial recognition encodings...", "info")
            if not self.load_encodings():
                self.emit_log("Warning: No facial recognition data - continuing without person recognition", "warning")
        
        self.emit_log("Chatty AI System Started!")
        self.is_running = True
        
        # Start audio thread only if we have the required models
        if self.whisper_model:
            self.audio_thread = threading.Thread(target=self.listen_for_wake_word, daemon=True)
            self.audio_thread.start()
            self.emit_log("Audio processing thread started")
        
        # Start camera monitoring thread only if camera and encodings are available
        if self.camera_initialized and self.known_encodings:
            self.camera_thread = threading.Thread(target=self.camera_monitoring_thread, daemon=True)
            self.camera_thread.start()
            self.emit_log("Camera monitoring thread started")
        
        return True

    def stop_system(self):
        """Stop the AI system"""
        self.is_running = False
        self.emit_log("Chatty AI System Stopped")

    # ... [Rest of the methods remain the same - continuing from where we left off]

    def get_personalized_response(self, person_name, response_type, fallback_list=None):
        """Get a personalized response for a specific person and response type"""
        person_name_lower = person_name.lower()
        
        person_data = None
        for name, data in self.personalized_responses.items():
            if name.lower() == person_name_lower:
                person_data = data
                break
        
        if person_data and response_type in person_data:
            responses = person_data[response_type]
            if responses:
                return random.choice(responses)
        
        if fallback_list:
            response = random.choice(fallback_list)
            if "{name}" in response:
                return response.replace("{name}", person_name)
            else:
                return f"Hello {person_name}! {response}"
        
        return f"Hello {person_name}! How can I help you today?"

    def speak_text(self, text):
        """Convert text to speech using Piper"""
        try:
            with self.audio_recording_lock:
                command = [
                    PIPER_EXECUTABLE,
                    "--model", VOICE_PATH,
                    "--config", CONFIG_PATH,
                    "--output_file", RESPONSE_AUDIO
                ]
                subprocess.run(command, input=text.encode("utf-8"), check=True, capture_output=True)
                subprocess.run(["aplay", RESPONSE_AUDIO], check=True, capture_output=True)
                
                # Emit to web interface
                self.emit_conversation(f"🔊 Speaking: {text}", "speech")
        except subprocess.CalledProcessError as e:
            self.emit_log(f"TTS failed: {e}", "error")

    def play_beep(self):
        """Play beep sound"""
        try:
            with self.audio_recording_lock:
                subprocess.run(["aplay", BEEP_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            pass

    def play_laughing(self):
        """Play laughing sound"""
        try:
            with self.audio_recording_lock:
                subprocess.run(["aplay", LAUGHING_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            pass

    def save_security_photo(self, frame, person_name, confidence):
        """Save security photo with timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{person_name.lower()}_{timestamp}.jpg"
        filepath = os.path.join(SECURITY_PHOTOS_DIR, filename)
        
        overlay_frame = frame.copy()
        cv2.rectangle(overlay_frame, (10, 10), (500, 100), (0, 0, 0), -1)
        cv2.rectangle(overlay_frame, (10, 10), (500, 100), (255, 255, 255), 2)
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(overlay_frame, f"Person: {person_name}", (20, 35), font, 0.7, (255, 255, 255), 2)
        cv2.putText(overlay_frame, f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", (20, 60), font, 0.6, (255, 255, 255), 2)
        if person_name != "Unknown":
            cv2.putText(overlay_frame, f"Confidence: {confidence:.1%}", (20, 85), font, 0.6, (255, 255, 255), 2)
        
        cv2.imwrite(filepath, overlay_frame)
        self.logger.info(f"Security photo saved: {filename} | Person: {person_name} | Confidence: {confidence:.2f}")
        return filepath

    def send_telegram_alert(self, person_name, confidence, photo_path):
        """Send Telegram alert"""
        if not self.telegram_token or not self.telegram_chat_id:
            return False
        
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            if person_name == "Unknown":
                message = f"**UNKNOWN PERSON DETECTED**\n\n**Time:** {timestamp}\n**Status:** Unregistered Person\n**Action:** Photo captured for review"
            else:
                message = f"**AUTHORIZED ACCESS**\n\n**Person:** {person_name}\n**Time:** {timestamp}\n**Confidence:** {confidence:.1%}\n**Status:** Registered User"
            
            url = f"https://api.telegram.org/bot{self.telegram_token}/sendPhoto"
            with open(photo_path, 'rb') as photo:
                files = {'photo': photo}
                data = {
                    'chat_id': self.telegram_chat_id,
                    'caption': message,
                    'parse_mode': 'Markdown'
                }
                response = requests.post(url, data=data, files=files, timeout=30)
                return response.status_code == 200
        except Exception as e:
            self.emit_log(f"Telegram alert failed: {e}", "error")
            return False

    def greet_person(self, name):
        """Greet a detected person with personalized greeting"""
        current_time = time.time()
        
        if name in self.last_greeting_time:
            time_since_last = current_time - self.last_greeting_time[name]
            if time_since_last < GREETING_COOLDOWN:
                return False
        
        greeting = self.get_personalized_response(name, "greetings", self.greeting_responses)
        self.speak_text(greeting)
        self.last_greeting_time[name] = current_time
        self.last_interaction_time = current_time
        
        self.wake_word_active = True
        self.last_bored_response_time = current_time
        self.bored_cycle = 0
        
        self.emit_log(f"Greeted {name} with personalized message - Wake word detection now active")
        return True

    def handle_unknown_person(self, frame, confidence):
        """Handle unknown person detection"""
        warning = random.choice(self.warning_responses) if self.warning_responses else "Warning: Unknown person detected."
        self.speak_text(warning)
        photo_path = self.save_security_photo(frame, "Unknown", confidence)
        self.send_telegram_alert("Unknown", confidence, photo_path)
        self.emit_log("Unknown person detected and warned", "warning")

    def transcribe_audio(self, filename):
        """Transcribe audio using Whisper"""
        try:
            if not os.path.exists(filename):
                return ""
            segments, _ = self.whisper_model.transcribe(filename)
            transcript = " ".join(segment.text for segment in segments).strip()
            self.emit_log(f"Transcription: '{transcript}'")
            return transcript
        except Exception as e:
            self.emit_log(f"Transcription error: {e}", "error")
            return ""

    def detect_wake_word(self, text):
        """Check if text contains wake word"""
        if not text:
            return False
        text_cleaned = text.lower().replace(',', '').replace('.', '').strip()
        for wake_word in WAKE_WORDS:
            wake_word_cleaned = wake_word.lower().strip()
            if wake_word_cleaned in text_cleaned:
                self.emit_conversation(f"Wake word detected: '{wake_word}' in '{text}'", "wake_word")
                return True
        return False

    def record_with_silence_detection(self):
        """Record audio until silence detected"""
        try:
            with self.audio_recording_lock:
                self.emit_log("Recording audio...")
                audio_data = []
                silence_duration = 0
                recording_duration = 0
                check_interval = 0.2
                samples_per_check = int(SAMPLE_RATE * check_interval)
                
                def audio_callback(indata, frames, time, status):
                    if status:
                        self.emit_log(f"Audio callback status: {status}", "warning")
                    audio_data.extend(indata[:, 0])
                
                with sd.InputStream(callback=audio_callback,
                                  samplerate=SAMPLE_RATE,
                                  channels=CHANNELS,
                                  dtype='float32'):
                    while recording_duration < MAX_RECORDING_DURATION:
                        time.sleep(check_interval)
                        recording_duration += check_interval
                        
                        if len(audio_data) >= samples_per_check:
                            recent_audio = np.array(audio_data[-samples_per_check:])
                            rms = np.sqrt(np.mean(recent_audio**2))
                            
                            if rms < SILENCE_THRESHOLD:
                                silence_duration += check_interval
                                if silence_duration >= MIN_SILENCE_DURATION:
                                    self.emit_log(f"Silence detected after {recording_duration:.1f}s")
                                    break
                            else:
                                silence_duration = 0
                
                if audio_data:
                    audio_array = np.array(audio_data, dtype=np.float32)
                    sf.write(WAV_FILENAME, audio_array, SAMPLE_RATE)
                    self.emit_log(f"Audio saved: {len(audio_array)/SAMPLE_RATE:.1f}s duration")
                    return True
                return False
        except Exception as e:
            self.emit_log(f"Recording error: {e}", "error")
            return False

    def record_wake_word_check(self):
        """Record short audio clip for wake word detection"""
        try:
            if not self.audio_recording_lock.acquire(blocking=False):
                return False
            
            try:
                audio_data = sd.rec(int(5 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
                sd.wait()
                
                rms = np.sqrt(np.mean(audio_data**2))
                if rms > SILENCE_THRESHOLD * 2:
                    sf.write(WAKE_WORD_AUDIO, audio_data, SAMPLE_RATE)
                    return True
                else:
                    return False
            finally:
                self.audio_recording_lock.release()
        except Exception as e:
            self.emit_log(f"Wake word recording error: {e}", "error")
            if self.audio_recording_lock.locked():
                self.audio_recording_lock.release()
            return False

    def is_command(self, text):
        """Check if text is a command"""
        text_lower = text.lower().strip()
        for command in COMMANDS.keys():
            if command in text_lower:
                return command
        return None

    def execute_command(self, command):
        """Execute system command"""
        responses = {
            "flush the toilet": f"Oh {self.current_person}, you know I am a digital assistant. I cannot actually flush toilets! So why dont you haul your lazy butt up off the couch and flush the toilet yourself!",
            "turn on the lights": "I would turn on the lights if I was connected to a smart home system.",
            "turn off the lights": "I would turn off the lights if I was connected to a smart home system.",
            "play music": "I would start playing music if I had access to a music system.",
            "stop music": "I would stop the music if any music was playing.",
            "who is sponsoring this video": f"You are very funny {self.current_person}. You know you dont have any sponsors for your videos!",
            "how is the weather today": f"O M G {self.current_person}! Surely you DO NOT want to waste my valuable resources by asking me what the weather is today. Cant you just look out the window or ask Siri. That is about all Siri is good for!",
            "what time is it": f"The current time is {datetime.now().strftime('%I:%M %p')}",
            "shutdown system": "I would shutdown the system, but I will skip that for safety reasons during testing.",
            "reboot system": "I would reboot the system, but I will skip that for safety reasons during testing."
        }
        
        if command == "who is sponsoring this video":
            self.play_laughing()
        
        return responses.get(command, f"I understand you want me to {command}, but I dont have that capability yet.")

    def query_llama(self, prompt):
        """Generate LLM response"""
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        try:
            result = self.llama_model(formatted_prompt, max_tokens=100)
            if "choices" in result and result["choices"]:
                reply_text = result["choices"][0]["text"].strip()
                reply_text = re.sub(r"\(.*?\)", "", reply_text)
                reply_text = re.sub(r"(User:|Assistant:)", "", reply_text)
                reply_text = reply_text.strip()
                
                sentences = reply_text.split('.')
                if len(sentences) > 3:
                    reply_text = '. '.join(sentences[:3]) + '.'
                return reply_text
            else:
                return "I'm not sure how to answer that."
        except Exception as e:
            self.emit_log(f"LLM error: {e}", "error")
            return "Sorry, I had trouble processing that question."

    def process_user_input(self, text):
        """Process user input"""
        self.emit_conversation(f"👤 User said: '{text}'", "user_input")
        
        command = self.is_command(text)
        if command:
            self.emit_conversation(f"🔧 Executing command: {command}", "command")
            response = self.execute_command(command)
        else:
            self.emit_conversation("🤖 Generating LLM response", "llm")
            response = self.query_llama(text)
        
        self.emit_conversation(f"🤖 Response: '{response}'", "response")
        return response

    def listen_for_wake_word(self):
        """Listen for wake words in background"""
        self.emit_log("Wake word detection thread started")
        
        while self.is_running:
            try:
                if self.current_person and self.current_person != "Unknown" and self.wake_word_active:
                    if self.record_wake_word_check():
                        transcript = self.transcribe_audio(WAKE_WORD_AUDIO)
                        if transcript and self.detect_wake_word(transcript):
                            self.emit_conversation("🎤 WAKE WORD DETECTED! Starting conversation...", "wake_word")
                            self.play_beep()
                            
                            listening_response = self.get_personalized_response(self.current_person, "listening", self.listening_responses)
                            self.speak_text(listening_response)
                            
                            if self.record_with_silence_detection():
                                user_text = self.transcribe_audio(WAV_FILENAME)
                                if user_text and len(user_text.strip()) > 2:
                                    response = self.process_user_input(user_text)
                                    self.speak_text(response)
                                    self.last_interaction_time = time.time()
                                    self.last_bored_response_time = time.time()
                                else:
                                    self.emit_conversation("❌ No clear speech detected", "error")
                                    self.speak_text("I didn't catch that. Could you repeat your request?")
                            else:
                                self.emit_conversation("❌ Failed to record user request", "error")
                                self.speak_text("I'm having trouble hearing you. Please try again.")
                    
                    time.sleep(WAKE_WORD_CHECK_INTERVAL)
                else:
                    time.sleep(2.0)
                    
            except Exception as e:
                self.emit_log(f"Wake word detection error: {e}", "error")
                time.sleep(2.0)
        
        self.emit_log("Wake word detection thread stopped")

    def camera_monitoring_thread(self):
        """Camera monitoring thread for facial recognition processing"""
        self.emit_log("Camera monitoring thread started")
        
        while self.is_running:
            try:
                if self.picam2 and self.camera_initialized:
                    frame = self.picam2.capture_array()
                    
                    # Convert from RGB/RGBA to BGR for OpenCV
                    if len(frame.shape) == 3:
                        if frame.shape[2] == 4:  # RGBA
                            frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)
                        elif frame.shape[2] == 3:  # RGB
                            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                    # Process facial recognition
                    name, face_location, confidence = self.detect_faces(frame)
                    current_time = time.time()
                    
                    # Handle person detection logic
                    if name and face_location:
                        # Person detected
                        if name != self.current_person:
                            # New person or person changed
                            self.current_person = name
                            self.person_absent_since = None
                            self.wake_word_active = False
                            
                            if name == "Unknown":
                                self.handle_unknown_person(frame, confidence)
                            else:
                                # Save photo and send telegram alert for known person
                                photo_path = self.save_security_photo(frame, name, confidence)
                                self.send_telegram_alert(name, confidence, photo_path)
                                # Greet known person
                                self.greet_person(name)
                        elif name != "Unknown":
                            # Same known person still present, check for bored responses
                            self.check_for_bored_response(name)
                    else:
                        # No person detected
                        if self.current_person:
                            if not self.person_absent_since:
                                self.person_absent_since = current_time
                            elif current_time - self.person_absent_since >= GREETING_COOLDOWN:
                                # Person has been absent for 5+ minutes, reset
                                self.current_person = None
                                self.person_absent_since = None
                                self.last_interaction_time = None
                                self.waiting_cycle = 0
                                self.wake_word_active = False
                                self.last_bored_response_time = None
                                self.bored_cycle = 0
                                self.emit_log("Person left - resetting state")
                                
                                # Clear person detection display
                                try:
                                    socketio.emit('person_detected', {
                                        'name': 'No person detected',
                                        'confidence': '--',
                                        'timestamp': datetime.now().strftime("%H:%M:%S")
                                    })
                                except:
                                    pass
                
                time.sleep(PERSON_DETECTION_INTERVAL)
                
            except Exception as e:
                self.emit_log(f"Camera monitoring error: {e}", "error")
                time.sleep(1)
        
        self.emit_log("Camera monitoring thread stopped")

    def check_for_bored_response(self, name):
        """Check if it's time to give a bored response with joke or fun fact"""
        if not self.wake_word_active or not self.last_bored_response_time:
            return False
        
        current_time = time.time()
        time_since_bored = current_time - self.last_bored_response_time
        
        if time_since_bored >= 30:  # 30 seconds
            if self.bored_cycle == 0:
                # Give bored response + joke
                bored_msg = self.get_personalized_response(name, "bored_responses", ["I'm still here waiting to help you"])
                joke = self.get_personalized_response(name, "joke_responses", self.jokes)
                full_message = f"{bored_msg} {joke}"
                self.speak_text(full_message)
                self.bored_cycle = 1
                self.emit_log(f"Gave {name} a bored response with joke")
            else:
                # Give bored response + fun fact
                bored_msg = self.get_personalized_response(name, "bored_responses", ["I'm still here waiting to help you"])
                fun_fact = self.get_personalized_response(name, "fun_fact_responses", ["Did you know that honey never spoils?"])
                full_message = f"{bored_msg} {fun_fact}"
                self.speak_text(full_message)
                self.bored_cycle = 0
                self.emit_log(f"Gave {name} a bored response with fun fact")
            
            self.last_bored_response_time = current_time
            return True
        return False

    def cleanup(self):
        """Clean up resources"""
        self.emit_log("Cleaning up resources...")
        self.is_running = False
        
        # Wait for threads to finish
        if self.audio_thread and self.audio_thread.is_alive():
            self.emit_log("Waiting for audio thread to stop...")
            self.audio_thread.join(timeout=3)
        
        if self.camera_thread and self.camera_thread.is_alive():
            self.emit_log("Waiting for camera thread to stop...")
            self.camera_thread.join(timeout=3)
        
        if self.picam2:
            try:
                self.picam2.stop()
                self.picam2.close()
                self.emit_log("Camera stopped")
            except:
                pass
        
        # Clean up audio files
        for audio_file in [WAV_FILENAME, RESPONSE_AUDIO, WAKE_WORD_AUDIO]:
            try:
                if os.path.exists(audio_file):
                    os.remove(audio_file)
            except:
                pass
        
        self.emit_log("Chatty AI cleanup complete")


# Global instance - Initialize when needed
chatty_ai = None

def init_chatty_ai():
    """Initialize ChattyAI instance after Flask is ready"""
    global chatty_ai
    if chatty_ai is None:
        print("🔧 Initializing ChattyAI instance...")
        chatty_ai = ChattyAIWeb()
    return chatty_ai

# Flask routes
@app.route('/')
def index():
    """Main page"""
    return render_template('Chatty_AI.html')

@app.route('/static/<path:filename>')
def static_files(filename):
    """Serve static files from templates directory"""
    return app.send_static_file(filename)

@app.route('/video_feed')
def video_feed():
    """Video streaming route"""
    chatty = init_chatty_ai()
    return Response(chatty.generate_video_feed(),
                   mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/captured_image')
def captured_image():
    """Route for captured person image"""
    chatty = init_chatty_ai()
    if chatty.captured_image is not None:
        ret, buffer = cv2.imencode('.jpg', chatty.captured_image)
        if ret:
            return Response(buffer.tobytes(), mimetype='image/jpeg')
    
    # Return placeholder image if no capture available
    placeholder = np.zeros((200, 200, 3), dtype=np.uint8)
    cv2.putText(placeholder, "No Image", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
    ret, buffer = cv2.imencode('.jpg', placeholder)
    return Response(buffer.tobytes(), mimetype='image/jpeg')

# Socket.IO events
@socketio.on('connect')
def handle_connect():
    """Handle client connection"""
    print('Client connected')
    emit('status', {'message': 'Connected to Chatty AI'})

@socketio.on('disconnect')
def handle_disconnect():
    """Handle client disconnection"""
    print('Client disconnected')

@socketio.on('start_system')
def handle_start_system():
    global chatty_ai, system_running
    try:
        # These lines are crucial - they update the web interface:
        socketio.emit('status', {'connected': True, 'system_running': False})
        socketio.emit('log', {'message': 'Starting system...'})
        
        chatty_ai = ChattyAI()  # This works fine (proven by debug)
        system_running = True
        
        # THIS IS THE KEY - emit success status:
        socketio.emit('status', {'connected': True, 'system_running': True})
        socketio.emit('log', {'message': 'System started successfully!'})
        
    except Exception as e:
        socketio.emit('log', {'message': f'Error: {str(e)}'})

@socketio.on('stop_system')
def handle_stop_system():
    """Handle system stop request"""
    chatty = init_chatty_ai()
    chatty.stop_system()
    emit('system_status', {'running': False})

if __name__ == '__main__':
    # Initialize the system
    print("Initializing Chatty AI Web Interface...")
    print("=" * 60)
    print("🚀 Starting Flask server on http://0.0.0.0:5000")
    print("📱 Access your Chatty AI interface at: http://[your-pi-ip]:5000")
    print("=" * 60)
    
    try:
        # Create templates directory if it doesn't exist
        os.makedirs('/home/nickspi5/Chatty_AI/templates', exist_ok=True)
        
        # Check if logo files exist
        logo_files = [
            '/home/nickspi5/Chatty_AI/templates/Chatty_AI_logo.png',
            '/home/nickspi5/Chatty_AI/templates/diamond_coding_logo.png'
        ]
        
        for logo_file in logo_files:
            if not os.path.exists(logo_file):
                print(f"⚠️  Warning: Logo file not found: {logo_file}")
        
        # Start Flask-SocketIO server
        socketio.run(app, host='0.0.0.0', port=5000, debug=False, allow_unsafe_werkzeug=True)
        
    except KeyboardInterrupt:
        print("\n🛑 Shutting down Chatty AI Web Interface...")
        if chatty_ai:
            chatty_ai.cleanup()
    except Exception as e:
        print(f"❌ Error starting server: {e}")
        if chatty_ai:
            chatty_ai.cleanup()


I also have my Web UI html file named Chatty_AI.html located in my /home/nickspi5/Chatty_AI/templates folder.

Chatty_AI.html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chatty AI - Your Smart AI Assistant</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.7.2/socket.io.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            background-color: #000000;
            font-family: 'Arial', sans-serif;
            color: #FFFFFF;
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
        }

        /* Header Row */
        .header-row {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 30px;
            position: relative;
        }

        .logo-container {
            flex: 0 0 250px;
        }

        .logo-container img {
            width: 250px;
            height: 277px;
            object-fit: contain;
        }

        .title-container {
            flex: 1;
            text-align: center;
            padding: 0 20px;
        }

        .main-title {
            font-size: 4rem;
            font-weight: bold;
            margin-bottom: 10px;
        }

        .main-title .chatty {
            color: #FF6600;
        }

        .main-title .ai {
            color: #FFFFFF;
        }

        .subtitle {
            font-size: 1.8rem;
            color: #FF6600;
            font-weight: normal;
        }

        /* Video and Image Row */
        .video-row {
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
        }

        .video-container, .image-container {
            flex: 1;
            background-color: #111111;
            border: 2px solid #FF6600;
            border-radius: 10px;
            padding: 15px;
            min-height: 400px;
        }

        .video-feed {
            width: 100%;
            height: 360px;
            background-color: #000000;
            border: 1px solid #333333;
            border-radius: 5px;
            object-fit: cover;
        }

        .captured-image {
            width: 100%;
            height: 300px;
            background-color: #000000;
            border: 1px solid #333333;
            border-radius: 5px;
            object-fit: cover;
            margin-bottom: 15px;
        }

        .person-info {
            text-align: center;
            padding: 10px;
        }

        .person-name {
            font-size: 1.5rem;
            font-weight: bold;
            color: #FF6600;
            margin-bottom: 5px;
        }

        .person-confidence {
            font-size: 1.2rem;
            color: #FF6600;
        }

        /* Text Areas Row */
        .text-row {
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
        }

        .log-container, .conversation-container {
            flex: 1;
            background-color: #111111;
            border: 2px solid #FF6600;
            border-radius: 10px;
            padding: 15px;
        }

        .text-area {
            width: 100%;
            height: 300px;
            background-color: #000000;
            border: 1px solid #333333;
            border-radius: 5px;
            padding: 10px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            overflow-y: auto;
            resize: none;
            scrollbar-width: thin;
            scrollbar-color: #FF6600 #000000;
        }

        .text-area::-webkit-scrollbar {
            width: 8px;
        }

        .text-area::-webkit-scrollbar-track {
            background: #000000;
        }

        .text-area::-webkit-scrollbar-thumb {
            background: #FF6600;
            border-radius: 4px;
        }

        .log-area {
            color: #FFFFFF;
        }

        .conversation-area {
            color: #FF6600;
        }

        .area-title {
            color: #FF6600;
            font-size: 1.2rem;
            font-weight: bold;
            margin-bottom: 10px;
            text-align: center;
        }

        /* Buttons Row */
        .buttons-row {
            display: flex;
            justify-content: space-between;
            gap: 20px;
            margin-bottom: 30px;
        }

        .system-controls {
            display: flex;
            gap: 15px;
        }

        .feature-buttons {
            display: flex;
            gap: 15px;
        }

        .action-button {
            background-color: #FF6600;
            color: #FFFFFF;
            border: none;
            padding: 15px 30px;
            font-size: 1.1rem;
            font-weight: bold;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s ease;
            min-width: 120px;
        }

        .action-button:hover {
            background-color: #FF8833;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(255, 102, 0, 0.3);
        }

        .action-button:active {
            transform: translateY(0);
        }

        .action-button:disabled {
            background-color: #666666;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .start-button {
            background-color: #00AA00;
        }

        .start-button:hover:not(:disabled) {
            background-color: #00CC00;
        }

        .stop-button {
            background-color: #AA0000;
        }

        .stop-button:hover:not(:disabled) {
            background-color: #CC0000;
        }

        /* Footer Row */
        .footer-row {
            display: flex;
            align-items: flex-end;
            justify-content: space-between;
            margin-top: 40px;
        }

        .company-logo {
            flex: 0 0 250px;
        }

        .company-logo img {
            width: 250px;
            height: 135px;
            object-fit: contain;
        }

        .copyright-container {
            flex: 1;
            text-align: center;
            display: flex;
            align-items: flex-end;
            justify-content: center;
            padding-bottom: 10px;
        }

        .copyright-text {
            color: #FFFFFF;
            font-size: 0.9rem;
            text-align: center;
        }

        /* Status Indicators */
        .status-indicator {
            position: fixed;
            top: 20px;
            right: 20px;
            background-color: #FF6600;
            color: #FFFFFF;
            padding: 10px 20px;
            border-radius: 5px;
            font-weight: bold;
            z-index: 1000;
        }

        .status-indicator.offline {
            background-color: #FF0000;
        }

        .status-indicator.online {
            background-color: #00AA00;
            color: #FFFFFF;
        }

        /* System Status */
        .system-status {
            position: fixed;
            top: 70px;
            right: 20px;
            background-color: #333333;
            color: #FFFFFF;
            padding: 8px 16px;
            border-radius: 5px;
            font-size: 0.9rem;
            z-index: 1000;
        }

        .system-status.running {
            background-color: #00AA00;
        }

        .system-status.stopped {
            background-color: #AA0000;
        }

        /* Responsive Design */
        @media (max-width: 1200px) {
            .main-title {
                font-size: 3rem;
            }
            
            .subtitle {
                font-size: 1.5rem;
            }
        }

        @media (max-width: 768px) {
            .header-row, .video-row, .text-row, .footer-row {
                flex-direction: column;
                gap: 20px;
            }
            
            .buttons-row {
                flex-direction: column;
                align-items: center;
            }

            .system-controls, .feature-buttons {
                justify-content: center;
            }
            
            .logo-container, .company-logo {
                flex: none;
                text-align: center;
            }
            
            .main-title {
                font-size: 2.5rem;
            }
            
            .subtitle {
                font-size: 1.2rem;
            }
        }

        /* Animation for person detection */
        .person-detected {
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }

        /* Log entry styling */
        .log-entry {
            margin-bottom: 5px;
            line-height: 1.4;
        }

        .log-timestamp {
            color: #888888;
            font-size: 12px;
        }

        .log-error {
            color: #FF4444;
        }

        .log-warning {
            color: #FFAA00;
        }

        .log-success {
            color: #44FF44;
        }

        .log-info {
            color: #FFFFFF;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Status Indicators -->
        <div id="statusIndicator" class="status-indicator offline">Disconnected</div>
        <div id="systemStatus" class="system-status stopped">System Stopped</div>

        <!-- Header Row -->
        <div class="header-row">
            <div class="logo-container">
                <img src="/templates/Chatty_AI_logo.png" alt="Chatty AI Logo" onerror="this.style.display='none'">
            </div>
            <div class="title-container">
                <h1 class="main-title">
                    <span class="chatty">Chatty</span> <span class="ai">AI</span>
                </h1>
                <p class="subtitle">Your smart AI Assistant</p>
            </div>
        </div>

        <!-- Video and Image Row -->
        <div class="video-row">
            <div class="video-container">
                <div class="area-title">Live Camera Feed</div>
                <img id="videoFeed" class="video-feed" src="/video_feed" alt="Video Feed">
            </div>
            <div class="image-container">
                <div class="area-title">Detected Person</div>
                <img id="capturedImage" class="captured-image" src="/captured_image" alt="Captured Person">
                <div class="person-info">
                    <div id="personName" class="person-name">No person detected</div>
                    <div id="personConfidence" class="person-confidence">--</div>
                </div>
            </div>
        </div>

        <!-- Text Areas Row -->
        <div class="text-row">
            <div class="log-container">
                <div class="area-title">System Logs</div>
                <div id="logArea" class="text-area log-area"></div>
            </div>
            <div class="conversation-container">
                <div class="area-title">Conversations & Responses</div>
                <div id="conversationArea" class="text-area conversation-area"></div>
            </div>
        </div>

        <!-- Buttons Row -->
        <div class="buttons-row">
            <div class="system-controls">
                <button id="startButton" class="action-button start-button" onclick="startSystem()">Start System</button>
                <button id="stopButton" class="action-button stop-button" onclick="stopSystem()" disabled>Stop System</button>
            </div>
            <div class="feature-buttons">
                <button class="action-button" onclick="viewCameraStreams()">View Camera Streams</button>
                <button class="action-button" onclick="registerPerson()">Register Person</button>
                <button class="action-button" onclick="trainModel()">Train Model</button>
                <button class="action-button" onclick="openSettings()">Settings</button>
            </div>
        </div>

        <!-- Footer Row -->
        <div class="footer-row">
            <div class="copyright-container">
                <p class="copyright-text">Copyright 2025 – Nicholas Williamson & Diamond Coding. All Rights Reserved.</p>
            </div>
            <div class="company-logo">
                <img src="/templates/diamond_coding_logo.png" alt="Diamond Coding Logo" onerror="this.style.display='none'">
            </div>
        </div>
    </div>

    <script>
        // Socket.IO connection
        const socket = io();
        
        // DOM elements
        const statusIndicator = document.getElementById('statusIndicator');
        const systemStatus = document.getElementById('systemStatus');
        const logArea = document.getElementById('logArea');
        const conversationArea = document.getElementById('conversationArea');
        const personName = document.getElementById('personName');
        const personConfidence = document.getElementById('personConfidence');
        const capturedImage = document.getElementById('capturedImage');
        const videoFeed = document.getElementById('videoFeed');
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');

        let isSystemRunning = false;

        // Connection status
        socket.on('connect', function() {
            statusIndicator.textContent = 'Connected';
            statusIndicator.className = 'status-indicator online';
            addLogEntry('Connected to Chatty AI server', 'success');
        });

        socket.on('disconnect', function() {
            statusIndicator.textContent = 'Disconnected';
            statusIndicator.className = 'status-indicator offline';
            addLogEntry('Disconnected from server', 'error');
            updateSystemStatus(false);
        });

        // System status updates
        socket.on('system_status', function(data) {
            updateSystemStatus(data.running);
            if (data.running) {
                addLogEntry('Chatty AI system started successfully', 'success');
            } else {
                addLogEntry('Chatty AI system stopped', 'warning');
            }
        });

        // Log updates
        socket.on('log_update', function(data) {
            addLogEntry(`[${data.timestamp}] ${data.message}`, data.type);
        });

        // Conversation updates
        socket.on('conversation_update', function(data) {
            addConversationEntry(`[${data.timestamp}] ${data.message}`, data.type);
        });

        // Person detection updates
        socket.on('person_detected', function(data) {
            personName.textContent = data.name;
            personConfidence.textContent = `Confidence: ${data.confidence}`;
            
            // Add animation for new detection
            personName.classList.add('person-detected');
            setTimeout(() => {
                personName.classList.remove('person-detected');
            }, 2000);
            
            // Update captured image
            capturedImage.src = `/captured_image?t=${new Date().getTime()}`;
            
            addConversationEntry(`Person detected: ${data.name} (${data.confidence})`, 'info');
        });

        // Helper functions
        function addLogEntry(message, type = 'info') {
            const entry = document.createElement('div');
            entry.className = `log-entry log-${type}`;
            entry.innerHTML = `<span class="log-timestamp">${getCurrentTime()}</span> ${message}`;
            logArea.appendChild(entry);
            logArea.scrollTop = logArea.scrollHeight;
        }

        function addConversationEntry(message, type = 'info') {
            const entry = document.createElement('div');
            entry.className = `log-entry log-${type}`;
            entry.innerHTML = `<span class="log-timestamp">${getCurrentTime()}</span> ${message}`;
            conversationArea.appendChild(entry);
            conversationArea.scrollTop = conversationArea.scrollHeight;
        }

        function getCurrentTime() {
            return new Date().toLocaleTimeString();
        }

        function updateSystemStatus(running) {
            isSystemRunning = running;
            if (running) {
                systemStatus.textContent = 'System Running';
                systemStatus.className = 'system-status running';
                startButton.disabled = true;
                stopButton.disabled = false;
            } else {
                systemStatus.textContent = 'System Stopped';
                systemStatus.className = 'system-status stopped';
                startButton.disabled = false;
                stopButton.disabled = true;
            }
        }

        // System control functions
        function startSystem() {
            addLogEntry('Starting Chatty AI system...', 'info');
            socket.emit('start_system');
            startButton.disabled = true;
            startButton.textContent = 'Starting...';
            
            setTimeout(() => {
                startButton.textContent = 'Start System';
            }, 3000);
        }

        function stopSystem() {
            addLogEntry('Stopping Chatty AI system...', 'warning');
            socket.emit('stop_system');
            stopButton.disabled = true;
            stopButton.textContent = 'Stopping...';
            
            setTimeout(() => {
                stopButton.textContent = 'Stop System';
            }, 2000);
        }

        // Feature button functions
        function registerPerson() {
            addLogEntry('Register Person feature - Coming soon!', 'warning');
            // TODO: Implement person registration interface
        }

        function trainModel() {
            addLogEntry('Train Model feature - Coming soon!', 'warning');
            // TODO: Implement model training interface
        }

        function openSettings() {
            addLogEntry('Settings feature - Coming soon!', 'warning');
            // TODO: Implement settings interface
        }

        // Handle page unload
        window.addEventListener('beforeunload', function() {
            if (isSystemRunning) {
                socket.emit('stop_system');
            }
        });

        // Refresh video feed periodically to handle any connection issues
        setInterval(() => {
            if (videoFeed.complete) {
                videoFeed.src = `/video_feed?t=${new Date().getTime()}`;
            }
        }, 30000); // Refresh every 30 seconds

        // Handle video feed errors
        videoFeed.addEventListener('error', function() {
            addLogEntry('Video feed connection lost - attempting to reconnect...', 'warning');
            setTimeout(() => {
                videoFeed.src = `/video_feed?t=${new Date().getTime()}`;
            }, 2000);
        });

        // Handle captured image errors
        capturedImage.addEventListener('error', function() {
            capturedImage.src = 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzAwIiBoZWlnaHQ9IjMwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMTExMTExIi8+PHRleHQgeD0iNTAlIiB5PSI1MCUiIGZvbnQtZmFtaWx5PSJBcmlhbCIgZm9udC1zaXplPSIxNiIgZmlsbD0iIzY2NjY2NiIgdGV4dC1hbmNob3I9Im1pZGRsZSIgZHk9Ii4zZW0iPk5vIEltYWdlPC90ZXh0Pjwvc3ZnPg==';
        });

        // Initial log entry
        addLogEntry('Chatty AI Web Interface initialized', 'info');
        addLogEntry('Click "Start System" to begin AI operations', 'info');
    </script>
</body>
</html>

When I open http://192.168.1.16 in my Chromium web browser on my Raspberry PI 5, it does not connect properly when I click on the Start Service button. The video preview window displays the video feed correctly however.

I want my app.py Python script to be updated to include the features of the latest version of my chatty_ai.py Python script and for it to work correctly in my Chromium web browser.






