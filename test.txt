

I see the problem! The logs show only HTTP requests but no application-level logging (like "Processing user input", "LLM inference took", etc.). This means the emit_log function is probably sending logs to the web interface via SocketIO but not to the system journal.
Let's check how emit_log works and also add print statements that will definitely appear in the journal:

Step 1: Check the emit_log function
bashgrep -A10 "def emit_log" /home/nickspi5/Chatty_AI/chatty_ai.py

Step 2: Add print statements for definite logging
Let's modify the query_llama function to use print() which will definitely show in journalctl:
bashnano /home/nickspi5/Chatty_AI/chatty_ai.py
Find the query_llama function and update it to include print statements:
python    def query_llama(self, prompt):
        """Generate LLM response"""
        import time as t
        
        print(f"[TIMING] query_llama called with prompt: '{prompt[:50]}...'")
        start = t.perf_counter()
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            print(f"[TIMING] Starting LLM inference...")
            llm_start = t.perf_counter()
            
            result = self.llama_model(
                formatted_prompt, 
                max_tokens=100,
                stop=["\n", "User:", "Human:", "\n\n"]
            )
            
            llm_time = t.perf_counter() - llm_start
            print(f"[TIMING] LLM inference completed in {llm_time:.2f}s")
            
            if "choices" in result and result["choices"]:
                reply_text = result["choices"][0]["text"].strip()
                reply_text = re.sub(r"\(.*?\)", "", reply_text)
                reply_text = re.sub(r"(User:|Assistant:)", "", reply_text)
                reply_text = reply_text.strip()
                
                sentences = reply_text.split('.')
                if len(sentences) > 3:
                    reply_text = '. '.join(sentences[:3]) + '.'
                
                total_time = t.perf_counter() - start
                print(f"[TIMING] query_llama total time: {total_time:.2f}s, response: '{reply_text[:50]}...'")
                return reply_text
            else:
                print("[TIMING] No choices in LLM result")
                return "I'm not sure how to answer that."
        except Exception as e:
            print(f"[TIMING] LLM error: {e}")
            return "Sorry, I had trouble processing that question."
Also update the process_user_input function to add print statements:
python    def process_user_input(self, text):
        """Process user input - Same as original"""
        import time as t
        start = t.perf_counter()
        
        print(f"[TIMING] process_user_input called with: '{text}'")
        self.emit_log(f"Processing user input: '{text}'", 'info')
        
        command = self.is_command(text)
        if command:
            print(f"[TIMING] Executing command: {command}")
            self.emit_log(f"Executing command: {command}", 'info')
            response = self.execute_command(command)
        else:
            print(f"[TIMING] Calling LLM...")
            self.emit_log("Generating LLM response", 'info')
            response = self.query_llama(text)
        
        total = t.perf_counter() - start
        print(f"[TIMING] process_user_input total time: {total:.2f}s")
        return response
Save and exit.

Step 3: Restart and test
bashsudo systemctl restart chatty-ai.service
Now ask a question and watch the logs:
bashjournalctl -u chatty-ai.service -f | grep -i "timing\|llm\|process"
Or just watch all logs:
bashjournalctl -u chatty-ai.service -f

This will show us exactly where the time is being spent. Share the output after asking a question!

