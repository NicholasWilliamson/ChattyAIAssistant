

Find the speak_text function and add this new streaming version right after it:
    def speak_text_streaming(self, text_queue, stop_event):
        """Stream TTS audio from a queue of text chunks - runs in separate thread"""
        import subprocess
        import threading
        
        try:
            while not stop_event.is_set() or not text_queue.empty():
                try:
                    # Get text chunk with timeout
                    text_chunk = text_queue.get(timeout=0.5)
                    if text_chunk and len(text_chunk.strip()) > 0:
                        with self.audio_recording_lock:
                            command = [
                                PIPER_EXECUTABLE,
                                "--model", VOICE_PATH,
                                "--config", CONFIG_PATH,
                                "--output_file", RESPONSE_AUDIO
                            ]
                            subprocess.run(command, input=text_chunk.encode("utf-8"), check=True, capture_output=True)
                            subprocess.run(["aplay", RESPONSE_AUDIO], check=True, capture_output=True)
                    text_queue.task_done()
                except Exception:
                    # Queue empty, continue waiting
                    pass
        except Exception as e:
            print(f"[STREAMING TTS] Error: {e}")

Step 2: Update query_llama_streaming to Use Threaded TTS
Replace the entire query_llama_streaming function with this improved version that uses a separate thread for TTS:
    def query_llama_streaming(self, prompt, callback=None):
        """Generate LLM response with streaming - speaks as tokens arrive using threaded TTS"""
        import time as t
        import threading
        import queue
        
        print(f"[TIMING] query_llama_streaming called with prompt: '{prompt[:50]}...'")
        start = t.perf_counter()
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            print(f"[TIMING] Starting streaming LLM inference...")
            llm_start = t.perf_counter()
            
            full_response = ""
            current_sentence = ""
            sentences_spoken = 0
            max_sentences = 30
            first_speech_logged = False
            
            # Create a queue for TTS chunks and start TTS thread
            tts_queue = queue.Queue()
            tts_stop_event = threading.Event()
            tts_thread = threading.Thread(
                target=self.speak_text_streaming,
                args=(tts_queue, tts_stop_event),
                daemon=True
            )
            tts_thread.start()
            
            # Stream tokens from LLM
            for token_data in self.llama_model(
                formatted_prompt, 
                max_tokens=150,
                stop=["User:", "Human:", "Assistant:", "\n\n"],
                stream=True
            ):
                token = token_data['choices'][0]['text']
                full_response += token
                current_sentence += token
                
                # Emit each token to web interface for real-time display
                if callback:
                    callback(token)
                
                # Check if we have a complete sentence
                if any(current_sentence.rstrip().endswith(p) for p in ['.', '!', '?']):
                    sentences_spoken += 1
                    
                    # Log time to first speech
                    if not first_speech_logged:
                        first_speech_time = t.perf_counter() - llm_start
                        print(f"[TIMING] Time to first speech: {first_speech_time:.2f}s")
                        first_speech_logged = True
                    
                    # Clean up the sentence
                    sentence_to_speak = current_sentence.strip()
                    sentence_to_speak = re.sub(r"\(.*?\)", "", sentence_to_speak)
                    sentence_to_speak = re.sub(r"(User:|Assistant:)", "", sentence_to_speak)
                    sentence_to_speak = sentence_to_speak.strip()
                    
                    if sentence_to_speak and len(sentence_to_speak) > 2:
                        print(f"[STREAMING] Queuing sentence {sentences_spoken}: '{sentence_to_speak[:50]}...'")
                        tts_queue.put(sentence_to_speak)
                    
                    current_sentence = ""
                    
                    # Stop after max sentences
                    if sentences_spoken >= max_sentences:
                        print(f"[STREAMING] Reached {max_sentences} sentences, stopping")
                        break
            
            # Queue any remaining text
            if current_sentence.strip() and sentences_spoken < max_sentences:
                remaining = current_sentence.strip()
                remaining = re.sub(r"\(.*?\)", "", remaining)
                remaining = re.sub(r"(User:|Assistant:)", "", remaining)
                if remaining and len(remaining) > 2:
                    print(f"[STREAMING] Queuing remaining: '{remaining[:50]}...'")
                    tts_queue.put(remaining)
            
            # Signal TTS thread to stop after queue is empty and wait for it
            tts_stop_event.set()
            tts_thread.join(timeout=60)  # Wait up to 60 seconds for TTS to finish
            
            llm_time = t.perf_counter() - llm_start
            total_time = t.perf_counter() - start
            print(f"[TIMING] Streaming LLM completed in {llm_time:.2f}s, total: {total_time:.2f}s")
            
            # Clean up full response for return
            full_response = re.sub(r"\(.*?\)", "", full_response)
            full_response = re.sub(r"(User:|Assistant:)", "", full_response)
            
            return full_response.strip()
            
        except Exception as e:
            print(f"[TIMING] Streaming LLM error: {e}")
            return "Sorry, I had trouble processing that question."

