
Thank you, Claude,

I ran: nickspi5@raspberrypi1:~ $ ls -la /home/nickspi5/.config/
total 120
drwxr-xr-x 26 nickspi5 nickspi5 4096 Sep 27 15:37 .
drwx------ 24 nickspi5 nickspi5 4096 Nov 20 21:39 ..
drwxr-xr-x  2 nickspi5 nickspi5 4096 Sep 10 20:51 autostart
drwx------ 37 nickspi5 nickspi5 4096 Nov 22 13:28 chromium
drwx------ 20 nickspi5 nickspi5 4096 Jul 17 06:17 Code
drwx------  2 nickspi5 nickspi5 4096 Nov 20 21:33 dconf
drwx------  2 nickspi5 nickspi5 4096 Sep 27 15:37 galculator
drwx------  5 nickspi5 nickspi5 4096 Nov 20 21:33 geany
drwxr-xr-x  3 nickspi5 nickspi5 4096 Jul 19 07:30 go
drwx------  2 nickspi5 nickspi5 4096 May 13  2025 gtk-3.0
drwx------  2 nickspi5 nickspi5 4096 Jul  4 17:55 htop
drwxr-xr-x  2 nickspi5 nickspi5 4096 May 13  2025 kanshi
drwxr-xr-x  2 nickspi5 nickspi5 4096 Sep 27 15:37 labwc
drwx------  2 nickspi5 nickspi5 4096 Oct  3 18:20 libfm
drwx------  3 nickspi5 nickspi5 4096 Sep 27 12:05 libreoffice
-rw-r--r--  1 nickspi5 nickspi5  164 Jul  2 20:25 lxtask.conf
drwx------  2 nickspi5 nickspi5 4096 Sep 27 15:37 lxterminal
drwx------  3 nickspi5 nickspi5 4096 Jul  4 07:58 mate
drwxr-xr-x  2 nickspi5 nickspi5 4096 Jul  4 07:58 matplotlib
drwx------  2 nickspi5 nickspi5 4096 Jul  3 17:59 Mousepad
drwx------  4 nickspi5 nickspi5 4096 Sep  9 06:24 pcmanfm
drwx------  2 nickspi5 nickspi5 4096 Sep  7 17:43 pulse
drwxr-xr-x  2 nickspi5 nickspi5 4096 Sep 27 15:37 qt5ct
drwxr-xr-x  2 nickspi5 nickspi5 4096 Jul 13 08:50 Ultralytics
-rw-------  1 nickspi5 nickspi5  624 Aug 12 20:32 user-dirs.dirs
-rw-r--r--  1 nickspi5 nickspi5    5 May 13  2025 user-dirs.locale
drwxr-xr-x  2 nickspi5 nickspi5 4096 Aug 25 19:44 vlc
-rw-r--r--  1 nickspi5 nickspi5  187 Sep 27 15:37 wf-panel-pi.ini
drwx------  2 nickspi5 nickspi5 4096 Jul 18 13:58 xarchiver
drwxr-xr-x  2 nickspi5 nickspi5 4096 Sep 27 15:37 xsettingsd
nickspi5@raspberrypi1:~ $ ls -la /ho.me/nickspi5/.config/autostart/
ls: cannot access '/ho.me/nickspi5/.config/autostart/': No such file or directory
nickspi5@raspberrypi1:~ $ ls -la /home/nickspi5/.config/autostart/
total 12
drwxr-xr-x  2 nickspi5 nickspi5 4096 Sep 10 20:51 .
drwxr-xr-x 26 nickspi5 nickspi5 4096 Sep 27 15:37 ..
-rwxr-xr-x  1 nickspi5 nickspi5  271 Sep 10 20:51 chatty-ai-launcher.desktop
nickspi5@raspberrypi1:~ $ find /home/nickspi5/.config -name "*.desktop" -type f
/home/nickspi5/.config/autostart/chatty-ai-launcher.desktop
nickspi5@raspberrypi1:~ $ ls -la /etc/xdg/autostart/
total 88
drwxr-xr-x  2 root root 4096 Sep  4 20:21 .
drwxr-xr-x 15 root root 4096 May 13  2025 ..
-rw-r--r--  1 root root  167 Nov  6  2024 autotouch.desktop
-rw-r--r--  1 root root  204 Apr 30  2025 env-display.desktop
-rw-r--r--  1 root root 8530 Mar 23  2023 gnome-keyring-pkcs11.desktop
-rw-r--r--  1 root root 8057 Mar 23  2023 gnome-keyring-secrets.desktop
-rw-r--r--  1 root root 6596 Mar 23  2023 gnome-keyring-ssh.desktop
-rw-r--r--  1 root root 2370 Jan 12  2024 lxpolkit.desktop
-rw-r--r--  1 root root 7784 Jan 12  2024 polkit-mate-authentication-agent-1.desktop
-rw-r--r--  1 root root  172 Apr 11  2025 pprompt.desktop
-rw-r--r--  1 root root 5315 Feb 10  2023 pulseaudio.desktop
-rw-r--r--  1 root root  188 Apr 30  2025 pwrkey.desktop
-rw-r--r--  1 root root  134 May 13  2025 xcompmgr.desktop
-rw-r--r--  1 root root  212 Sep 27  2022 xdg-user-dirs.desktop
-rw-r--r--  1 root root  173 Jul 12  2023 xdg-user-dirs-kde.desktop
-rw-r--r--  1 root root  185 Apr 30  2025 xwayauth.desktop
nickspi5@raspberrypi1:~ $ find /home/nickspi5 -name "*chatty*.desktop" -type f 2>/dev/null
/home/nickspi5/.config/autostart/chatty-ai-launcher.desktop
nickspi5@raspberrypi1:~ $ systemctl list-unti-files --state=enabled | grep chatty
Unknown command verb list-unti-files.
nickspi5@raspberrypi1:~ $ systemctl list-unit-files --state=enabled | grep chatty
chatty-ai-preloader.service          enabled enabled
chatty-ai.service                    enabled enabled
nickspi5@raspberrypi1:~ $ systemctl --user list-unit-files --state=enabled | grep chatty
nickspi5@raspberrypi1:~ $ ls -la /etc/systemd/system/multi-user.target.wants/ | grep chatty
lrwxrwxrwx  1 root root   47 Aug 25 20:19 chatty-ai-preloader.service -> /etc/systemd/system/chatty-ai-preloader.service
lrwxrwxrwx  1 root root   37 Aug 16 18:31 chatty-ai.service -> /etc/systemd/system/chatty-ai.service
nickspi5@raspberrypi1:~ $ ls -la /etc/systems/system/graphical.target.wants/ | grep chatty
ls: cannot access '/etc/systems/system/graphical.target.wants/': No such file or directory
nickspi5@raspberrypi1:~ $ ls -la /etc/systemd/system/graphical.target.wants/ | grep chatty
nickspi5@raspberrypi1:~ $ systemctl list-units --all | grep chatty
  chatty-ai-preloader.service                                                                                                                           loaded    active   running   Chatty AI Model Preloader Service
  chatty-ai.service                                                                                                                                     loaded    active   running   Chatty AI Web Interface
nickspi5@raspberrypi1:~ $ cat /home/nickspi5/.config/autostart/*.desktop 2>/dev/null
[Desktop Entry]
Type=Application
Name=Chatty AI Auto Launcher
Comment=Automatically launch Chatty AI on boot
Exec=/bin/bash /home/nickspi5/Chatty_AI/chatty_ai_startup_final.sh
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Terminal=false
StartupNotify=false
nickspi5@raspberrypi1:~ $ ls -lh /home/nickspi5/.config/autostart/
total 4.0K
-rwxr-xr-x 1 nickspi5 nickspi5 271 Sep 10 20:51 chatty-ai-launcher.desktop
nickspi5@raspberrypi1:~ $ cat /etc/systemd/system/chatty-ai-preloader.service 2>/dev/null
[Unit]
Description=Chatty AI Model Preloader Service
After=network.target
Before=chatty-ai.service
Wants=network-online.target

[Service]
Type=simple
User=nickspi5
Group=nickspi5
WorkingDirectory=/home/nickspi5/Chatty_AI
Environment=PATH=/home/nickspi5/Chatty_AI/chatty-venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Environment=PYTHONPATH=/home/nickspi5/Chatty_AI
Environment=PYTHONUNBUFFERED=1
Environment=OMP_NUM_THREADS=4
Environment=MKL_NUM_THREADS=4

# Run the preloader script
ExecStart=/home/nickspi5/Chatty_AI/chatty-venv/bin/python /home/nickspi5/Chatty_AI/chatty_ai_preloader.py

# Restart policy
Restart=always
RestartSec=10
StartLimitInterval=60
StartLimitBurst=3

# Resource limits for better performance
LimitNOFILE=4096
Nice=-10
IOSchedulingClass=best-effort
IOSchedulingPriority=0

# Memory settings
MemoryMax=2G
MemorySwapMax=0

# Security settings
NoNewPrivileges=true
PrivateTmp=false

[Install]
WantedBy=multi-user.target
nickspi5@raspberrypi1:~ $ cat /home/nickspi5/Chatty_AI/chatty_ai_preloader.py 2>/dev/null
#!/usr/bin/env python3
"""
chatty_ai_preloader.py - Preload AI models and keep them in memory
This service runs at boot to preload all AI models for faster response times
"""

import os
import sys
import time
import logging
import signal
import mmap
import pickle
import threading
import subprocess
from pathlib import Path
from faster_whisper import WhisperModel
from llama_cpp import Llama

# Configuration
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "/home/nickspi5/Chatty_AI/tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
ENCODINGS_FILE = "/home/nickspi5/Chatty_AI/encodings.pickle"

# Shared memory paths for model state
SHARED_MEM_DIR = "/dev/shm/chatty_ai"
PRELOAD_STATUS_FILE = f"{SHARED_MEM_DIR}/preload_status"
MODEL_CACHE_DIR = f"{SHARED_MEM_DIR}/model_cache"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/home/nickspi5/Chatty_AI/logs/preloader.log')
    ]
)
logger = logging.getLogger(__name__)

class ChattyAIPreloader:
    def __init__(self):
        self.running = True
        self.whisper_model = None
        self.llama_model = None
        self.face_encodings = None
        self.voice_model_loaded = False
        
        # Create shared memory directories
        os.makedirs(SHARED_MEM_DIR, exist_ok=True)
        os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
        os.makedirs('/home/nickspi5/Chatty_AI/logs', exist_ok=True)
        
        # Signal handling
        signal.signal(signal.SIGTERM, self.signal_handler)
        signal.signal(signal.SIGINT, self.signal_handler)
        
    def signal_handler(self, sig, frame):
        """Handle shutdown signals"""
        logger.info("Received shutdown signal")
        self.running = False
        self.cleanup()
        sys.exit(0)
        
    def update_status(self, status_dict):
        """Update preload status in shared memory"""
        try:
            with open(PRELOAD_STATUS_FILE, 'w') as f:
                f.write(str(status_dict))
            logger.info(f"Status updated: {status_dict}")
        except Exception as e:
            logger.error(f"Failed to update status: {e}")
            
    def preload_whisper(self):
        """Preload Whisper model into memory"""
        try:
            logger.info(f"Loading Whisper model: {WHISPER_MODEL_SIZE}")
            start_time = time.time()
            
            self.whisper_model = WhisperModel(
                WHISPER_MODEL_SIZE, 
                device="cpu", 
                compute_type="int8",
                download_root="/home/nickspi5/Chatty_AI/.cache/whisper"
            )
            
            # Warm up the model with a dummy transcription
            logger.info("Warming up Whisper model...")
            dummy_audio = "/home/nickspi5/Chatty_AI/audio_files/beep.wav"
            if os.path.exists(dummy_audio):
                segments, _ = self.whisper_model.transcribe(dummy_audio)
                _ = list(segments)  # Force evaluation
            
            load_time = time.time() - start_time
            logger.info(f"âœ… Whisper model loaded and warmed up in {load_time:.2f} seconds")
            
            self.update_status({'whisper': 'loaded', 'whisper_time': load_time})
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load Whisper model: {e}")
            self.update_status({'whisper': 'failed', 'whisper_error': str(e)})
            return False
            
    def preload_llama(self):
        """Preload LLaMA model into memory"""
        try:
            logger.info(f"Loading LLaMA model: {LLAMA_MODEL_PATH}")
            start_time = time.time()
            
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH,
                n_ctx=2048,
                n_batch=512,
                n_threads=4,  # Use all 4 cores on RPi5
                temperature=0.7,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                verbose=False,
                mlock=True,  # Lock model in RAM
                use_mmap=True  # Use memory mapping for faster loading
            )
            
            # Warm up the model with a dummy query
            logger.info("Warming up LLaMA model...")
            warmup_prompt = "Hello, this is a test. Please respond briefly."
            response = self.llama_model(warmup_prompt, max_tokens=10)
            
            load_time = time.time() - start_time
            logger.info(f"âœ… LLaMA model loaded and warmed up in {load_time:.2f} seconds")
            
            self.update_status({'llama': 'loaded', 'llama_time': load_time})
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load LLaMA model: {e}")
            self.update_status({'llama': 'failed', 'llama_error': str(e)})
            return False
            
    def preload_piper(self):
        """Preload Piper TTS voice model"""
        try:
            logger.info(f"Preloading Piper voice model: {VOICE_PATH}")
            start_time = time.time()
            
            # Load the voice model file into memory
            if os.path.exists(VOICE_PATH):
                with open(VOICE_PATH, 'rb') as f:
                    voice_data = f.read()
                    
                # Cache the voice model in shared memory
                cache_path = f"{MODEL_CACHE_DIR}/voice_model.onnx"
                with open(cache_path, 'wb') as f:
                    f.write(voice_data)
                    
                # Test Piper with the cached model
                test_text = "System initialized"
                test_output = "/tmp/test_piper.wav"
                
                piper_command = [
                    PIPER_EXECUTABLE,
                    "--model", cache_path,
                    "--config", CONFIG_PATH,
                    "--output_file", test_output
                ]
                
                result = subprocess.run(
                    piper_command,
                    input=test_text,
                    text=True,
                    capture_output=True,
                    timeout=10
                )
                
                if result.returncode == 0:
                    self.voice_model_loaded = True
                    load_time = time.time() - start_time
                    logger.info(f"âœ… Piper voice model loaded and tested in {load_time:.2f} seconds")
                    
                    # Clean up test file
                    if os.path.exists(test_output):
                        os.remove(test_output)
                        
                    self.update_status({'piper': 'loaded', 'piper_time': load_time})
                    return True
                else:
                    raise Exception(f"Piper test failed: {result.stderr}")
                    
            else:
                raise FileNotFoundError(f"Voice model not found: {VOICE_PATH}")
                
        except Exception as e:
            logger.error(f"âŒ Failed to load Piper voice model: {e}")
            self.update_status({'piper': 'failed', 'piper_error': str(e)})
            return False
            
    def preload_face_encodings(self):
        """Preload face recognition encodings"""
        try:
            logger.info(f"Loading face encodings from: {ENCODINGS_FILE}")
            start_time = time.time()
            
            if os.path.exists(ENCODINGS_FILE):
                with open(ENCODINGS_FILE, "rb") as f:
                    self.face_encodings = pickle.loads(f.read())
                    
                # Cache in shared memory for faster access
                cache_path = f"{MODEL_CACHE_DIR}/face_encodings.pkl"
                with open(cache_path, "wb") as f:
                    pickle.dump(self.face_encodings, f)
                    
                num_faces = len(self.face_encodings.get("encodings", []))
                load_time = time.time() - start_time
                logger.info(f"âœ… Loaded {num_faces} face encodings in {load_time:.2f} seconds")
                
                self.update_status({'face_encodings': 'loaded', 'faces': num_faces, 'face_time': load_time})
                return True
            else:
                logger.warning("Face encodings file not found")
                self.update_status({'face_encodings': 'not_found'})
                return False
                
        except Exception as e:
            logger.error(f"âŒ Failed to load face encodings: {e}")
            self.update_status({'face_encodings': 'failed', 'face_error': str(e)})
            return False
            
    def preload_response_files(self):
        """Preload and cache response text files"""
        try:
            logger.info("Caching response files...")
            start_time = time.time()
            
            response_files = [
                "jokes.txt", "fun_facts.txt", "greeting_responses.txt",
                "listening_responses.txt", "waiting_responses.txt",
                "warning_responses.txt", "bored_responses.txt",
                "visitor_greeting_responses.txt", "bored_responses_generic.txt",
                "waiting_responses_generic.txt"
            ]
            
            cached_count = 0
            for filename in response_files:
                filepath = f"/home/nickspi5/Chatty_AI/{filename}"
                if os.path.exists(filepath):
                    with open(filepath, 'r') as f:
                        content = f.read()
                    
                    # Cache in shared memory
                    cache_path = f"{MODEL_CACHE_DIR}/{filename}"
                    with open(cache_path, 'w') as f:
                        f.write(content)
                    cached_count += 1
                    
            load_time = time.time() - start_time
            logger.info(f"âœ… Cached {cached_count} response files in {load_time:.2f} seconds")
            
            self.update_status({'response_files': 'cached', 'files': cached_count, 'cache_time': load_time})
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to cache response files: {e}")
            self.update_status({'response_files': 'failed', 'cache_error': str(e)})
            return False
            
    def optimize_system(self):
        """Apply system optimizations for better performance"""
        try:
            logger.info("Applying system optimizations...")
            
            # Set CPU governor to performance mode
            try:
                subprocess.run(['sudo', 'cpufreq-set', '-g', 'performance'], 
                             capture_output=True, timeout=5)
                logger.info("âœ… CPU governor set to performance mode")
            except:
                logger.warning("Could not set CPU governor (requires sudo)")
                
            # Increase file descriptor limits
            try:
                import resource
                resource.setrlimit(resource.RLIMIT_NOFILE, (4096, 4096))
                logger.info("âœ… File descriptor limit increased")
            except:
                logger.warning("Could not increase file descriptor limit")
                
            # Clear system caches to make room for models
            try:
                subprocess.run(['sync'], capture_output=True, timeout=5)
                logger.info("âœ… System caches synchronized")
            except:
                pass
                
            return True
            
        except Exception as e:
            logger.error(f"System optimization failed: {e}")
            return False
            
    def keep_alive_loop(self):
        """Keep models in memory and prevent them from being paged out"""
        logger.info("Starting keep-alive loop...")
        
        while self.running:
            try:
                # Touch each model periodically to keep in RAM
                if self.whisper_model:
                    # Access model attributes to keep in memory
                    _ = self.whisper_model.model
                    
                if self.llama_model:
                    # Access model context to keep in memory
                    _ = self.llama_model.n_ctx
                    
                # Update heartbeat
                self.update_status({'heartbeat': time.time(), 'status': 'running'})
                
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"Keep-alive error: {e}")
                time.sleep(30)
                
    def run(self):
        """Main preloader process"""
        logger.info("="*60)
        logger.info("ðŸš€ Chatty AI Model Preloader Starting...")
        logger.info("="*60)
        
        # Apply system optimizations
        self.optimize_system()
        
        # Preload all models
        total_start = time.time()
        
        self.preload_whisper()
        self.preload_llama()
        self.preload_piper()
        self.preload_face_encodings()
        self.preload_response_files()
        
        total_time = time.time() - total_start
        
        logger.info("="*60)
        logger.info(f"âœ¨ All models preloaded in {total_time:.2f} seconds")
        logger.info("Models will remain in memory for fast access")
        logger.info("="*60)
        
        # Update final status
        self.update_status({
            'status': 'ready',
            'total_load_time': total_time,
            'timestamp': time.time()
        })
        
        # Keep models in memory
        self.keep_alive_loop()
        
    def cleanup(self):
        """Clean up resources"""
        logger.info("Cleaning up preloader resources...")
        self.running = False
        
        # Update status
        self.update_status({'status': 'stopped'})
        
        # Models will be garbage collected
        self.whisper_model = None
        self.llama_model = None
        self.face_encodings = None


def main():
    """Main entry point"""
    try:
        preloader = ChattyAIPreloader()
        preloader.run()
    except KeyboardInterrupt:
        logger.info("Preloader interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == '__main__':
nickspi5@raspberrypi1:~ $ cat /etc/systemd/system/chatty-ai.service 2>/dev/null
[Unit]
Description=Chatty AI Web Interface
After=network.target

[Service]
Type=simple
User=nickspi5
Group=nickspi5
SupplementaryGroups=audio
WorkingDirectory=/home/nickspi5/Chatty_AI
Environment=PATH=/home/nickspi5/Chatty_AI/chatty-venv/bin:/usr/bin:/bin
Environment=PULSE_RUNTIME_PATH=/run/user/1000/pulse

ExecStart=/home/nickspi5/Chatty_AI/chatty-venv/bin/python /home/nickspi5/Chatty_AI/chatty_ai.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target





chatty-ai-launcher.desktop
[Desktop Entry]
Type=Application
Name=Chatty AI Auto Launcher
Comment=Automatically launch Chatty AI on boot
Exec=/bin/bash /home/nickspi5/Chatty_AI/chatty_ai_startup_final.sh
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Terminal=false
StartupNotify=false


chatty_ai_startup_final.sh
#!/bin/bash
#
# Chatty AI Startup Script - Perfect Fix
#

# Configuration
VIDEO_PATH="/home/nickspi5/Chatty_AI/Chatty_AI_starting.mp4"
LOADING_PAGE="/home/nickspi5/Chatty_AI/templates/chatty_loading.html"
BLACK_PAGE="/home/nickspi5/Chatty_AI/templates/black.html"
CHATTY_URL="http://localhost:5000"
LOG_FILE="/home/nickspi5/Chatty_AI/logs/startup.log"

# Create log directory
mkdir -p /home/nickspi5/Chatty_AI/logs

# Function to log messages
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Start logging
log_message "========================================="
log_message "Chatty AI Startup Sequence Initiated"
log_message "========================================="

# Set display
export DISPLAY=:0
export XAUTHORITY=/home/nickspi5/.Xauthority

# Wait for X server to be ready
log_message "Waiting for X server..."
while ! xset q &>/dev/null; do
    sleep 0.5
done

# Hide desktop elements IMMEDIATELY and AGGRESSIVELY
log_message "Hiding desktop elements..."
pkill lxpanel 2>/dev/null
pkill pcmanfm 2>/dev/null
sleep 0.2

# STEP 1: Show black screen FIRST and IMMEDIATELY
log_message "Displaying black screen..."
chromium-browser \
    --kiosk \
    --noerrdialogs \
    --disable-infobars \
    --disable-session-crashed-bubble \
    --disable-restore-session-state \
    --disable-web-security \
    --start-fullscreen \
    --window-position=0,0 \
    --window-size=1920,1080 \
    "file://$BLACK_PAGE" 2>/dev/null &

BLACK_PID=$!
sleep 2  # Wait for black screen to render

# Ensure desktop is completely hidden
pcmanfm --desktop-off 2>/dev/null

# STEP 2: CLOSE black screen, THEN play video (so video replaces black screen)
log_message "Closing black screen before video..."
kill $BLACK_PID 2>/dev/null
sleep 0.5

if [ -f "$VIDEO_PATH" ]; then
    log_message "Playing startup video..."
    
    # Video is 15.02 seconds, so we know exact duration
    log_message "Video duration: 15 seconds"
    
    # Play video with optimal settings
    cvlc \
        --fullscreen \
        --no-video-title-show \
        --no-osd \
        --quiet \
        --intf dummy \
        --play-and-exit \
        "$VIDEO_PATH" 2>/dev/null &
    
    VLC_PID=$!
    
    # Wait for exact video duration (15 seconds + 1 second buffer)
    sleep 16
    
    # Force kill VLC if still running
    if ps -p $VLC_PID > /dev/null; then
        kill $VLC_PID 2>/dev/null
        sleep 1
    fi
    
    log_message "Video playback complete"
else
    log_message "Video not found at $VIDEO_PATH"
    sleep 15  # Show black screen for 15 seconds if no video
fi

# Kill any remaining VLC processes
pkill vlc 2>/dev/null
pkill cvlc 2>/dev/null
sleep 1

# STEP 3: Show loading screen for FULL 74 seconds
log_message "Launching loading screen for 74 seconds ..."
chromium-browser \
    --kiosk \
    --noerrdialogs \
    --disable-infobars \
    --disable-session-crashed-bubble \
    --disable-restore-session-state \
    --disable-web-security \
    --start-fullscreen \
    "file://$LOADING_PAGE" 2>/dev/null &

LOADING_PID=$!

# Wait for FULL loading screen duration (74 seconds)
log_message "Loading screen running for 74 seconds ..."
sleep 74

# Kill loading screen
log_message "Loading screen complete, closing..."
kill $LOADING_PID 2>/dev/null
sleep 0.5

# STEP 4: Launch main Chatty AI interface
log_message "Launching Chatty AI interface..."
chromium-browser \
    --kiosk \
    --noerrdialogs \
    --disable-infobars \
    --disable-session-crashed-bubble \
    --disable-restore-session-state \
    --disable-web-security \
    --start-fullscreen \
    "$CHATTY_URL" 2>/dev/null &

MAIN_PID=$!
log_message "Chatty AI interface launched successfully"
log_message "========================================="

# Keep the main interface running
wait $MAIN_PID

