

    def query_llama_streaming(self, prompt, callback=None):
        """Generate LLM response with streaming - speaks as tokens arrive"""
        import time as t
        
        print(f"[TIMING] query_llama_streaming called with prompt: '{prompt[:50]}...'")
        start = t.perf_counter()
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            print(f"[TIMING] Starting streaming LLM inference...")
            llm_start = t.perf_counter()
            
            full_response = ""
            current_sentence = ""
            sentences_spoken = 0
            max_sentences = 99
            
            # Stream tokens from LLM
            for token_data in self.llama_model(
                formatted_prompt, 
                max_tokens=200,
                stop=["\n", "\n\n", "User:", "Human:", "Assistant:"],
                stream=True
            ):
                token = token_data['choices'][0]['text']
                full_response += token
                current_sentence += token
                
                # Emit each token to web interface for real-time display
                if callback:
                    callback(token)
                
                # Check if we have a complete sentence
                if any(current_sentence.rstrip().endswith(p) for p in ['.', '!', '?']):
                    sentences_spoken += 1
                    
                    # Log time to first speech
                    if sentences_spoken == 1:
                        first_speech_time = t.perf_counter() - llm_start
                        print(f"[TIMING] Time to first speech: {first_speech_time:.2f}s")
                    
                    # Clean up the sentence
                    sentence_to_speak = current_sentence.strip()
                    sentence_to_speak = re.sub(r"\(.*?\)", "", sentence_to_speak)
                    sentence_to_speak = re.sub(r"(User:|Assistant:)", "", sentence_to_speak)
                    sentence_to_speak = sentence_to_speak.strip()
                    
                    if sentence_to_speak and len(sentence_to_speak) > 2:
                        print(f"[STREAMING] Speaking sentence {sentences_spoken}: '{sentence_to_speak[:50]}...'")
                        self.speak_text(sentence_to_speak)
                    
                    current_sentence = ""
                    
                    # Stop after max sentences
                    if sentences_spoken >= max_sentences:
                        print(f"[STREAMING] Reached {max_sentences} sentences, stopping")
                        break
            
            # Speak any remaining text
            if current_sentence.strip() and sentences_spoken < max_sentences:
                remaining = current_sentence.strip()
                remaining = re.sub(r"\(.*?\)", "", remaining)
                remaining = re.sub(r"(User:|Assistant:)", "", remaining)
                if remaining and len(remaining) > 2:
                    print(f"[STREAMING] Speaking remaining: '{remaining[:50]}...'")
                    self.speak_text(remaining)
            
            llm_time = t.perf_counter() - llm_start
            total_time = t.perf_counter() - start
            print(f"[TIMING] Streaming LLM completed in {llm_time:.2f}s, total: {total_time:.2f}s")
            
            # Clean up full response for return
            full_response = re.sub(r"\(.*?\)", "", full_response)
            full_response = re.sub(r"(User:|Assistant:)", "", full_response)
            
            return full_response.strip()
            
        except Exception as e:
            print(f"[TIMING] Streaming LLM error: {e}")
            return "Sorry, I had trouble processing that question."

