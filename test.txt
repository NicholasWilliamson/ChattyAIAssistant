

Thank you, Claude,

I ran: nickspi5@raspberrypi:~ $ nano /home/nickspi5/Chatty_AI/chatty_ai.py

I then found the existing record_with_silence_detection method in my chatty_ai.py Python script and right after it, I added the with the record_with_streaming_transcription method, that you recommended, as follows:

    def record_with_streaming_transcription(self):
        """Record audio with real-time transcription in chunks"""
        try:
            # Auto-detect working sample rate
            working_sample_rate = None
            for rate in [44100, 48000, 22050, 16000]:
                try:
                    sd.check_input_settings(channels=1, samplerate=rate, dtype='float32')
                    working_sample_rate = rate
                    break
                except:
                    continue
            
            if not working_sample_rate:
                self.emit_log("No compatible sample rate found", 'error')
                return None
            
            with self.audio_recording_lock:
                self.emit_log(f"Streaming recording at {working_sample_rate} Hz", 'debug')
                
                full_audio = []
                full_transcript = ""
                silence_duration = 0
                recording_duration = 0
                chunk_duration = 2.0  # Transcribe every 2 seconds
                last_transcription_time = 0
                check_interval = 0.1
                samples_per_check = int(working_sample_rate * check_interval)
                
                audio_buffer = []
                
                def audio_callback(indata, frames, time_info, status):
                    if status:
                        self.emit_log(f"Audio status: {status}", 'warning')
                    audio_buffer.extend(indata[:, 0])
                
                with sd.InputStream(callback=audio_callback, 
                                  samplerate=working_sample_rate, 
                                  channels=1,
                                  dtype='float32'):
                    
                    while recording_duration < 60:  # Max 60 seconds for longer questions
                        time.sleep(check_interval)
                        recording_duration += check_interval
                        
                        if len(audio_buffer) >= samples_per_check:
                            recent_audio = np.array(audio_buffer[-samples_per_check:])
                            rms = np.sqrt(np.mean(recent_audio**2))
                            
                            if recording_duration >= 1.0:
                                if rms < SILENCE_THRESHOLD:
                                    silence_duration += check_interval
                                else:
                                    if silence_duration > 0.5:
                                        self.emit_log(f"Speech resumed after {silence_duration:.1f}s silence", 'debug')
                                    silence_duration = 0
                                
                                # Log progress every second
                                if int(recording_duration) > int(recording_duration - check_interval):
                                    self.emit_log(f"Recording: {recording_duration:.1f}s | RMS: {rms:.4f} | Silence: {silence_duration:.1f}s", 'debug')
                                
                                # Transcribe chunk every chunk_duration seconds
                                if recording_duration - last_transcription_time >= chunk_duration and len(audio_buffer) > 0:
                                    # Save current audio to temp file
                                    temp_audio = np.array(audio_buffer)
                                    temp_file = "/tmp/streaming_chunk.wav"
                                    
                                    # Resample if needed
                                    if working_sample_rate != 16000:
                                        from scipy import signal
                                        num_samples = int(len(temp_audio) * 16000 / working_sample_rate)
                                        temp_audio = signal.resample(temp_audio, num_samples)
                                    
                                    # Normalize
                                    if np.max(np.abs(temp_audio)) > 0:
                                        temp_audio = temp_audio / np.max(np.abs(temp_audio)) * 0.9
                                    
                                    # Save as WAV
                                    import wave
                                    with wave.open(temp_file, 'w') as wf:
                                        wf.setnchannels(1)
                                        wf.setsampwidth(2)
                                        wf.setframerate(16000)
                                        wf.writeframes((temp_audio * 32767).astype(np.int16).tobytes())
                                    
                                    # Transcribe chunk
                                    chunk_transcript = self.transcribe_audio(temp_file)
                                    if chunk_transcript and chunk_transcript.strip():
                                        full_transcript = chunk_transcript  # Use full audio transcription
                                        self.emit_log(f"[STREAMING STT] Partial: '{full_transcript}'", 'info')
                                        socketio.emit('partial_transcript', {'text': full_transcript})
                                        
                                        # Check for command in partial transcript
                                        command = self.is_command(full_transcript)
                                        if command:
                                            self.emit_log(f"[STREAMING STT] Command detected early: {command}", 'info')
                                            # Save full audio and return
                                            full_audio = list(audio_buffer)
                                            break
                                    
                                    last_transcription_time = recording_duration
                                
                                # Stop on silence
                                if silence_duration >= SILENCE_DURATION:
                                    self.emit_log(f"Silence detected! Recorded {recording_duration:.1f}s", 'debug')
                                    break
                
                # Save final audio
                if len(audio_buffer) > 0:
                    full_audio = np.array(audio_buffer)
                    
                    # Resample to 16kHz for Whisper
                    if working_sample_rate != 16000:
                        from scipy import signal
                        num_samples = int(len(full_audio) * 16000 / working_sample_rate)
                        full_audio = signal.resample(full_audio, num_samples)
                    
                    # Normalize
                    if np.max(np.abs(full_audio)) > 0:
                        full_audio = full_audio / np.max(np.abs(full_audio)) * 0.9
                    
                    # Save final audio
                    import wave
                    with wave.open(WAV_FILENAME, 'w') as wf:
                        wf.setnchannels(1)
                        wf.setsampwidth(2)
                        wf.setframerate(16000)
                        wf.writeframes((full_audio * 32767).astype(np.int16).tobytes())
                    
                    self.emit_log(f"Audio saved: {len(full_audio)/16000:.1f}s", 'debug')
                    return True
                
                return False
                
        except Exception as e:
            self.emit_log(f"Streaming recording error: {e}", 'error')
            import traceback
            traceback.print_exc()
            return False

I then saved and exited to save the changes to my chatty_ai.py Python script.

I then ran: nickspi5@raspberrypi:~ $ cd /home/nickspi5/Chatty_AI               
nickspi5@raspberrypi:~/Chatty_AI $ source chatty-venv/bin/activate
(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ python3 -m py_compile chatty_ai.py && echo "No syntax errors"
No syntax errors
(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ 








