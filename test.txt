âœ… 1. Test wyoming-faster-whisper (Speech-to-Text)
In one terminal:

bash
Copy
Edit
source ~/Chatty_AI/chatty-venv/bin/activate
python3 -m wyoming_faster_whisper \
  --uri tcp://0.0.0.0:10300 \
  --model base \
  --language en
ðŸ”¹ This runs the STT server on localhost:10300 with the Whisper base model.
You can replace base with another model if you have one downloaded.

In another terminal, test it with:

bash
Copy
Edit
source ~/Chatty_AI/chatty-venv/bin/activate
arecord -f cd -t wav -d 5 -r 16000 test.wav
Then you can send test.wav to the Whisper service manually or build a test Python script using wyoming-client. Let me know if you'd like the client test script.

âœ… 2. Test wyoming-piper (Text-to-Speech)
In a terminal:

bash
Copy
Edit
source ~/Chatty_AI/chatty-venv/bin/activate
python3 -m wyoming_piper \
  --uri tcp://0.0.0.0:10200 \
  --voice ./voices/en_US-amy-low/en_US-amy-low.onnx \
  --voice-config ./voices/en_US-amy-low/en_US-amy-low.json
ðŸ”¹ This launches Piper TTS using your downloaded en_US-amy-low voice on port 10200.

Now test TTS by sending it text via a client. Here's a simple Python test client:

python
Copy
Edit
# tts_test.py
from wyoming.client import Client
from wyoming.audio import AudioChunk
from wyoming.tts import Synthesize

import wave
import sounddevice as sd

async def test_tts():
    async with Client.connect("tcp://localhost:10200") as client:
        await client.write(Synthesize(text="Hello, Chatty AI is speaking!"))
        audio = await client.read()

        if isinstance(audio, AudioChunk):
            with wave.open("output.wav", "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(22050)
                wf.writeframes(audio.audio)

            print("Playing audio...")
            sd.play(audio.audio, 22050)
            sd.wait()

import asyncio
asyncio.run(test_tts())
To run:

bash
Copy
Edit
pip install sounddevice
python3 tts_test.py
âœ… 3. Combine STT â†’ LLM â†’ TTS (End-to-End Test)
You can build a full pipeline script that:

Records voice (arecord)

Sends to Whisper (via TCP)

Forwards recognized text to your LLM (TinyLLaMA or Qwen)

Sends response text to Piper for speech

Let me know and Iâ€™ll generate that full chatty_assistant.py pipeline script for you.

âœ… 4. Optional: Confirm your .env is loaded
Make sure .env in ~/Chatty_AI/ has:

dotenv
Copy
Edit
WHISPER_HOST=localhost
WHISPER_PORT=10300
PIPER_HOST=localhost
PIPER_PORT=10200
AUDIO_DEVICE_INDEX=0
SAMPLE_RATE=16000
CHUNK_SIZE=1024
And that you're using python-dotenv in any custom script to load it:

python
Copy
Edit
from dotenv import load_dotenv
load_dotenv()
Would you like me to generate the full end-to-end chat assistant Python script that ties together:

ðŸŽ¤ Microphone (live recording)

ðŸ§  Whisper STT â†’ Local LLM (TinyLLaMA or Qwen2.5)

ðŸ”Š Piper TTS voice response?

Let me know!