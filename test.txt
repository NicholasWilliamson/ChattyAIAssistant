Thank you ChatGPT. You are amazing.

I have my AI Assistant Python script as follows:

run_chatty_ai_1.py
#!/usr/bin/env python3
"""
run_chatty_ai_1.py
Record voice, transcribe using Whisper, reply with TinyLLaMA, and speak with Piper.
"""

import os
import subprocess
import sounddevice as sd
import soundfile as sf
from faster_whisper import WhisperModel
from llama_cpp import Llama

# -------------------------------
# Config
# -------------------------------
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
WAV_FILENAME = "user_input.wav"
RESPONSE_AUDIO = "output.wav"

# -------------------------------
# Record 5 seconds of audio
# -------------------------------
def record_audio(filename=WAV_FILENAME, duration=5, samplerate=16000, channels=1):
    print("üé§ Recording 5s of audio...")
    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=channels, dtype='int16')
    sd.wait()
    sf.write(filename, audio, samplerate)
    print(f"‚úÖ Saved audio to: {filename}")

# -------------------------------
# Transcribe using Whisper
# -------------------------------
def transcribe_audio(filename):
    print("üß† Transcribing with Faster Whisper...")
    model = WhisperModel(WHISPER_MODEL_SIZE, device="cpu", compute_type="int8")
    segments, _ = model.transcribe(filename)
    transcript = " ".join(segment.text for segment in segments).strip()
    print(f"üìù Transcript: {transcript}")
    return transcript

# -------------------------------
# Generate LLM response
# -------------------------------
def query_llama(prompt):
    print("ü§ñ Generating response...")
    try:
        llm = Llama(model_path=LLAMA_MODEL_PATH, n_ctx=2048, temperature=0.7, repeat_penalty=1.1, n_gpu_layers=0, verbose=False)
    except Exception as e:
        print(f"‚ùå Failed to load TinyLLaMA: {e}")
        return "Sorry, I couldn't load the AI model."

    formatted_prompt = f"Answer clearly and concisely:\n{prompt}"

    try:
        result = llm(formatted_prompt, max_tokens=64)
        if "choices" in result and result["choices"]:
            import re
            reply_text = result["choices"][0]["text"].strip()
            reply_text = re.sub(r"\(.*?\)", "", reply_text) # Remove roleplay like (laughs)
            reply_text = re.sub(r"(User:|Assistant:)", "", reply_text) # Remove speaker labels
            reply_text = reply_text.strip()
            print(f"üí¨ Chatty AI says: {reply_text}")
            speak_text(reply_text)
            return reply_text
        else:
            print("‚ö†Ô∏è No response from model.")
            return "I did not understand that."
    except Exception as e:
        print(f"‚ùå Inference failed: {e}")
        return "An error occurred while generating a reply."

# -------------------------------
# Speak with Piper
# -------------------------------
def speak_text(text):
    print("üîä Speaking with Piper ...")
    try:
        command = [
            PIPER_EXECUTABLE,
            "--model", VOICE_PATH,
            "--config", CONFIG_PATH,
            "--output_file", RESPONSE_AUDIO
        ]
        subprocess.run(command, input=text.encode("utf-8"), check=True)
        subprocess.run(["aplay", RESPONSE_AUDIO])
    except subprocess.CalledProcessError as e:
        print("‚ùå Piper playback failed:", e)

# -------------------------------
# Main Orchestration
# -------------------------------
def main():
    record_audio()
    user_text = transcribe_audio(WAV_FILENAME)
    if not user_text:
        print("‚ùå No voice input detected.")
        return
    query_llama(user_text)

if __name__ == "__main__":
    main()

I ran: (chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ python3 run_chatty_ai_1.py
üé§ Recording 5s of audio...
‚úÖ Saved audio to: user_input.wav
üß† Transcribing with Faster Whisper...
üìù Transcript: Tell me some fun facts about Darwin or Australia.
ü§ñ Generating response...
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
üí¨ Chatty AI says: 
üîä Speaking with Piper ...
[2025-07-20 08:16:14.542] [piper] [info] Loaded voice in 0.358937832 second(s)
[2025-07-20 08:16:14.543] [piper] [info] Initialized piper
[2025-07-20 08:16:14.543] [piper] [info] Terminated piper
output.wav: No such file or directory
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ 

There is no output.wav file being created to speak the LLM's response to the user question.

How can we fix this?


