
Thank you, Claude,

I ran the alternative quick test first: (chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ grep -A15 "self.llama_model = Llama" /home/nickspi5/Chatty_AI/chatty_ai.py
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH,
                n_ctx=2048,
                n_threads=4,
                temperature=0.7,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                verbose=False
            )
            self.emit_log("LLaMA model loaded successfully", 'success')
            
            return True
            
        except Exception as e:
            self.emit_log(f"Failed to load models: {e}", 'error')
            return False
(chatty-venv) nickspi5@raspberrypi:~/Chatty_AI $ 











