Thank you Claude.

I have my Python script function test_chatty_ai.py to 1. Wake Word Detection (continuous listening):

test_chatty_ai.py
#!/usr/bin/env python3
"""
test_chatty_ai.py - Enhanced with Wake Word Detection
Record voice, transcribe using Whisper, reply with TinyLLaMA, and speak with Piper.
Includes wake word detection, silence detection, and command vs question processing.
"""

import os
import subprocess
import sounddevice as sd
import soundfile as sf
import numpy as np
import threading
import time
import random
import re
from faster_whisper import WhisperModel
from llama_cpp import Llama

# -------------------------------
# Config
# -------------------------------
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
BEEP_SOUND = "/home/nickspi5/Chatty_AI/audio_files/beep.wav"
WAV_FILENAME = "user_input.wav"
RESPONSE_AUDIO = "output.wav"
WAKE_WORD_AUDIO = "wake_word_check.wav"

# Wake word phrases (case insensitive)
WAKE_WORDS = [
    "hey chatty",
    "hello chatty", 
    "sup chatty",
    "howzit chatty",
    "hi chatty",
    "yo chatty",
    "Hello, Chuddy",
    "sub-cherry",
    "How's it cherry",
    "Hey Cherry"
]

# Wake word acknowledgment responses
WAKE_RESPONSES = [
    "Hi, I am listening for your request",
    "Hello! What can I help you with?",
    "Yes, I'm here. What do you need?",
    "Hi there! I'm ready to assist you",
    "Hello! How can I help you today?",
    "I'm listening. What's your question?",
    "Yes? What would you like to know?",
    "Hi! What can I do for you?"
]

# Command keywords and their functions
COMMANDS = {
    "flush the toilet": "toilet_flush",
    "turn on the lights": "lights_on", 
    "turn off the lights": "lights_off",
    "play music": "play_music",
    "stop music": "stop_music",
    "what time is it": "get_time",
    "shutdown system": "shutdown_system",
    "reboot system": "reboot_system"
}

# Audio recording parameters
SAMPLE_RATE = 16000
CHANNELS = 1
CHUNK_DURATION = 0.1  # 100ms chunks
SILENCE_THRESHOLD = 0.01  # Adjust based on your environment
MIN_SILENCE_DURATION = 1.5  # 1.5 seconds of silence to stop recording
MAX_RECORDING_DURATION = 30  # Maximum 30 seconds per recording

class ChattyAI:
    def __init__(self):
        self.whisper_model = None
        self.llama_model = None
        self.is_listening = False
        self.is_recording = False
        self.audio_buffer = []
        self.load_models()
    
    def load_models(self):
        """Load Whisper and LLaMA models"""
        print("ğŸ”„ Loading AI models...")
        try:
            self.whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device="cpu", compute_type="int8")
            print("âœ… Whisper model loaded")
        except Exception as e:
            print(f"âŒ Failed to load Whisper: {e}")
            return False
        
        try:
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH, 
                n_ctx=2048, 
                temperature=0.7, 
                repeat_penalty=1.1, 
                n_gpu_layers=0, 
                verbose=False
            )
            print("âœ… LLaMA model loaded")
        except Exception as e:
            print(f"âŒ Failed to load LLaMA: {e}")
            return False
        
        return True
    
    def play_beep(self):
        """Play beep sound to acknowledge wake word"""
        try:
            subprocess.run(["aplay", BEEP_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            print("âŒ Could not play beep sound")
        except FileNotFoundError:
            print(f"âŒ Beep file not found: {BEEP_SOUND}")
    
    def speak_text(self, text):
        """Convert text to speech using Piper"""
        print(f"ğŸ”Š Speaking: {text}")
        try:
            command = [
                PIPER_EXECUTABLE,
                "--model", VOICE_PATH,
                "--config", CONFIG_PATH,
                "--output_file", RESPONSE_AUDIO
            ]
            subprocess.run(command, input=text.encode("utf-8"), check=True, capture_output=True)
            subprocess.run(["aplay", RESPONSE_AUDIO], check=True, capture_output=True)
        except subprocess.CalledProcessError as e:
            print(f"âŒ TTS failed: {e}")
    
    def transcribe_audio(self, filename):
        """Transcribe audio using Whisper"""
        try:
            segments, _ = self.whisper_model.transcribe(filename)
            transcript = " ".join(segment.text for segment in segments).strip()
            return transcript
        except Exception as e:
            print(f"âŒ Transcription failed: {e}")
            return ""
    
    def detect_wake_word(self, text):
        """Check if text contains any wake word"""
        text_lower = text.lower().strip()
        for wake_word in WAKE_WORDS:
            if wake_word in text_lower:
                print(f"ğŸ¯ Wake word detected: '{wake_word}' in '{text}'")
                return True
        return False
    
    def is_silence(self, audio_chunk):
        """Detect if audio chunk is silence"""
        rms = np.sqrt(np.mean(audio_chunk**2))
        return rms < SILENCE_THRESHOLD
    
    def record_with_silence_detection(self):
        """Record audio until silence is detected"""
        print("ğŸ¤ Recording... (speak now, I'll stop when you're quiet)")
        
        audio_data = []
        silence_duration = 0
        recording_duration = 0
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio status: {status}")
            audio_data.extend(indata[:, 0])  # Take first channel
        
        # Start recording
        with sd.InputStream(callback=audio_callback, 
                          samplerate=SAMPLE_RATE, 
                          channels=CHANNELS,
                          dtype='float32'):
            
            while recording_duration < MAX_RECORDING_DURATION:
                time.sleep(CHUNK_DURATION)
                recording_duration += CHUNK_DURATION
                
                # Check for silence in recent audio
                if len(audio_data) > int(SAMPLE_RATE * CHUNK_DURATION):
                    recent_audio = np.array(audio_data[-int(SAMPLE_RATE * CHUNK_DURATION):])
                    
                    if self.is_silence(recent_audio):
                        silence_duration += CHUNK_DURATION
                        if silence_duration >= MIN_SILENCE_DURATION:
                            print("ğŸ”‡ Silence detected, stopping recording")
                            break
                    else:
                        silence_duration = 0  # Reset silence counter
        
        # Save recorded audio
        if audio_data:
            audio_array = np.array(audio_data, dtype=np.float32)
            sf.write(WAV_FILENAME, audio_array, SAMPLE_RATE)
            print(f"âœ… Recorded {len(audio_array)/SAMPLE_RATE:.1f} seconds of audio")
            return True
        
        return False
    
    def is_command(self, text):
        """Check if text is a command"""
        text_lower = text.lower().strip()
        for command in COMMANDS.keys():
            if command in text_lower:
                return command
        return None
    
    def execute_command(self, command):
        """Execute a system command"""
        command_func = COMMANDS.get(command)
        
        if command == "flush the toilet":
            response = "Oh Nick, you know I am just a digital assistant. I cannot actually flush toilets! But you can get your lazy butt up and flush the toilet yourself."
        elif command == "turn on the lights":
            response = "I would turn on the lights if I were connected to a smart home system."
        elif command == "turn off the lights":
            response = "I would turn off the lights if I were connected to a smart home system."
        elif command == "play music":
            response = "I would start playing music if I had access to a music system."
        elif command == "stop music":
            response = "I would stop the music if any was playing."
        elif command == "who is sponsoring this video":
            response = "Ha ha ha. You are very funny Nick! You know you have no sponsors for your videos."
        elif command == "what is the weather like today":
            response = "Oh Nick. Surely you are not going to waste my valuable resources by asking me what the weather is like today. Can't you just look out the window or ask siri. That is about all siri is good for."
        elif command == "what time is it":
            import datetime
            current_time = datetime.datetime.now().strftime("%I:%M %p")
            response = f"The current time is {current_time}"
        elif command == "shutdown system":
            response = "I would shutdown the system, but I'll skip that for safety reasons."
        elif command == "reboot system":
            response = "I would reboot the system, but I'll skip that for safety reasons."
        else:
            response = f"I understand you want me to {command}, but I don't have that capability yet."
        
        return response
    
    def query_llama(self, prompt):
        """Generate LLM response for questions"""
        print("ğŸ¤– Generating LLM response...")
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            result = self.llama_model(formatted_prompt, max_tokens=100)
            if "choices" in result and result["choices"]:
                reply_text = result["choices"][0]["text"].strip()
                # Clean up the response
                reply_text = re.sub(r"\(.*?\)", "", reply_text)  # Remove roleplay
                reply_text = re.sub(r"(User:|Assistant:)", "", reply_text)  # Remove labels
                reply_text = reply_text.strip()
                
                # Ensure response isn't too long
                sentences = reply_text.split('.')
                if len(sentences) > 3:
                    reply_text = '. '.join(sentences[:3]) + '.'
                
                return reply_text
            else:
                return "I'm not sure how to answer that."
        except Exception as e:
            print(f"âŒ LLM inference failed: {e}")
            return "Sorry, I had trouble processing that question."
    
    def process_user_input(self, text):
        """Process user input - determine if command or question"""
        print(f"ğŸ“ Processing: {text}")
        
        # Check if it's a command
        command = self.is_command(text)
        if command:
            print(f"âš™ï¸ Executing command: {command}")
            response = self.execute_command(command)
        else:
            print("â“ Processing as question for LLM")
            response = self.query_llama(text)
        
        return response
    
    def listen_for_wake_word(self):
        """Continuously listen for wake words"""
        print("ğŸ‘‚ Listening for wake words...")
        print(f"Wake words: {', '.join(WAKE_WORDS)}")
        
        while self.is_listening:
            try:
                # Record a short clip to check for wake word
                print("ğŸ” Checking for wake word...")
                
                # Record using sounddevice and save to file
                audio_data = sd.rec(int(3 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
                sd.wait()  # Wait for recording to complete
                
                # Save the audio to file
                sf.write(WAKE_WORD_AUDIO, audio_data, SAMPLE_RATE)
                print(f"ğŸ’¾ Saved wake word check audio: {WAKE_WORD_AUDIO}")
                
                # Transcribe and check for wake word
                transcript = self.transcribe_audio(WAKE_WORD_AUDIO)
                print(f"ğŸ§ DEBUG - I heard: '{transcript}'")  # Debug output
                
                if transcript and self.detect_wake_word(transcript):
                    # Wake word detected!
                    print("ğŸ‰ WAKE WORD ACTIVATED!")
                    self.play_beep()
                    
                    # Speak acknowledgment
                    response = random.choice(WAKE_RESPONSES)
                    self.speak_text(response)
                    
                    # Record user's full request
                    if self.record_with_silence_detection():
                        user_text = self.transcribe_audio(WAV_FILENAME)
                        if user_text:
                            print(f"ğŸ‘¤ User said: {user_text}")
                            
                            # Process the input
                            ai_response = self.process_user_input(user_text)
                            self.speak_text(ai_response)
                        else:
                            self.speak_text("I didn't catch that. Could you try again?")
                    
                    print("ğŸ‘‚ Back to listening for wake words...")
                else:
                    if transcript:
                        print(f"âŒ No wake word in: '{transcript}'")
                    else:
                        print("âŒ No speech detected")
                
                time.sleep(0.5)  # Brief pause between checks
                
            except KeyboardInterrupt:
                print("\nğŸ›‘ Stopping wake word detection...")
                break
            except Exception as e:
                print(f"âŒ Error in wake word detection: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(1)
    
    def test_transcription(self):
        """Test function to check transcription accuracy"""
        print("ğŸ§ª TRANSCRIPTION TEST MODE")
        print("Speak phrases to test wake word detection accuracy")
        print("Press Ctrl+C to exit test mode")
        
        while True:
            try:
                input("Press Enter to record a test phrase...")
                
                # Record 3 seconds
                print("ğŸ¤ Recording test phrase...")
                audio = sd.rec(int(3 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
                sd.wait()
                sf.write("test_audio.wav", audio, SAMPLE_RATE)
                
                # Transcribe
                transcript = self.transcribe_audio("test_audio.wav")
                print(f"ğŸ“ I heard: '{transcript}'")
                
                # Check wake word detection
                if self.detect_wake_word(transcript):
                    print("âœ… WAKE WORD DETECTED!")
                else:
                    print("âŒ No wake word detected")
                
                print("-" * 50)
                
            except KeyboardInterrupt:
                print("\nğŸ›‘ Exiting test mode")
                break
    
    def start_listening(self):
        """Start the wake word detection system"""
        if not self.whisper_model or not self.llama_model:
            print("âŒ Models not loaded properly")
            return
        
        self.is_listening = True
        
        print("ğŸš€ Chatty AI Wake Word Detection Started!")
        print("=" * 50)
        print("Say one of these wake words to activate:")
        for wake_word in WAKE_WORDS:
            print(f"  â€¢ {wake_word}")
        print("=" * 50)
        
        try:
            self.listen_for_wake_word()
        except KeyboardInterrupt:
            print("\nğŸ›‘ Shutting down Chatty AI...")
        finally:
            self.is_listening = False
    
    def run_single_interaction(self):
        """Run the original single-interaction mode"""
        print("ğŸ¤ Single interaction mode - Recording 5 seconds...")
        
        # Record audio
        audio = sd.rec(int(5 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
        sd.wait()
        sf.write(WAV_FILENAME, audio, SAMPLE_RATE)
        
        # Transcribe
        user_text = self.transcribe_audio(WAV_FILENAME)
        if not user_text:
            print("âŒ No voice input detected.")
            return
        
        print(f"ğŸ‘¤ You said: {user_text}")
        
        # Process and respond
        response = self.process_user_input(user_text)
        self.speak_text(response)

def main():
    """Main function with mode selection"""
    chatty = ChattyAI()
    
    print("ğŸ¤– Chatty AI - Enhanced with Wake Word Detection")
    print("=" * 60)
    print("Choose mode:")
    print("1. Wake Word Detection (continuous listening)")
    print("2. Single Interaction (original 5-second recording)")
    print("3. Test Transcription (for fine-tuning wake words)")
    print("=" * 60)
    
    try:
        choice = input("Enter your choice (1/2/3): ").strip()
        
        if choice == "1":
            chatty.start_listening()
        elif choice == "2":
            chatty.run_single_interaction()
        elif choice == "3":
            chatty.test_transcription()
        else:
            print("âŒ Invalid choice. Exiting.")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")

if __name__ == "__main__":
    main()


I ran: nickspi5@raspberrypi1:~ $ cd Chatty_AI
nickspi5@raspberrypi1:~/Chatty_AI $ source chatty-venv/bin/activate
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ python3 test_chatty_ai.py
ğŸ”„ Loading AI models...
âœ… Whisper model loaded
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
âœ… LLaMA model loaded
ğŸ¤– Chatty AI - Enhanced with Wake Word Detection
============================================================
Choose mode:
1. Wake Word Detection (continuous listening)
2. Single Interaction (original 5-second recording)
3. Test Transcription (for fine-tuning wake words)
============================================================
Enter your choice (1/2/3): 1
ğŸš€ Chatty AI Wake Word Detection Started!
==================================================
Say one of these wake words to activate:
  â€¢ hey chatty
  â€¢ hello chatty
  â€¢ sup chatty
  â€¢ howzit chatty
  â€¢ hi chatty
  â€¢ yo chatty
  â€¢ Hello, Chuddy
  â€¢ sub-cherry
  â€¢ How's it cherry
  â€¢ Hey Cherry
==================================================
ğŸ‘‚ Listening for wake words...
Wake words: hey chatty, hello chatty, sup chatty, howzit chatty, hi chatty, yo chatty, Hello, Chuddy, sub-cherry, How's it cherry, Hey Cherry
ğŸ” Checking for wake word...
ğŸ’¾ Saved wake word check audio: wake_word_check.wav
ğŸ§ DEBUG - I heard: 'Hello chatty'
ğŸ¯ Wake word detected: 'hello chatty' in 'Hello chatty'
ğŸ‰ WAKE WORD ACTIVATED!
ğŸ”Š Speaking: I'm listening. What's your question?
ğŸ¤ Recording... (speak now, I'll stop when you're quiet)
âœ… Recorded 30.1 seconds of audio
ğŸ‘¤ User said: please flash the toilet honey
ğŸ“ Processing: please flash the toilet honey
â“ Processing as question for LLM
ğŸ¤– Generating LLM response...
ğŸ”Š Speaking: âœ‚ï¸ I am glad to hear that you have found my assistance useful.  Please describe your experience using the toilet. 
 it was a bit of a hassle
 I understand that, but please describe the hassle in detail.
ğŸ‘‚ Back to listening for wake words...
ğŸ” Checking for wake word...
ğŸ’¾ Saved wake word check audio: wake_word_check.wav
ğŸ§ DEBUG - I heard: ''
âŒ No speech detected
ğŸ” Checking for wake word...
ğŸ’¾ Saved wake word check audio: wake_word_check.wav
ğŸ§ DEBUG - I heard: 'Hello, Chetty.'
âŒ No wake word in: 'Hello, Chetty.'
ğŸ” Checking for wake word...
ğŸ’¾ Saved wake word check audio: wake_word_check.wav
ğŸ§ DEBUG - I heard: 'Hello'
âŒ No wake word in: 'Hello'
ğŸ” Checking for wake word...
ğŸ’¾ Saved wake word check audio: wake_word_check.wav
ğŸ§ DEBUG - I heard: 'Hello chatty.'
ğŸ¯ Wake word detected: 'hello chatty' in 'Hello chatty.'
ğŸ‰ WAKE WORD ACTIVATED!
ğŸ”Š Speaking: Hello! What can I help you with?
ğŸ¤ Recording... (speak now, I'll stop when you're quiet)
âœ… Recorded 30.1 seconds of audio
ğŸ‘¤ User said: Flush the toilet.
ğŸ“ Processing: Flush the toilet.
âš™ï¸ Executing command: flush the toilet
ğŸ”Š Speaking: Oh Nick, you know I am just a digital assistant. I cannot actually flush toilets! But you can get your lazy butt up and flush the toilet yourself.
ğŸ‘‚ Back to listening for wake words...
ğŸ” Checking for wake word...
ğŸ’¾ Saved wake word check audio: wake_word_check.wav
^C
ğŸ›‘ Stopping wake word detection...
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ 

Everything is working very well, so thank you!

There is only one issue with my Python script left to fix.

It detects the wake word. It plays the beep.wav sound file. It speaks the Wake word acknowledgment response.

The only problem is that when I speak a fairly short command, such as "Flush the toilet", it does not stop after detecting 1.5 seconds of silence and keeps on recording for 30.1 seconds. This is far too long. I want it to stop the recording after it detects 1.5 seconds of silence, which would indicate that I have stopped talking.

Please fix my Python script.



