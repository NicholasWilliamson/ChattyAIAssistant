
Thank you, Claude,

I ran: nickspi5@raspberrypi:~ $ mkdir -p /home/nickspi5/.config/autostart
nickspi5@raspberrypi:~ $ cat > /home/nickspi5/.config/autostart/chatty-ai-launcher.desktop << 'EOF'
[Desktop Entry]
Type=Application
Name=Chatty AI Auto Launcher
Comment=Automatically launch Chatty AI on boot
Exec=/bin/bash /home/nickspi5/Chatty_AI/chatty_ai_startup.sh
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Terminal=false
StartupNotify=false
EOF
nickspi5@raspberrypi:~ $ chmod +x /home/nickspi5/.config/autostart/chatty-ai-launcher.desktop
nickspi5@raspberrypi:~ $ sudo systemctl disable chatty-ai-launcher.service
Removed '/etc/systemd/system/graphical.target.wants/chatty-ai-launcher.service'.
nickspi5@raspberrypi:~ $ sudo systemctl stop chatty-ai-launcher.service
nickspi5@raspberrypi:~ $ sudo systemctl disable chatty-ai.service
Removed '/etc/systemd/system/multi-user.target.wants/chatty-ai.service'.
nickspi5@raspberrypi:~ $ sudo systemctl stop chatty-ai.service

I then renamed my /home/nickspi5/Chatty_AI/chatty_ai_startup_final.sh file as /home/nickspi5/Chatty_AI/chatty_ai_startup.sh

I then ran: nickspi5@raspberrypi:~ $ chmod +x /home/nickspi5/Chatty_AI/chatty_ai_startup.sh
nickspi5@raspberrypi:~ $ sudo nano /etc/sysytemd/system/chatty-ai.service
nickspi5@raspberrypi:~ $ sudo nano /etc/systemd/system/chatty-ai.service  
nickspi5@raspberrypi:~ $ sudo systemctl disable chatty-ai-preloader.service
Removed '/etc/systemd/system/multi-user.target.wants/chatty-ai-preloader.service'.
nickspi5@raspberrypi:~ $ sudo systemctl stop chatty-ai-preloader.service
nickspi5@raspberrypi:~ $ sudo nano /etc/systemd/system/chatty-ai-preloader.service

My files are now as follows:

/home/nickspi5/.config/autostart/chatty-ai-launcher.desktop
[Desktop Entry]
Type=Application
Name=Chatty AI Auto Launcher
Comment=Automatically launch Chatty AI on boot
Exec=/bin/bash /home/nickspi5/Chatty_AI/chatty_ai_startup.sh
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Terminal=false
StartupNotify=false

/home/nickspi5/Chatty_AI/chatty_ai_startup.sh
#!/bin/bash
#
# Chatty AI Startup Script
#

# Configuration
VIDEO_PATH="/home/nickspi5/Chatty_AI/Chatty_AI_starting.mp4"
LOADING_PAGE="/home/nickspi5/Chatty_AI/templates/chatty_loading.html"
BLACK_PAGE="/home/nickspi5/Chatty_AI/templates/black.html"
CHATTY_URL="http://localhost:5000"
LOG_FILE="/home/nickspi5/Chatty_AI/logs/startup.log"

# Create log directory
mkdir -p /home/nickspi5/Chatty_AI/logs

# Function to log messages
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Start logging
log_message "========================================="
log_message "Chatty AI Startup Sequence Initiated"
log_message "========================================="

# Set display
export DISPLAY=:0
export XAUTHORITY=/home/nickspi5/.Xauthority

# Wait for X server to be ready
log_message "Waiting for X server..."
while ! xset q &>/dev/null; do
    sleep 0.5
done

# Hide desktop elements
log_message "Hiding desktop elements..."
pkill lxpanel 2>/dev/null
pkill pcmanfm 2>/dev/null
sleep 0.2

# STEP 1: Show black screen
log_message "Displaying black screen..."
chromium-browser \
    --kiosk \
    --noerrdialogs \
    --disable-infobars \
    --disable-session-crashed-bubble \
    --disable-restore-session-state \
    --disable-web-security \
    --start-fullscreen \
    --window-position=0,0 \
    --window-size=1920,1080 \
    "file://$BLACK_PAGE" 2>/dev/null &

BLACK_PID=$!
sleep 2  # Wait for black screen to render

# Ensure desktop is completely hidden
pcmanfm --desktop-off 2>/dev/null

# STEP 2: CLOSE black screen, THEN play video (so video replaces black screen)
log_message "Closing black screen before video..."
kill $BLACK_PID 2>/dev/null
sleep 0.5

if [ -f "$VIDEO_PATH" ]; then
    log_message "Playing startup video..."
    
    # Video is 15.02 seconds, so we know exact duration
    log_message "Video duration: 15 seconds"
    
    # Play video with optimal settings
    cvlc \
        --fullscreen \
        --no-video-title-show \
        --no-osd \
        --quiet \
        --intf dummy \
        --play-and-exit \
        "$VIDEO_PATH" 2>/dev/null &
    
    VLC_PID=$!
    
    # Wait for exact video duration (15 seconds + 1 second buffer)
    sleep 16
    
    # Force kill VLC if still running
    if ps -p $VLC_PID > /dev/null; then
        kill $VLC_PID 2>/dev/null
        sleep 1
    fi
    
    log_message "Video playback complete"
else
    log_message "Video not found at $VIDEO_PATH"
    sleep 15  # Show black screen for 15 seconds if no video
fi

# Kill any remaining VLC processes
pkill vlc 2>/dev/null
pkill cvlc 2>/dev/null
sleep 1

# STEP 3: Show loading screen for FULL 74 seconds
log_message "Launching loading screen for 74 seconds ..."
chromium-browser \
    --kiosk \
    --noerrdialogs \
    --disable-infobars \
    --disable-session-crashed-bubble \
    --disable-restore-session-state \
    --disable-web-security \
    --start-fullscreen \
    "file://$LOADING_PAGE" 2>/dev/null &

LOADING_PID=$!

# Wait for FULL loading screen duration (74 seconds)
log_message "Loading screen running for 74 seconds ..."
sleep 74

# Kill loading screen
log_message "Loading screen complete, closing..."
kill $LOADING_PID 2>/dev/null
sleep 0.5

# STEP 4: Launch main Chatty AI interface
log_message "Launching Chatty AI interface..."
chromium-browser \
    --kiosk \
    --noerrdialogs \
    --disable-infobars \
    --disable-session-crashed-bubble \
    --disable-restore-session-state \
    --disable-web-security \
    --start-fullscreen \
    "$CHATTY_URL" 2>/dev/null &

MAIN_PID=$!
log_message "Chatty AI interface launched successfully"
log_message "========================================="

# Keep the main interface running
wait $MAIN_PID

/etc/systemd/system/chatty-ai-preloader.service
[Unit]
Description=Chatty AI Model Preloader Service
After=network.target
Before=chatty-ai.service
Wants=network-online.target

[Service]
Type=simple
User=nickspi5
Group=nickspi5
WorkingDirectory=/home/nickspi5/Chatty_AI
Environment=PATH=/home/nickspi5/Chatty_AI/chatty-venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Environment=PYTHONPATH=/home/nickspi5/Chatty_AI
Environment=PYTHONUNBUFFERED=1
Environment=OMP_NUM_THREADS=4
Environment=MKL_NUM_THREADS=4

# Run the preloader script
ExecStart=/home/nickspi5/Chatty_AI/chatty-venv/bin/python /home/nickspi5/Chatty_AI/chatty_ai_preloader.py

# Restart policy
Restart=always
RestartSec=10
StartLimitInterval=60
StartLimitBurst=3

# Resource limits for better performance
LimitNOFILE=4096
Nice=-10
IOSchedulingClass=best-effort
IOSchedulingPriority=0

# Memory settings
MemoryMax=4G
MemorySwapMax=0

# Security settings
NoNewPrivileges=true
PrivateTmp=false

[Install]
WantedBy=multi-user.target

/home/nickspi5/Chatty_AI/chatty_ai_preloader.py
#!/usr/bin/env python3
"""
chatty_ai_preloader.py - Preload AI models and keep them in memory
This service runs at boot to preload all AI models for faster response times
"""

import os
import sys
import time
import logging
import signal
import mmap
import pickle
import threading
import subprocess
from pathlib import Path
from faster_whisper import WhisperModel
from llama_cpp import Llama

# Configuration
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "/home/nickspi5/Chatty_AI/tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
ENCODINGS_FILE = "/home/nickspi5/Chatty_AI/encodings.pickle"

# Shared memory paths for model state
SHARED_MEM_DIR = "/dev/shm/chatty_ai"
PRELOAD_STATUS_FILE = f"{SHARED_MEM_DIR}/preload_status"
MODEL_CACHE_DIR = f"{SHARED_MEM_DIR}/model_cache"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/home/nickspi5/Chatty_AI/logs/preloader.log')
    ]
)
logger = logging.getLogger(__name__)

class ChattyAIPreloader:
    def __init__(self):
        self.running = True
        self.whisper_model = None
        self.llama_model = None
        self.face_encodings = None
        self.voice_model_loaded = False
        
        # Create shared memory directories
        os.makedirs(SHARED_MEM_DIR, exist_ok=True)
        os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
        os.makedirs('/home/nickspi5/Chatty_AI/logs', exist_ok=True)
        
        # Signal handling
        signal.signal(signal.SIGTERM, self.signal_handler)
        signal.signal(signal.SIGINT, self.signal_handler)
        
    def signal_handler(self, sig, frame):
        """Handle shutdown signals"""
        logger.info("Received shutdown signal")
        self.running = False
        self.cleanup()
        sys.exit(0)
        
    def update_status(self, status_dict):
        """Update preload status in shared memory"""
        try:
            with open(PRELOAD_STATUS_FILE, 'w') as f:
                f.write(str(status_dict))
            logger.info(f"Status updated: {status_dict}")
        except Exception as e:
            logger.error(f"Failed to update status: {e}")
            
    def preload_whisper(self):
        """Preload Whisper model into memory"""
        try:
            logger.info(f"Loading Whisper model: {WHISPER_MODEL_SIZE}")
            start_time = time.time()
            
            self.whisper_model = WhisperModel(
                WHISPER_MODEL_SIZE, 
                device="cpu", 
                compute_type="int8",
                download_root="/home/nickspi5/Chatty_AI/.cache/whisper"
            )
            
            # Warm up the model with a dummy transcription
            logger.info("Warming up Whisper model...")
            dummy_audio = "/home/nickspi5/Chatty_AI/audio_files/beep.wav"
            if os.path.exists(dummy_audio):
                segments, _ = self.whisper_model.transcribe(dummy_audio)
                _ = list(segments)  # Force evaluation
            
            load_time = time.time() - start_time
            logger.info(f"‚úÖ Whisper model loaded and warmed up in {load_time:.2f} seconds")
            
            self.update_status({'whisper': 'loaded', 'whisper_time': load_time})
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load Whisper model: {e}")
            self.update_status({'whisper': 'failed', 'whisper_error': str(e)})
            return False
            
    def preload_llama(self):
        """Preload LLaMA model into memory"""
        try:
            logger.info(f"Loading LLaMA model: {LLAMA_MODEL_PATH}")
            start_time = time.time()
            
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH,
                n_ctx=2048,
                n_batch=512,
                n_threads=4,  # Use all 4 cores on RPi5
                temperature=0.7,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                verbose=False,
                mlock=True,  # Lock model in RAM
                use_mmap=True  # Use memory mapping for faster loading
            )
            
            # Warm up the model with a dummy query
            logger.info("Warming up LLaMA model...")
            warmup_prompt = "Hello, this is a test. Please respond briefly."
            response = self.llama_model(warmup_prompt, max_tokens=10)
            
            load_time = time.time() - start_time
            logger.info(f"‚úÖ LLaMA model loaded and warmed up in {load_time:.2f} seconds")
            
            self.update_status({'llama': 'loaded', 'llama_time': load_time})
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load LLaMA model: {e}")
            self.update_status({'llama': 'failed', 'llama_error': str(e)})
            return False
            
    def preload_piper(self):
        """Preload Piper TTS voice model"""
        try:
            logger.info(f"Preloading Piper voice model: {VOICE_PATH}")
            start_time = time.time()
            
            # Load the voice model file into memory
            if os.path.exists(VOICE_PATH):
                with open(VOICE_PATH, 'rb') as f:
                    voice_data = f.read()
                    
                # Cache the voice model in shared memory
                cache_path = f"{MODEL_CACHE_DIR}/voice_model.onnx"
                with open(cache_path, 'wb') as f:
                    f.write(voice_data)
                    
                # Test Piper with the cached model
                test_text = "System initialized"
                test_output = "/tmp/test_piper.wav"
                
                piper_command = [
                    PIPER_EXECUTABLE,
                    "--model", cache_path,
                    "--config", CONFIG_PATH,
                    "--output_file", test_output
                ]
                
                result = subprocess.run(
                    piper_command,
                    input=test_text,
                    text=True,
                    capture_output=True,
                    timeout=10
                )
                
                if result.returncode == 0:
                    self.voice_model_loaded = True
                    load_time = time.time() - start_time
                    logger.info(f"‚úÖ Piper voice model loaded and tested in {load_time:.2f} seconds")
                    
                    # Clean up test file
                    if os.path.exists(test_output):
                        os.remove(test_output)
                        
                    self.update_status({'piper': 'loaded', 'piper_time': load_time})
                    return True
                else:
                    raise Exception(f"Piper test failed: {result.stderr}")
                    
            else:
                raise FileNotFoundError(f"Voice model not found: {VOICE_PATH}")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to load Piper voice model: {e}")
            self.update_status({'piper': 'failed', 'piper_error': str(e)})
            return False

    def preload_face_encodings(self):
        """Preload face recognition encodings"""
        try:
            logger.info(f"Loading face encodings from: {ENCODINGS_FILE}")
            start_time = time.time()
            
            if os.path.exists(ENCODINGS_FILE):
                with open(ENCODINGS_FILE, "rb") as f:
                    self.face_encodings = pickle.loads(f.read())
                    
                # Cache in shared memory for faster access
                cache_path = f"{MODEL_CACHE_DIR}/face_encodings.pkl"
                with open(cache_path, "wb") as f:
                    pickle.dump(self.face_encodings, f)
                    
                num_faces = len(self.face_encodings.get("encodings", []))
                load_time = time.time() - start_time
                logger.info(f"‚úÖ Loaded {num_faces} face encodings in {load_time:.2f} seconds")
                
                self.update_status({'face_encodings': 'loaded', 'faces': num_faces, 'face_time': load_time})
                return True
            else:
                logger.warning("Face encodings file not found")
                self.update_status({'face_encodings': 'not_found'})
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Failed to load face encodings: {e}")
            self.update_status({'face_encodings': 'failed', 'face_error': str(e)})
            return False
            
    def preload_response_files(self):
        """Preload and cache response text files"""
        try:
            logger.info("Caching response files...")
            start_time = time.time()
            
            response_files = [
                "jokes.txt", "fun_facts.txt", "greeting_responses.txt",
                "listening_responses.txt", "waiting_responses.txt",
                "warning_responses.txt", "bored_responses.txt",
                "visitor_greeting_responses.txt", "bored_responses_generic.txt",
                "waiting_responses_generic.txt"
            ]
            
            cached_count = 0
            for filename in response_files:
                filepath = f"/home/nickspi5/Chatty_AI/{filename}"
                if os.path.exists(filepath):
                    with open(filepath, 'r') as f:
                        content = f.read()
                    
                    # Cache in shared memory
                    cache_path = f"{MODEL_CACHE_DIR}/{filename}"
                    with open(cache_path, 'w') as f:
                        f.write(content)
                    cached_count += 1
                    
            load_time = time.time() - start_time
            logger.info(f"‚úÖ Cached {cached_count} response files in {load_time:.2f} seconds")
            
            self.update_status({'response_files': 'cached', 'files': cached_count, 'cache_time': load_time})
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to cache response files: {e}")
            self.update_status({'response_files': 'failed', 'cache_error': str(e)})
            return False
            
    def optimize_system(self):
        """Apply system optimizations for better performance"""
        try:
            logger.info("Applying system optimizations...")
            
            # Set CPU governor to performance mode
            try:
                subprocess.run(['sudo', 'cpufreq-set', '-g', 'performance'], 
                             capture_output=True, timeout=5)
                logger.info("‚úÖ CPU governor set to performance mode")
            except:
                logger.warning("Could not set CPU governor (requires sudo)")
                
            # Increase file descriptor limits
            try:
                import resource
                resource.setrlimit(resource.RLIMIT_NOFILE, (4096, 4096))
                logger.info("‚úÖ File descriptor limit increased")
            except:
                logger.warning("Could not increase file descriptor limit")
                
            # Clear system caches to make room for models
            try:
                subprocess.run(['sync'], capture_output=True, timeout=5)
                logger.info("‚úÖ System caches synchronized")
            except:
                pass
                
            return True
            
        except Exception as e:
            logger.error(f"System optimization failed: {e}")
            return False
            
    def keep_alive_loop(self):
        """Keep models in memory and prevent them from being paged out"""
        logger.info("Starting keep-alive loop...")
        
        while self.running:
            try:
                # Touch each model periodically to keep in RAM
                if self.whisper_model:
                    # Access model attributes to keep in memory
                    _ = self.whisper_model.model
                    
                if self.llama_model:
                    # Access model context to keep in memory
                    _ = self.llama_model.n_ctx
                    
                # Update heartbeat
                self.update_status({'heartbeat': time.time(), 'status': 'running'})
                
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"Keep-alive error: {e}")
                time.sleep(30)
                
    def run(self):
        """Main preloader process"""
        logger.info("="*60)
        logger.info("üöÄ Chatty AI Model Preloader Starting...")
        logger.info("="*60)
        
        # Apply system optimizations
        self.optimize_system()
        
        # Preload all models
        total_start = time.time()
        
        self.preload_whisper()
        self.preload_llama()
        self.preload_piper()
        self.preload_face_encodings()
        self.preload_response_files()
        
        total_time = time.time() - total_start
        
        logger.info("="*60)
        logger.info(f"‚ú® All models preloaded in {total_time:.2f} seconds")
        logger.info("Models will remain in memory for fast access")
        logger.info("="*60)
        
        # Update final status
        self.update_status({
            'status': 'ready',
            'total_load_time': total_time,
            'timestamp': time.time()
        })
        
        # Keep models in memory
        self.keep_alive_loop()
        
    def cleanup(self):
        """Clean up resources"""
        logger.info("Cleaning up preloader resources...")
        self.running = False
        
        # Update status
        self.update_status({'status': 'stopped'})
        
        # Models will be garbage collected
        self.whisper_model = None
        self.llama_model = None
        self.face_encodings = None


def main():
    """Main entry point"""
    try:
        preloader = ChattyAIPreloader()
        preloader.run()
    except KeyboardInterrupt:
        logger.info("Preloader interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()

/etc/systemd/system/chatty-ai.service
[Unit]
Description=Chatty AI Web Server
After=network.target chatty-ai-preloader.service
Before=pironman5.service
Wants=chatty-ai-preloader.service

[Service]
Type=simple
User=nickspi5
Group=nickspi5
SupplementaryGroups=audio
WorkingDirectory=/home/nickspi5/Chatty_AI
Environment=PATH=/home/nickspi5/Chatty_AI/chatty-venv/bin:/usr/bin:/bin
Environment=PULSE_RUNTIME_PATH=/run/user/1000/pulse
Environment=PYTHONPATH=/home/nickspi5/Chatty_AI

ExecStart=/home/nickspi5/Chatty_AI/chatty-venv/bin/python /home/nickspi5/Chatty_AI/chatty_ai.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target

How can I restart my chatty-ai.service and chatty-ai-preloader.service services, so that I can reboot my Raspberry PI 5 and check if it is all working properly?



