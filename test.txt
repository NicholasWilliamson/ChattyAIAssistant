Thank you, Claude.

I tested my PI Camera Module 2 camera using the following Python script:

test_camera.py
#!/usr/bin/env python3
"""
test_camera.py - Simple camera test for Chatty AI
This script tests basic camera functionality
"""

import sys
import time
import cv2
import numpy as np
from datetime import datetime

def test_camera():
    """Test camera functionality"""
    print("🎥 Testing Raspberry Pi Camera...")
    print("=" * 50)
    
    try:
        # Try to import Picamera2
        print("📦 Importing Picamera2...")
        from picamera2 import Picamera2
        print("✅ Picamera2 imported successfully")
        
        # Create camera instance
        print("🔧 Creating camera instance...")
        picam2 = Picamera2()
        print("✅ Camera instance created")
        
        # Configure camera
        print("⚙️ Configuring camera...")
        config = picam2.create_preview_configuration(
            main={"format": 'XRGB8888', "size": (640, 480)}
        )
        picam2.configure(config)
        print("✅ Camera configured")
        
        # Start camera
        print("🚀 Starting camera...")
        picam2.start()
        print("✅ Camera started")
        
        # Wait for camera to warm up
        print("⏱️ Waiting for camera to warm up...")
        time.sleep(3)
        
        # Test capture
        print("📸 Testing capture...")
        for i in range(5):
            try:
                frame = picam2.capture_array()
                print(f"✅ Capture {i+1}: Shape={frame.shape}, dtype={frame.dtype}")
                
                # Convert and save test image
                if len(frame.shape) == 3:
                    if frame.shape[2] == 4:  # RGBA
                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)
                    elif frame.shape[2] == 3:  # RGB
                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    else:
                        frame_bgr = frame
                    
                    # Add timestamp overlay
                    timestamp = datetime.now().strftime("%H:%M:%S")
                    cv2.putText(frame_bgr, f"Test {i+1} - {timestamp}", (10, 30), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    
                    # Save test image
                    filename = f"camera_test_{i+1}.jpg"
                    cv2.imwrite(filename, frame_bgr)
                    print(f"💾 Saved test image: {filename}")
                
                time.sleep(1)
                
            except Exception as e:
                print(f"❌ Capture {i+1} failed: {e}")
        
        # Stop camera
        print("🛑 Stopping camera...")
        picam2.stop()
        print("✅ Camera stopped successfully")
        
        print("\n🎉 Camera test completed successfully!")
        print("✅ Your camera is working properly")
        print("📁 Check the saved test images in the current directory")
        
        return True
        
    except ImportError as e:
        print(f"❌ Failed to import Picamera2: {e}")
        print("💡 Make sure you have picamera2 installed: pip install picamera2")
        return False
        
    except Exception as e:
        print(f"❌ Camera test failed: {e}")
        import traceback
        print(f"📋 Full traceback:\n{traceback.format_exc()}")
        return False

def test_opencv():
    """Test OpenCV functionality"""
    print("\n👁️ Testing OpenCV...")
    print("=" * 50)
    
    try:
        print(f"📦 OpenCV version: {cv2.__version__}")
        
        # Test creating an image
        test_img = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(test_img, "OpenCV Test", (200, 240), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Test encoding
        ret, buffer = cv2.imencode('.jpg', test_img)
        if ret:
            print("✅ OpenCV image encoding test passed")
            cv2.imwrite("opencv_test.jpg", test_img)
            print("💾 Saved OpenCV test image: opencv_test.jpg")
        else:
            print("❌ OpenCV image encoding failed")
            return False
        
        print("✅ OpenCV is working correctly")
        return True
        
    except Exception as e:
        print(f"❌ OpenCV test failed: {e}")
        return False

def test_audio_permissions():
    """Test audio permissions"""
    print("\n🔊 Testing audio setup...")
    print("=" * 50)
    
    try:
        import sounddevice as sd
        print("✅ sounddevice imported successfully")
        
        # Query audio devices
        devices = sd.query_devices()
        print("🎤 Available audio devices:")
        for i, device in enumerate(devices):
            print(f"  {i}: {device['name']} - {device['max_input_channels']} in, {device['max_output_channels']} out")
        
        return True
        
    except Exception as e:
        print(f"❌ Audio test failed: {e}")
        return False

if __name__ == "__main__":
    print("🧪 Chatty AI Component Tests")
    print("=" * 50)
    
    all_passed = True
    
    # Run tests
    if not test_opencv():
        all_passed = False
    
    if not test_camera():
        all_passed = False
    
    if not test_audio_permissions():
        all_passed = False
    
    print("\n" + "=" * 50)
    if all_passed:
        print("🎉 ALL TESTS PASSED!")
        print("✅ Your system should work with the web interface")
    else:
        print("❌ SOME TESTS FAILED")
        print("🔧 Please fix the issues above before running the web interface")
    
    print("=" * 50)

I ran: (chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ python3 test_camera.py
🧪 Chatty AI Component Tests
==================================================

👁️ Testing OpenCV...
==================================================
📦 OpenCV version: 4.11.0
✅ OpenCV image encoding test passed
💾 Saved OpenCV test image: opencv_test.jpg
✅ OpenCV is working correctly
🎥 Testing Raspberry Pi Camera...
==================================================
📦 Importing Picamera2...
✅ Picamera2 imported successfully
🔧 Creating camera instance...
[1:57:43.776441511] [3852]  INFO Camera camera_manager.cpp:326 libcamera v0.5.1+100-e53bdf1f
[1:57:43.783402296] [3862]  INFO RPI pisp.cpp:720 libpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)
[1:57:43.792716713] [3862]  INFO RPI pisp.cpp:1179 Registered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 to CFE device /dev/media0 and ISP device /dev/media2 using PiSP variant BCM2712_C0
✅ Camera instance created
⚙️ Configuring camera...
[1:57:43.795700193] [3852]  INFO Camera camera.cpp:1205 configuring streams: (0) 640x480-XRGB8888/sRGB (1) 640x480-BGGR_PISP_COMP1/RAW
[1:57:43.795798933] [3862]  INFO RPI pisp.cpp:1483 Sensor: /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 - Selected sensor format: 640x480-SBGGR10_1X10 - Selected CFE format: 640x480-PC1B
✅ Camera configured
🚀 Starting camera...
✅ Camera started
⏱️ Waiting for camera to warm up...
📸 Testing capture...
✅ Capture 1: Shape=(480, 640, 4), dtype=uint8
💾 Saved test image: camera_test_1.jpg
✅ Capture 2: Shape=(480, 640, 4), dtype=uint8
💾 Saved test image: camera_test_2.jpg
✅ Capture 3: Shape=(480, 640, 4), dtype=uint8
💾 Saved test image: camera_test_3.jpg
✅ Capture 4: Shape=(480, 640, 4), dtype=uint8
💾 Saved test image: camera_test_4.jpg
✅ Capture 5: Shape=(480, 640, 4), dtype=uint8
💾 Saved test image: camera_test_5.jpg
🛑 Stopping camera...
✅ Camera stopped successfully

🎉 Camera test completed successfully!
✅ Your camera is working properly
📁 Check the saved test images in the current directory

🔊 Testing audio setup...
==================================================
✅ sounddevice imported successfully
🎤 Available audio devices:
  0: USB PnP Sound Device: Audio (hw:2,0) - 1 in, 0 out
  1: pulse - 32 in, 32 out
  2: default - 32 in, 32 out

==================================================
🎉 ALL TESTS PASSED!
✅ Your system should work with the web interface
==================================================
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ 

I then tested the debug_app.py script you provided:

debug_app.app
#!/usr/bin/env python3
"""
debug_app.py - Debug version of Chatty AI Web Application
This version includes extensive debugging to identify issues
"""

import os
import sys
import traceback
from flask import Flask, render_template, Response
from flask_socketio import SocketIO, emit
import cv2
import numpy as np
import time
from datetime import datetime
import threading

# Add current directory to path
sys.path.append('/home/nickspi5/Chatty_AI')

# Flask application setup
app = Flask(__name__, 
            template_folder='/home/nickspi5/Chatty_AI/templates',
            static_folder='/home/nickspi5/Chatty_AI/templates')
app.config['SECRET_KEY'] = 'chatty_ai_debug_key'
socketio = SocketIO(app, cors_allowed_origins="*")

class DebugChattyAI:
    def __init__(self):
        print("🔧 Initializing DebugChattyAI...")
        self.is_running = False
        self.picam2 = None
        self.current_frame = None
        self.captured_image = None
        self.setup_camera()

    def emit_log(self, message, log_type="info"):
        """Emit log message to web interface"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {log_type.upper()}: {message}")
        try:
            socketio.emit('log_update', {
                'timestamp': timestamp,
                'message': message,
                'type': log_type
            })
        except Exception as e:
            print(f"Failed to emit log: {e}")

    def setup_camera(self):
        """Initialize camera with detailed debugging"""
        try:
            print("🎥 Setting up camera...")
            
            # Check if we can import Picamera2
            try:
                from picamera2 import Picamera2
                print("✅ Picamera2 imported successfully")
            except ImportError as e:
                print(f"❌ Failed to import Picamera2: {e}")
                return False
            
            # Try to initialize camera
            self.picam2 = Picamera2()
            print("✅ Picamera2 instance created")
            
            # Configure camera
            config = self.picam2.create_preview_configuration(
                main={"format": 'XRGB8888', "size": (640, 480)}
            )
            self.picam2.configure(config)
            print("✅ Camera configured")
            
            # Start camera
            self.picam2.start()
            print("✅ Camera started")
            
            # Test capture
            time.sleep(2)
            test_frame = self.picam2.capture_array()
            print(f"✅ Test capture successful - Frame shape: {test_frame.shape}")
            
            self.emit_log("Camera initialized successfully")
            return True
            
        except Exception as e:
            print(f"❌ Camera setup failed: {e}")
            print(f"❌ Traceback: {traceback.format_exc()}")
            self.emit_log(f"Camera setup failed: {e}", "error")
            return False

    def generate_video_feed(self):
        """Generate video frames for streaming with debugging"""
        print("🎬 Starting video feed generation...")
        
        frame_count = 0
        while True:
            try:
                frame_count += 1
                
                if self.picam2:
                    # Capture frame
                    frame = self.picam2.capture_array()
                    
                    if frame_count % 30 == 0:  # Log every 30th frame
                        print(f"📸 Frame {frame_count}: shape={frame.shape}, dtype={frame.dtype}")
                    
                    # Convert color space if needed
                    if len(frame.shape) == 3 and frame.shape[2] == 3:
                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    elif len(frame.shape) == 3 and frame.shape[2] == 4:
                        frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)
                    
                    # Add debug overlay
                    timestamp = datetime.now().strftime("%H:%M:%S")
                    cv2.putText(frame, f"Debug Mode - {timestamp}", (10, 30), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(frame, f"Frame: {frame_count}", (10, 60), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    cv2.putText(frame, f"Shape: {frame.shape}", (10, 90), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                    # Store current frame
                    self.current_frame = frame
                    
                else:
                    # Create test pattern if no camera
                    frame = np.zeros((480, 640, 3), dtype=np.uint8)
                    cv2.putText(frame, "NO CAMERA AVAILABLE", (200, 240), 
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                    cv2.putText(frame, f"Frame: {frame_count}", (10, 30), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # Encode frame as JPEG
                ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                else:
                    print(f"❌ Failed to encode frame {frame_count}")
                
                time.sleep(0.1)  # 10 FPS
                
            except Exception as e:
                print(f"❌ Video feed error at frame {frame_count}: {e}")
                print(f"❌ Traceback: {traceback.format_exc()}")
                
                # Create error frame
                error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                cv2.putText(error_frame, f"ERROR: {str(e)[:30]}", (50, 240), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                ret, buffer = cv2.imencode('.jpg', error_frame)
                if ret:
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
                time.sleep(1)

    def start_system(self):
        """Start the debug system"""
        self.emit_log("Debug system started")
        self.is_running = True
        return True

    def stop_system(self):
        """Stop the debug system"""
        self.emit_log("Debug system stopped")
        self.is_running = False

    def cleanup(self):
        """Clean up resources"""
        print("🧹 Cleaning up debug system...")
        self.is_running = False
        if self.picam2:
            try:
                self.picam2.stop()
                print("✅ Camera stopped")
            except Exception as e:
                print(f"⚠️  Error stopping camera: {e}")

# Create global instance
debug_chatty = DebugChattyAI()

# Flask routes
@app.route('/')
def index():
    """Main page"""
    try:
        print("📄 Serving main page...")
        return render_template('Chatty_AI.html')
    except Exception as e:
        print(f"❌ Error serving main page: {e}")
        return f"Error loading page: {e}"

@app.route('/static/<path:filename>')
def static_files(filename):
    """Serve static files from templates directory"""
    try:
        print(f"📁 Serving static file: {filename}")
        file_path = f"/home/nickspi5/Chatty_AI/templates/{filename}"
        
        if os.path.exists(file_path):
            print(f"✅ File found: {file_path}")
            return app.send_static_file(filename)
        else:
            print(f"❌ File not found: {file_path}")
            return f"File not found: {filename}", 404
    except Exception as e:
        print(f"❌ Error serving static file {filename}: {e}")
        return f"Error: {e}", 500

@app.route('/video_feed')
def video_feed():
    """Video streaming route"""
    try:
        print("🎥 Starting video feed...")
        return Response(debug_chatty.generate_video_feed(),
                       mimetype='multipart/x-mixed-replace; boundary=frame')
    except Exception as e:
        print(f"❌ Video feed error: {e}")
        print(f"❌ Traceback: {traceback.format_exc()}")
        return f"Video feed error: {e}", 500

@app.route('/captured_image')
def captured_image():
    """Route for captured person image"""
    try:
        if debug_chatty.captured_image is not None:
            ret, buffer = cv2.imencode('.jpg', debug_chatty.captured_image)
            if ret:
                return Response(buffer.tobytes(), mimetype='image/jpeg')
        
        # Return placeholder image
        placeholder = np.zeros((200, 200, 3), dtype=np.uint8)
        cv2.putText(placeholder, "No Image", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
        ret, buffer = cv2.imencode('.jpg', placeholder)
        return Response(buffer.tobytes(), mimetype='image/jpeg')
        
    except Exception as e:
        print(f"❌ Captured image error: {e}")
        return "Error", 500

# Socket.IO events
@socketio.on('connect')
def handle_connect():
    """Handle client connection"""
    print('🔌 Client connected')
    emit('status', {'message': 'Connected to Debug Chatty AI'})

@socketio.on('start_system')
def handle_start_system():
    """Handle system start request"""
    print("🚀 Start system requested")
    success = debug_chatty.start_system()
    emit('system_status', {'running': success})

@socketio.on('stop_system')
def handle_stop_system():
    """Handle system stop request"""
    print("🛑 Stop system requested")
    debug_chatty.stop_system()
    emit('system_status', {'running': False})

if __name__ == '__main__':
    print("🐛 Chatty AI Debug Mode")
    print("=" * 50)
    
    # System checks
    print("🔍 Running system checks...")
    
    # Check Python version
    print(f"🐍 Python version: {sys.version}")
    
    # Check working directory
    print(f"📁 Working directory: {os.getcwd()}")
    
    # Check if templates directory exists
    templates_dir = "/home/nickspi5/Chatty_AI/templates"
    if os.path.exists(templates_dir):
        print(f"✅ Templates directory found: {templates_dir}")
        files = os.listdir(templates_dir)
        print(f"📄 Files in templates: {files}")
    else:
        print(f"❌ Templates directory not found: {templates_dir}")
    
    # Check camera permissions
    try:
        import picamera2
        print("✅ Picamera2 module available")
    except ImportError as e:
        print(f"❌ Picamera2 import error: {e}")
    
    # Check OpenCV
    print(f"👁️ OpenCV version: {cv2.__version__}")
    
    # Check if running as root/sudo
    if os.geteuid() == 0:
        print("⚠️  Running as root - this may cause issues")
    else:
        print(f"👤 Running as user: {os.getenv('USER')}")
    
    print("=" * 50)
    
    try:
        print("🌐 Starting debug Flask server...")
        print(f"🔗 Access at: http://localhost:5000")
        
        # Start server
        socketio.run(app, host='0.0.0.0', port=5000, debug=True, allow_unsafe_werkzeug=True)
        
    except KeyboardInterrupt:
        print("\n🛑 Debug server stopped by user")
        debug_chatty.cleanup()
    except Exception as e:
        print(f"❌ Debug server error: {e}")
        print(f"❌ Traceback: {traceback.format_exc()}")
        debug_chatty.cleanup()

using the start_chatty_web.sh script your provided:

start_chatty_web.sh
#!/bin/bash

# Chatty AI Web Interface Startup Script
# Place this script in /home/nickspi5/Chatty_AI/

echo "=========================================="
echo "  Starting Chatty AI Web Interface"
echo "=========================================="

# Activate virtual environment
echo "🔧 Activating virtual environment..."
source chatty-venv/bin/activate

# Check if required files exist
echo "🔍 Checking required files..."

required_files=(
    "app.py"
    "templates/Chatty_AI.html"
    "templates/Chatty_AI_logo.png"
    "templates/diamond_coding_logo.png"
    "encodings.pickle"
    "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
)

missing_files=0
for file in "${required_files[@]}"; do
    if [ ! -f "$file" ]; then
        echo "❌ Missing file: $file"
        missing_files=$((missing_files + 1))
    else
        echo "✅ Found: $file"
    fi
done

if [ $missing_files -gt 0 ]; then
    echo "❌ $missing_files required files are missing. Please ensure all files are in place."
    exit 1
fi

# Check if response files exist, create if missing
echo "📝 Checking response files..."
response_files=(
    "jokes.txt"
    "listening_responses.txt"
    "waiting_responses.txt"
    "warning_responses.txt"
    "greeting_responses.txt"
)

for file in "${response_files[@]}"; do
    if [ ! -f "$file" ]; then
        echo "⚠️  Creating missing response file: $file"
        case $file in
            "jokes.txt")
                echo "Why don't scientists trust atoms? Because they make up everything!" > "$file"
                echo "What do you call a bear with no teeth? A gummy bear!" >> "$file"
                ;;
            "listening_responses.txt")
                echo "I'm listening, what would you like to know?" > "$file"
                echo "Yes, how can I help you?" >> "$file"
                ;;
            "waiting_responses.txt")
                echo "I'm still here if you need anything" > "$file"
                echo "Let me know if you need help" >> "$file"
                ;;
            "warning_responses.txt")
                echo "Warning: Unknown person detected. Please identify yourself." > "$file"
                ;;
            "greeting_responses.txt")
                echo "Hello! How can I help you today?" > "$file"
                echo "Welcome! What can I do for you?" >> "$file"
                ;;
        esac
    fi
done

# Set permissions for audio devices
echo "🔊 Setting up audio permissions..."
sudo usermod -a -G audio $USER

# Display network information
echo "🌐 Network Information:"
IP_ADDRESS=$(hostname -I | awk '{print $1}')
echo "   Local IP: $IP_ADDRESS"
echo "   Web Interface: http://$IP_ADDRESS:5000"
echo "   Alternative: http://localhost:5000"

echo ""
echo "🚀 Starting Chatty AI Web Server..."
echo "   Press Ctrl+C to stop the server"
echo "=========================================="

# Start the Flask application
python3 debug_app.py

I ran: (chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ chmod +x start_chatty_web.sh
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ ./start_chatty_web.sh
bash: ./start_chatty_web.sh: cannot execute: required file not found
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ python3 debug_app.py
🔧 Initializing DebugChattyAI...
🎥 Setting up camera...
✅ Picamera2 imported successfully
[2:11:18.755543643] [4079]  INFO Camera camera_manager.cpp:326 libcamera v0.5.1+100-e53bdf1f
[2:11:18.762573881] [4086]  INFO RPI pisp.cpp:720 libpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)
[2:11:18.771816285] [4086]  INFO RPI pisp.cpp:1179 Registered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 to CFE device /dev/media0 and ISP device /dev/media2 using PiSP variant BCM2712_C0
✅ Picamera2 instance created
[2:11:18.774886932] [4079]  INFO Camera camera.cpp:1205 configuring streams: (0) 640x480-XRGB8888/sRGB (1) 640x480-BGGR_PISP_COMP1/RAW
[2:11:18.775022765] [4086]  INFO RPI pisp.cpp:1483 Sensor: /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 - Selected sensor format: 640x480-SBGGR10_1X10 - Selected CFE format: 640x480-PC1B
✅ Camera configured
✅ Camera started
✅ Test capture successful - Frame shape: (480, 640, 4)
[20:05:43] INFO: Camera initialized successfully
🐛 Chatty AI Debug Mode
==================================================
🔍 Running system checks...
🐍 Python version: 3.11.2 (main, Apr 28 2025, 14:11:48) [GCC 12.2.0]
📁 Working directory: /home/nickspi5/Chatty_AI
✅ Templates directory found: /home/nickspi5/Chatty_AI/templates
📄 Files in templates: ['diamond_coding_logo.png', 'diamond_coding_Logo.png', 'chatty_ai_html_template.html', 'Chatty_AI.html', 'Chatty_AI_logo.png', 'Chatty_AI_Logo.png', 'image_analyse.html']
✅ Picamera2 module available
👁️ OpenCV version: 4.11.0
👤 Running as user: nickspi5
==================================================
🌐 Starting debug Flask server...
🔗 Access at: http://localhost:5000
 * Restarting with stat
🔧 Initializing DebugChattyAI...
🎥 Setting up camera...
✅ Picamera2 imported successfully
[2:11:21.590779937] [4092]  INFO Camera camera_manager.cpp:326 libcamera v0.5.1+100-e53bdf1f
[2:11:21.598030657] [4098]  INFO RPI pisp.cpp:720 libpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)
[2:11:21.598595823] [4098] ERROR V4L2 v4l2_device.cpp:390 'imx219 10-0010': Unable to set controls: Device or resource busy
[2:11:21.608050283] [4098]  INFO RPI pisp.cpp:1179 Registered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 to CFE device /dev/media0 and ISP device /dev/media2 using PiSP variant BCM2712_C0
[2:11:21.609150542] [4092]  INFO Camera camera.cpp:1011 Pipeline handler in use by another process
Camera __init__ sequence did not complete.
❌ Camera setup failed: Camera __init__ sequence did not complete.
❌ Traceback: Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/picamera2/picamera2.py", line 353, in __init__
    self._open_camera()
  File "/usr/lib/python3/dist-packages/picamera2/picamera2.py", line 565, in _open_camera
    self.camera.acquire()
RuntimeError: Failed to acquire camera: Device or resource busy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nickspi5/Chatty_AI/debug_app.py", line 64, in setup_camera
    self.picam2 = Picamera2()
                  ^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/picamera2/picamera2.py", line 365, in __init__
    raise RuntimeError("Camera __init__ sequence did not complete.")
RuntimeError: Camera __init__ sequence did not complete.

[20:05:44] ERROR: Camera setup failed: Camera __init__ sequence did not complete.
🐛 Chatty AI Debug Mode
==================================================
🔍 Running system checks...
🐍 Python version: 3.11.2 (main, Apr 28 2025, 14:11:48) [GCC 12.2.0]
📁 Working directory: /home/nickspi5/Chatty_AI
✅ Templates directory found: /home/nickspi5/Chatty_AI/templates
📄 Files in templates: ['diamond_coding_logo.png', 'diamond_coding_Logo.png', 'chatty_ai_html_template.html', 'Chatty_AI.html', 'Chatty_AI_logo.png', 'Chatty_AI_Logo.png', 'image_analyse.html']
✅ Picamera2 module available
👁️ OpenCV version: 4.11.0
👤 Running as user: nickspi5
==================================================
🌐 Starting debug Flask server...
🔗 Access at: http://localhost:5000
 * Debugger is active!
 * Debugger PIN: 138-965-334
(4092) wsgi starting up on http://0.0.0.0:5000
(4092) accepted ('127.0.0.1', 33992)
(4092) accepted ('127.0.0.1', 33996)
📄 Serving main page...
127.0.0.1 - - [04/Aug/2025 20:06:45] "GET / HTTP/1.1" 200 16198 0.010558
📁 Serving static file: Chatty_AI_logo.png
✅ File found: /home/nickspi5/Chatty_AI/templates/Chatty_AI_logo.png
127.0.0.1 - - [04/Aug/2025 20:06:45] "GET /static/Chatty_AI_logo.png HTTP/1.1" 200 66380 0.005360
🎥 Starting video feed...
🎬 Starting video feed generation...
Traceback (most recent call last):
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/wsgi.py", line 641, in handle_one_response
    write(b''.join(towrite))
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/wsgi.py", line 575, in write
    wfile.flush()
  File "/usr/lib/python3.11/socket.py", line 724, in write
    return self._sock.send(b)
           ^^^^^^^^^^^^^^^^^^
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/greenio/base.py", line 383, in send
    return self._send_loop(self.fd.send, data, flags)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/greenio/base.py", line 370, in _send_loop
    return send_method(data, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

127.0.0.1 - - [04/Aug/2025 20:07:12] "GET /video_feed HTTP/1.1" 200 3309320 26.935814
127.0.0.1 - - [04/Aug/2025 20:07:12] "GET /captured_image HTTP/1.1" 200 3122 0.001163
(4092) accepted ('127.0.0.1', 34004)
(4092) accepted ('127.0.0.1', 34016)
(4092) accepted ('127.0.0.1', 54828)
📁 Serving static file: diamond_coding_logo.png
✅ File found: /home/nickspi5/Chatty_AI/templates/diamond_coding_logo.png
Traceback (most recent call last):
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/wsgi.py", line 641, in handle_one_response
    write(b''.join(towrite))
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/wsgi.py", line 575, in write
    wfile.flush()
  File "/usr/lib/python3.11/socket.py", line 724, in write
    return self._sock.send(b)
           ^^^^^^^^^^^^^^^^^^
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/greenio/base.py", line 383, in send
    return self._send_loop(self.fd.send, data, flags)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nickspi5/Chatty_AI/chatty-venv/lib/python3.11/site-packages/eventlet/greenio/base.py", line 370, in _send_loop
    return send_method(data, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

127.0.0.1 - - [04/Aug/2025 20:07:12] "GET /static/diamond_coding_logo.png HTTP/1.1" 200 8501 0.001643
127.0.0.1 - - [04/Aug/2025 20:07:12] "GET /socket.io/?EIO=4&transport=polling&t=PXqZ42I HTTP/1.1" 200 300 0.000367
127.0.0.1 - - [04/Aug/2025 20:07:12] "GET /socket.io/?EIO=4&transport=polling&t=PXqZ9GC HTTP/1.1" 200 300 0.000264

I then tried to run the updated Python app.py script which is as follows:

app.py
except KeyboardInterrupt:
    print    def generate_video_feed(self):
    """Generate video frames for streaming"""
    print("Video feed generation started")
        
    while True:  # Keep generating even if system not started
        try:
            if self.picam2 and self.is_running:
                frame = self.picam2.capture_array()
                    
                # Convert from RGB to BGR for OpenCV
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                # Process facial recognition only if system is running
                name, face_location, confidence = self.detect_faces(frame)
                    
                # Draw face rectangles and labels
                if name and face_location:
                    top, right, bottom, left = face_location
                        
                    # Choose color based on recognition
                    if name == "Unknown":
                        color = (0, 0, 255)  # Red
                        label = f"Unknown ({confidence:.2f})"
                    else:
                        color = (0, 255, 0)  # Green
                        label = f"{name} ({confidence:.2f})"
                        
                    # Draw rectangle and label
                    cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
                    cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)
                    cv2.putText(frame, label, (left + 6, bottom - 6),
                              cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                        
                    # Capture image for display
                    face_img = frame[top:bottom, left:right].copy()
                    if face_img.size > 0:
                        self.captured_image = cv2.resize(face_img, (200, 200))
                        
                    # Emit person detection update
                    try:
                        socketio.emit('person_detected', {
                            'name': name,
                            'confidence': f"{confidence:.1%}",
                            'timestamp': datetime.now().strftime("%H:%M:%S")
                        })
                    except:
                        pass
                    
                # Add status overlay to frame
                if self.is_running:
                    status_text = "Chatty AI Active"
                    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                else:
                    status_text = "Chatty AI Inactive"
                    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                    
                if self.current_person:
                    person_text = f"Current: {self.current_person}"
                    cv2.putText(frame, person_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                if self.wake_word_active:
                    wake_text = "Wake Word: ACTIVE"
                    cv2.putText(frame, wake_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                else:
                    wake_text = "Wake Word: INACTIVE"
                    cv2.putText(frame, wake_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                
            elif self.picam2:  # Camera available but system not started
                frame = self.picam2.capture_array()
                if len(frame.shape) == 3 and frame.shape[2] == 3:
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                # Add "System Inactive" overlay
                cv2.putText(frame, "System Inactive - Initializing...", (10, 30), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
            else:  # No camera available
                # Create a black frame with error message
                frame = np.zeros((480, 640, 3), dtype=np.uint8)
                cv2.putText(frame, "Camera Not Available", (200, 240), 
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                
            # Encode frame as JPEG
            ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
            if ret:
                frame_bytes = buffer.tobytes()
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                
            time.sleep(0.1)  # Control frame rate
                
        except Exception as e:
            print(f"Video feed error: {e}")
            # Create error frame
            error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
            cv2.putText(error_frame, f"Video Error: {str(e)[:50]}", (50, 240), 
                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            ret, buffer = cv2.imencode('.jpg', error_frame)
            if ret:
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
            time.sleep(1)#!/usr/bin/env python3

"""
app.py - Chatty AI Web Application
Flask web interface for the AI Assistant with video streaming
"""

import os
import subprocess
import sounddevice as sd
import soundfile as sf
import numpy as np
import threading
import time
import random
import re
import cv2
import face_recognition
import pickle
import json
import requests
import logging
from datetime import datetime, timedelta
from faster_whisper import WhisperModel
from llama_cpp import Llama
from picamera2 import Picamera2
from flask import Flask, render_template, Response, jsonify, request
from flask_socketio import SocketIO, emit
import base64
import io
from PIL import Image

# Flask application setup
app = Flask(__name__, 
            template_folder='/home/nickspi5/Chatty_AI/templates',
            static_folder='/home/nickspi5/Chatty_AI/templates')
app.config['SECRET_KEY'] = 'chatty_ai_secret_key_2025'
socketio = SocketIO(app, cors_allowed_origins="*")

# -------------------------------
# Configuration (same as original)
# -------------------------------
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
BEEP_SOUND = "/home/nickspi5/Chatty_AI/audio_files/beep.wav"
LAUGHING_SOUND = "/home/nickspi5/Chatty_AI/audio_files/laughing.wav"
ENCODINGS_FILE = "encodings.pickle"
TELEGRAM_CONFIG_FILE = "telegram_config.json"

# Audio files
WAV_FILENAME = "user_input.wav"
RESPONSE_AUDIO = "output.wav"
WAKE_WORD_AUDIO = "wake_word_check.wav"

# Security directories
SECURITY_PHOTOS_DIR = "/home/nickspi5/Chatty_AI/security_photos"
SECURITY_LOGS_DIR = "/home/nickspi5/Chatty_AI/security_logs"

# Response files
JOKES_FILE = "jokes.txt"
LISTENING_RESPONSES_FILE = "listening_responses.txt"
WAITING_RESPONSES_FILE = "waiting_responses.txt"
WARNING_RESPONSES_FILE = "warning_responses.txt"
GREETING_RESPONSES_FILE = "greeting_responses.txt"
PERSONALIZED_RESPONSES_FILE = "personalized_responses.json"

# Wake words and commands (same as original)
WAKE_WORDS = [
    "are you awake", "are you alive", "hey chatty", "hello chatty", "sup chatty",
    "sub-chatty", "how's it chatty", "howzit chatty", "hi chatty", "yo chatty",
    "hey chuddy", "hello chuddy", "sup chuddy", "sub-chuddy", "how's it chuddy",
    "howzit chuddy", "hi chuddy", "yo chuddy", "hey cheddy", "hello cheddy",
    "sup cheddy", "sub-cheddy", "how's it cheddy", "howzit cheddy", "hi cheddy",
    "yo cheddy", "hey chetty", "hello chetty", "sup chetty", "sub-chetty",
    "how's it chetty", "howzit chetty", "hi chetty", "yo chetty", "hey cherry",
    "hello cherry", "sup cherry", "sub-cherry", "how's it cherry", "howzit cherry",
    "hi cherry", "yo cherry"
]

COMMANDS = {
    "flush the toilet": "toilet_flush",
    "turn on the lights": "lights_on",
    "turn off the lights": "lights_off",
    "play music": "play_music",
    "stop music": "stop_music",
    "what time is it": "get_time",
    "shutdown system": "shutdown_system",
    "who is sponsoring this video": "who_is_sponsoring_this_video",
    "how is the weather today": "how_is_the_weather_today",
    "reboot system": "reboot_system"
}

# Audio parameters
SAMPLE_RATE = 16000
CHANNELS = 1
SILENCE_THRESHOLD = 0.035
MIN_SILENCE_DURATION = 1.5
MAX_RECORDING_DURATION = 30

# Timing parameters
GREETING_COOLDOWN = 300
WAITING_INTERVAL = 30
PERSON_DETECTION_INTERVAL = 0.5
WAKE_WORD_CHECK_INTERVAL = 1.0

class ChattyAIWeb:
    def __init__(self):
        # AI Models
        self.whisper_model = None
        self.llama_model = None
        
        # Facial Recognition
        self.known_encodings = []
        self.known_names = []
        
        # Camera
        self.picam2 = None
        
        # State variables
        self.is_running = False
        self.current_person = None
        self.last_greeting_time = {}
        self.last_interaction_time = None
        self.person_absent_since = None
        self.waiting_cycle = 0
        self.last_bored_response_time = None
        self.bored_cycle = 0
        self.audio_recording_lock = threading.Lock()
        self.wake_word_active = False
        
        # Web-specific variables
        self.current_frame = None
        self.current_detected_person = None
        self.current_confidence = 0.0
        self.captured_image = None
        
        # Response lists
        self.jokes = []
        self.listening_responses = []
        self.waiting_responses = []
        self.warning_responses = []
        self.greeting_responses = []
        self.personalized_responses = {}
        
        # Telegram
        self.telegram_token = None
        self.telegram_chat_id = None
        
        # Threading
        self.camera_thread = None
        self.audio_thread = None
        
        # Initialize basic components first
        self.setup_directories()
        self.setup_logging()
        self.load_response_files()
        self.load_personalized_responses()
        self.load_telegram_config()
        
        print("ChattyAIWeb instance created - models will be loaded when system starts")

    def emit_log(self, message, log_type="info"):
        """Emit log message to web interface"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")  # Also print to console
        try:
            socketio.emit('log_update', {
                'timestamp': timestamp,
                'message': message,
                'type': log_type
            })
        except:
            pass  # Ignore if socketio not ready

    def emit_conversation(self, message, msg_type="info"):
        """Emit conversation message to web interface"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] CONVERSATION: {message}")  # Also print to console
        try:
            socketio.emit('conversation_update', {
                'timestamp': timestamp,
                'message': message,
                'type': msg_type
            })
        except:
            pass  # Ignore if socketio not ready

    # [Include all the original methods from ChattyAI class here with modifications for web interface]
    
    def setup_directories(self):
        """Create necessary directories"""
        os.makedirs(SECURITY_PHOTOS_DIR, exist_ok=True)
        os.makedirs(SECURITY_LOGS_DIR, exist_ok=True)

    def setup_logging(self):
        """Setup logging for detections"""
        log_file = os.path.join(SECURITY_LOGS_DIR, "chatty_ai.log")
        self.logger = logging.getLogger('chatty_ai')
        self.logger.setLevel(logging.INFO)
        
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
            
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

    def load_response_files(self):
        """Load response text files"""
        try:
            with open(JOKES_FILE, 'r') as f:
                self.jokes = [line.strip() for line in f if line.strip()]
            with open(LISTENING_RESPONSES_FILE, 'r') as f:
                self.listening_responses = [line.strip() for line in f if line.strip()]
            with open(GREETING_RESPONSES_FILE, 'r') as f:
                self.greeting_responses = [line.strip() for line in f if line.strip()]
            with open(WAITING_RESPONSES_FILE, 'r') as f:
                self.waiting_responses = [line.strip() for line in f if line.strip()]
            with open(WARNING_RESPONSES_FILE, 'r') as f:
                self.warning_responses = [line.strip() for line in f if line.strip()]
            self.emit_log("Response files loaded successfully")
        except FileNotFoundError as e:
            self.emit_log(f"Response file not found: {e}", "error")
            self.create_default_responses()

    def load_personalized_responses(self):
        """Load personalized responses from JSON file"""
        try:
            with open(PERSONALIZED_RESPONSES_FILE, 'r') as f:
                self.personalized_responses = json.load(f)
            self.emit_log(f"Personalized responses loaded for {len(self.personalized_responses)} people")
        except FileNotFoundError:
            self.emit_log("Personalized responses file not found. Creating default...", "warning")
            self.create_default_personalized_responses()
        except json.JSONDecodeError as e:
            self.emit_log(f"Error reading personalized responses JSON: {e}", "error")
            self.create_default_personalized_responses()

    def create_default_personalized_responses(self):
        """Create default personalized responses file"""
        default_responses = {
            "Nick": {
                "greetings": [
                    "Hello Nick, my master! It is so lovely to see you again. Thank you for creating me. How may I assist you?",
                    "Welcome back Nick! Your brilliant creation is ready to serve. What can I help you with today?",
                    "Nick! My creator has returned! I've been waiting patiently for your commands."
                ],
                "listening": [
                    "Yes Nick, I'm listening. What would you like to know?",
                    "I'm all ears, Nick. What's on your mind?",
                    "Ready for your instructions, Nick!"
                ],
                "waiting": [
                    "Nick, I'm still here if you need anything",
                    "Your faithful AI assistant is standing by, Nick",
                    "Still waiting to help you, Nick"
                ]
            }
        }
        
        try:
            with open(PERSONALIZED_RESPONSES_FILE, 'w') as f:
                json.dump(default_responses, f, indent=4)
            self.personalized_responses = default_responses
            self.emit_log(f"Created default personalized responses file: {PERSONALIZED_RESPONSES_FILE}")
        except Exception as e:
            self.emit_log(f"Failed to create personalized responses file: {e}", "error")
            self.personalized_responses = default_responses

    def create_default_responses(self):
        """Create default responses if files are missing"""
        self.jokes = ["Why don't scientists trust atoms? Because they make up everything!"]
        self.greeting_responses = ["Hello! How can I help you today?"]
        self.listening_responses = ["I'm listening, what would you like to know?"]
        self.waiting_responses = ["I'm still here if you need anything"]
        self.warning_responses = ["Warning: Unknown person detected. Please identify yourself."]

    def load_models(self):
        """Load AI models"""
        self.emit_log("Loading AI models...")
        try:
            self.whisper_model = WhisperModel(WHISPER_MODEL_SIZE, device="cpu", compute_type="int8")
            self.emit_log("Whisper model loaded")
        except Exception as e:
            self.emit_log(f"Failed to load Whisper: {e}", "error")
            return False

        try:
            self.llama_model = Llama(
                model_path=LLAMA_MODEL_PATH,
                n_ctx=2048,
                temperature=0.7,
                repeat_penalty=1.1,
                n_gpu_layers=0,
                verbose=False
            )
            self.emit_log("LLaMA model loaded")
        except Exception as e:
            self.emit_log(f"Failed to load LLaMA: {e}", "error")
            return False
        return True

    def load_encodings(self):
        """Load facial recognition encodings"""
        try:
            with open(ENCODINGS_FILE, "rb") as f:
                data = pickle.loads(f.read())
            self.known_encodings = data["encodings"]
            self.known_names = data["names"]
            self.emit_log(f"Loaded {len(self.known_encodings)} face encodings")
            return True
        except FileNotFoundError:
            self.emit_log(f"Encodings file '{ENCODINGS_FILE}' not found!", "error")
            return False
        except Exception as e:
            self.emit_log(f"Failed to load encodings: {e}", "error")
            return False

    def load_telegram_config(self):
        """Load Telegram configuration"""
        try:
            with open(TELEGRAM_CONFIG_FILE, 'r') as f:
                config = json.load(f)
            self.telegram_token = config.get('bot_token')
            self.telegram_chat_id = config.get('chat_id')
            self.emit_log("Telegram configuration loaded")
        except FileNotFoundError:
            self.emit_log("Telegram config not found - alerts disabled", "warning")
        except Exception as e:
            self.emit_log(f"Failed to load Telegram config: {e}", "error")

    def setup_camera(self):
        """Initialize camera"""
        try:
            self.picam2 = Picamera2()
            self.picam2.configure(self.picam2.create_preview_configuration(
                main={"format": 'XRGB8888', "size": (640, 480)}
            ))
            self.picam2.start()
            time.sleep(2)
            self.emit_log("Camera initialized")
            return True
        except Exception as e:
            self.emit_log(f"Failed to initialize camera: {e}", "error")
            return False

    def detect_faces(self, frame):
        """Detect and recognize faces in frame"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        face_locations = face_recognition.face_locations(rgb_frame, model="hog")
        
        if len(face_locations) == 0:
            return None, None, 0.0

        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        best_name = "Unknown"
        best_confidence = 0.0

        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(self.known_encodings, face_encoding, tolerance=0.6)
            if True in matches:
                face_distances = face_recognition.face_distance(self.known_encodings, face_encoding)
                best_match_index = face_distances.argmin()
                confidence = 1.0 - face_distances[best_match_index]
                
                if matches[best_match_index] and confidence > 0.4:
                    if confidence > best_confidence:
                        best_name = self.known_names[best_match_index]
                        best_confidence = confidence

        return best_name, face_locations[0], best_confidence

    def generate_video_feed(self):
        """Generate video frames for streaming"""
        while self.is_running:
            try:
                if self.picam2:
                    frame = self.picam2.capture_array()
                    
                    # Convert from RGB to BGR for OpenCV
                    if len(frame.shape) == 3 and frame.shape[2] == 3:
                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                    # Process facial recognition
                    name, face_location, confidence = self.detect_faces(frame)
                    
                    # Update current detection state
                    self.current_detected_person = name
                    self.current_confidence = confidence
                    
                    # Draw face rectangles and labels
                    if name and face_location:
                        top, right, bottom, left = face_location
                        
                        # Choose color based on recognition
                        if name == "Unknown":
                            color = (0, 0, 255)  # Red
                            label = f"Unknown ({confidence:.2f})"
                        else:
                            color = (0, 255, 0)  # Green
                            label = f"{name} ({confidence:.2f})"
                        
                        # Draw rectangle and label
                        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
                        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), color, cv2.FILLED)
                        cv2.putText(frame, label, (left + 6, bottom - 6),
                                  cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)
                        
                        # Capture image for display
                        self.captured_image = frame[top:bottom, left:right].copy()
                        
                        # Emit person detection update
                        socketio.emit('person_detected', {
                            'name': name,
                            'confidence': f"{confidence:.1%}",
                            'timestamp': datetime.now().strftime("%H:%M:%S")
                        })
                    
                    # Store current frame
                    self.current_frame = frame
                    
                    # Encode frame as JPEG
                    ret, buffer = cv2.imencode('.jpg', frame)
                    if ret:
                        frame_bytes = buffer.tobytes()
                        yield (b'--frame\r\n'
                               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                
                time.sleep(0.1)  # Control frame rate
                
            except Exception as e:
                self.emit_log(f"Video feed error: {e}", "error")
                time.sleep(1)

    def start_system(self):
        """Start the AI system"""
        # Load models first if not already loaded
        if not self.whisper_model or not self.llama_model:
            self.emit_log("Loading AI models...", "info")
            if not self.load_models():
                self.emit_log("Failed to load AI models", "error")
                return False
        
        if not self.picam2:
            self.emit_log("Initializing camera...", "info")
            if not self.setup_camera():
                self.emit_log("Failed to initialize camera", "error")
                return False
        
        if not self.known_encodings:
            self.emit_log("Loading facial recognition encodings...", "info")
            if not self.load_encodings():
                self.emit_log("Failed to load facial recognition encodings", "error")
                return False
        
        self.emit_log("Chatty AI System Started!")
        self.is_running = True
        
        # Start audio thread
        self.audio_thread = threading.Thread(target=self.listen_for_wake_word, daemon=True)
        self.audio_thread.start()
        
        # Start camera monitoring thread
        self.camera_thread = threading.Thread(target=self.camera_monitoring_thread, daemon=True)
        self.camera_thread.start()
        
        return True

    def stop_system(self):
        """Stop the AI system"""
        self.is_running = False
        self.emit_log("Chatty AI System Stopped")

    def get_personalized_response(self, person_name, response_type, fallback_list=None):
        """Get a personalized response for a specific person and response type"""
        person_name_lower = person_name.lower()
        
        person_data = None
        for name, data in self.personalized_responses.items():
            if name.lower() == person_name_lower:
                person_data = data
                break
        
        if person_data and response_type in person_data:
            responses = person_data[response_type]
            if responses:
                return random.choice(responses)
        
        if fallback_list:
            response = random.choice(fallback_list)
            if "{name}" in response:
                return response.replace("{name}", person_name)
            else:
                return f"Hello {person_name}! {response}"
        
        return f"Hello {person_name}! How can I help you today?"

    def speak_text(self, text):
        """Convert text to speech using Piper"""
        try:
            with self.audio_recording_lock:
                command = [
                    PIPER_EXECUTABLE,
                    "--model", VOICE_PATH,
                    "--config", CONFIG_PATH,
                    "--output_file", RESPONSE_AUDIO
                ]
                subprocess.run(command, input=text.encode("utf-8"), check=True, capture_output=True)
                subprocess.run(["aplay", RESPONSE_AUDIO], check=True, capture_output=True)
                
                # Emit to web interface
                self.emit_conversation(f"🔊 Speaking: {text}", "speech")
        except subprocess.CalledProcessError as e:
            self.emit_log(f"TTS failed: {e}", "error")

    def play_beep(self):
        """Play beep sound"""
        try:
            with self.audio_recording_lock:
                subprocess.run(["aplay", BEEP_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            pass

    def play_laughing(self):
        """Play laughing sound"""
        try:
            with self.audio_recording_lock:
                subprocess.run(["aplay", LAUGHING_SOUND], check=True, capture_output=True)
        except subprocess.CalledProcessError:
            pass

    def save_security_photo(self, frame, person_name, confidence):
        """Save security photo with timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{person_name.lower()}_{timestamp}.jpg"
        filepath = os.path.join(SECURITY_PHOTOS_DIR, filename)
        
        overlay_frame = frame.copy()
        cv2.rectangle(overlay_frame, (10, 10), (500, 100), (0, 0, 0), -1)
        cv2.rectangle(overlay_frame, (10, 10), (500, 100), (255, 255, 255), 2)
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(overlay_frame, f"Person: {person_name}", (20, 35), font, 0.7, (255, 255, 255), 2)
        cv2.putText(overlay_frame, f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", (20, 60), font, 0.6, (255, 255, 255), 2)
        if person_name != "Unknown":
            cv2.putText(overlay_frame, f"Confidence: {confidence:.1%}", (20, 85), font, 0.6, (255, 255, 255), 2)
        
        cv2.imwrite(filepath, overlay_frame)
        self.logger.info(f"Security photo saved: {filename} | Person: {person_name} | Confidence: {confidence:.2f}")
        return filepath

    def send_telegram_alert(self, person_name, confidence, photo_path):
        """Send Telegram alert"""
        if not self.telegram_token or not self.telegram_chat_id:
            return False
        
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            if person_name == "Unknown":
                message = f"**UNKNOWN PERSON DETECTED**\n\n**Time:** {timestamp}\n**Status:** Unregistered Person\n**Action:** Photo captured for review"
            else:
                message = f"**AUTHORIZED ACCESS**\n\n**Person:** {person_name}\n**Time:** {timestamp}\n**Confidence:** {confidence:.1%}\n**Status:** Registered User"
            
            url = f"https://api.telegram.org/bot{self.telegram_token}/sendPhoto"
            with open(photo_path, 'rb') as photo:
                files = {'photo': photo}
                data = {
                    'chat_id': self.telegram_chat_id,
                    'caption': message,
                    'parse_mode': 'Markdown'
                }
                response = requests.post(url, data=data, files=files, timeout=30)
                return response.status_code == 200
        except Exception as e:
            self.emit_log(f"Telegram alert failed: {e}", "error")
            return False

    def greet_person(self, name):
        """Greet a detected person with personalized greeting"""
        current_time = time.time()
        
        if name in self.last_greeting_time:
            time_since_last = current_time - self.last_greeting_time[name]
            if time_since_last < GREETING_COOLDOWN:
                return False
        
        greeting = self.get_personalized_response(name, "greetings", self.greeting_responses)
        self.speak_text(greeting)
        self.last_greeting_time[name] = current_time
        self.last_interaction_time = current_time
        
        self.wake_word_active = True
        self.last_bored_response_time = current_time
        self.bored_cycle = 0
        
        self.emit_log(f"Greeted {name} with personalized message - Wake word detection now active")
        return True

    def handle_unknown_person(self, frame, confidence):
        """Handle unknown person detection"""
        warning = random.choice(self.warning_responses) if self.warning_responses else "Warning: Unknown person detected."
        self.speak_text(warning)
        photo_path = self.save_security_photo(frame, "Unknown", confidence)
        self.send_telegram_alert("Unknown", confidence, photo_path)
        self.emit_log("Unknown person detected and warned", "warning")

    def transcribe_audio(self, filename):
        """Transcribe audio using Whisper"""
        try:
            if not os.path.exists(filename):
                return ""
            segments, _ = self.whisper_model.transcribe(filename)
            transcript = " ".join(segment.text for segment in segments).strip()
            self.emit_log(f"Transcription: '{transcript}'")
            return transcript
        except Exception as e:
            self.emit_log(f"Transcription error: {e}", "error")
            return ""

    def detect_wake_word(self, text):
        """Check if text contains wake word"""
        if not text:
            return False
        text_cleaned = text.lower().replace(',', '').replace('.', '').strip()
        for wake_word in WAKE_WORDS:
            wake_word_cleaned = wake_word.lower().strip()
            if wake_word_cleaned in text_cleaned:
                self.emit_conversation(f"Wake word detected: '{wake_word}' in '{text}'", "wake_word")
                return True
        return False

    def record_with_silence_detection(self):
        """Record audio until silence detected"""
        try:
            with self.audio_recording_lock:
                self.emit_log("Recording audio...")
                audio_data = []
                silence_duration = 0
                recording_duration = 0
                check_interval = 0.2
                samples_per_check = int(SAMPLE_RATE * check_interval)
                
                def audio_callback(indata, frames, time, status):
                    if status:
                        self.emit_log(f"Audio callback status: {status}", "warning")
                    audio_data.extend(indata[:, 0])
                
                with sd.InputStream(callback=audio_callback,
                                  samplerate=SAMPLE_RATE,
                                  channels=CHANNELS,
                                  dtype='float32'):
                    while recording_duration < MAX_RECORDING_DURATION:
                        time.sleep(check_interval)
                        recording_duration += check_interval
                        
                        if len(audio_data) >= samples_per_check:
                            recent_audio = np.array(audio_data[-samples_per_check:])
                            rms = np.sqrt(np.mean(recent_audio**2))
                            
                            if rms < SILENCE_THRESHOLD:
                                silence_duration += check_interval
                                if silence_duration >= MIN_SILENCE_DURATION:
                                    self.emit_log(f"Silence detected after {recording_duration:.1f}s")
                                    break
                            else:
                                silence_duration = 0
                
                if audio_data:
                    audio_array = np.array(audio_data, dtype=np.float32)
                    sf.write(WAV_FILENAME, audio_array, SAMPLE_RATE)
                    self.emit_log(f"Audio saved: {len(audio_array)/SAMPLE_RATE:.1f}s duration")
                    return True
                return False
        except Exception as e:
            self.emit_log(f"Recording error: {e}", "error")
            return False

    def record_wake_word_check(self):
        """Record short audio clip for wake word detection"""
        try:
            if not self.audio_recording_lock.acquire(blocking=False):
                return False
            
            try:
                audio_data = sd.rec(int(5 * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32')
                sd.wait()
                
                rms = np.sqrt(np.mean(audio_data**2))
                if rms > SILENCE_THRESHOLD * 2:
                    sf.write(WAKE_WORD_AUDIO, audio_data, SAMPLE_RATE)
                    return True
                else:
                    return False
            finally:
                self.audio_recording_lock.release()
        except Exception as e:
            self.emit_log(f"Wake word recording error: {e}", "error")
            if self.audio_recording_lock.locked():
                self.audio_recording_lock.release()
            return False

    def is_command(self, text):
        """Check if text is a command"""
        text_lower = text.lower().strip()
        for command in COMMANDS.keys():
            if command in text_lower:
                return command
        return None

    def execute_command(self, command):
        """Execute system command"""
        responses = {
            "flush the toilet": f"Oh {self.current_person}, you know I am a digital assistant. I cannot actually flush toilets! So why dont you haul your lazy butt up off the couch and flush the toilet yourself!",
            "turn on the lights": "I would turn on the lights if I was connected to a smart home system.",
            "turn off the lights": "I would turn off the lights if I was connected to a smart home system.",
            "play music": "I would start playing music if I had access to a music system.",
            "stop music": "I would stop the music if any music was playing.",
            "who is sponsoring this video": f"You are very funny {self.current_person}. You know you dont have any sponsors for your videos!",
            "how is the weather today": f"O M G {self.current_person}! Surely you DO NOT want to waste my valuable resources by asking me what the weather is today. Cant you just look out the window or ask Siri. That is about all Siri is good for!",
            "what time is it": f"The current time is {datetime.now().strftime('%I:%M %p')}",
            "shutdown system": "I would shutdown the system, but I will skip that for safety reasons during testing.",
            "reboot system": "I would reboot the system, but I will skip that for safety reasons during testing."
        }
        
        if command == "who is sponsoring this video":
            self.play_laughing()
        
        return responses.get(command, f"I understand you want me to {command}, but I dont have that capability yet.")

    def query_llama(self, prompt):
        """Generate LLM response"""
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        try:
            result = self.llama_model(formatted_prompt, max_tokens=100)
            if "choices" in result and result["choices"]:
                reply_text = result["choices"][0]["text"].strip()
                reply_text = re.sub(r"\(.*?\)", "", reply_text)
                reply_text = re.sub(r"(User:|Assistant:)", "", reply_text)
                reply_text = reply_text.strip()
                
                sentences = reply_text.split('.')
                if len(sentences) > 3:
                    reply_text = '. '.join(sentences[:3]) + '.'
                return reply_text
            else:
                return "I'm not sure how to answer that."
        except Exception as e:
            self.emit_log(f"LLM error: {e}", "error")
            return "Sorry, I had trouble processing that question."

    def process_user_input(self, text):
        """Process user input"""
        self.emit_conversation(f"👤 User said: '{text}'", "user_input")
        
        command = self.is_command(text)
        if command:
            self.emit_conversation(f"🔧 Executing command: {command}", "command")
            response = self.execute_command(command)
        else:
            self.emit_conversation("🤖 Generating LLM response", "llm")
            response = self.query_llama(text)
        
        self.emit_conversation(f"🤖 Response: '{response}'", "response")
        return response

    def listen_for_wake_word(self):
        """Listen for wake words in background"""
        self.emit_log("Wake word detection thread started")
        
        while self.is_running:
            try:
                if self.current_person and self.current_person != "Unknown" and self.wake_word_active:
                    if self.record_wake_word_check():
                        transcript = self.transcribe_audio(WAKE_WORD_AUDIO)
                        if transcript and self.detect_wake_word(transcript):
                            self.emit_conversation("🎤 WAKE WORD DETECTED! Starting conversation...", "wake_word")
                            self.play_beep()
                            
                            listening_response = self.get_personalized_response(self.current_person, "listening", self.listening_responses)
                            self.speak_text(listening_response)
                            
                            if self.record_with_silence_detection():
                                user_text = self.transcribe_audio(WAV_FILENAME)
                                if user_text and len(user_text.strip()) > 2:
                                    response = self.process_user_input(user_text)
                                    self.speak_text(response)
                                    self.last_interaction_time = time.time()
                                    self.last_bored_response_time = time.time()
                                else:
                                    self.emit_conversation("❌ No clear speech detected", "error")
                                    self.speak_text("I didn't catch that. Could you repeat your request?")
                            else:
                                self.emit_conversation("❌ Failed to record user request", "error")
                                self.speak_text("I'm having trouble hearing you. Please try again.")
                    
                    time.sleep(WAKE_WORD_CHECK_INTERVAL)
                else:
                    time.sleep(2.0)
                    
            except Exception as e:
                self.emit_log(f"Wake word detection error: {e}", "error")
                time.sleep(2.0)
        
        self.emit_log("Wake word detection thread stopped")

    def camera_monitoring_thread(self):
        """Camera monitoring thread for facial recognition processing"""
        self.emit_log("Camera monitoring thread started")
        
        while self.is_running:
            try:
                if self.picam2:
                    frame = self.picam2.capture_array()
                    
                    # Convert from RGB to BGR for OpenCV
                    if len(frame.shape) == 3 and frame.shape[2] == 3:
                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                    # Process facial recognition
                    name, face_location, confidence = self.detect_faces(frame)
                    current_time = time.time()
                    
                    # Handle person detection logic
                    if name and face_location:
                        # Person detected
                        if name != self.current_person:
                            # New person or person changed
                            self.current_person = name
                            self.person_absent_since = None
                            self.wake_word_active = False
                            
                            if name == "Unknown":
                                self.handle_unknown_person(frame, confidence)
                            else:
                                # Save photo and send telegram alert for known person
                                photo_path = self.save_security_photo(frame, name, confidence)
                                self.send_telegram_alert(name, confidence, photo_path)
                                # Greet known person
                                self.greet_person(name)
                        elif name != "Unknown":
                            # Same known person still present, check for bored responses
                            self.check_for_bored_response(name)
                    else:
                        # No person detected
                        if self.current_person:
                            if not self.person_absent_since:
                                self.person_absent_since = current_time
                            elif current_time - self.person_absent_since >= GREETING_COOLDOWN:
                                # Person has been absent for 5+ minutes, reset
                                self.current_person = None
                                self.person_absent_since = None
                                self.last_interaction_time = None
                                self.waiting_cycle = 0
                                self.wake_word_active = False
                                self.last_bored_response_time = None
                                self.bored_cycle = 0
                                self.emit_log("Person left - resetting state")
                                
                                # Clear person detection display
                                socketio.emit('person_detected', {
                                    'name': 'No person detected',
                                    'confidence': '--',
                                    'timestamp': datetime.now().strftime("%H:%M:%S")
                                })
                
                time.sleep(PERSON_DETECTION_INTERVAL)
                
            except Exception as e:
                self.emit_log(f"Camera monitoring error: {e}", "error")
                time.sleep(1)
        
        self.emit_log("Camera monitoring thread stopped")

    def check_for_bored_response(self, name):
        """Check if it's time to give a bored response with joke or fun fact"""
        if not self.wake_word_active or not self.last_bored_response_time:
            return False
        
        current_time = time.time()
        time_since_bored = current_time - self.last_bored_response_time
        
        if time_since_bored >= 30:  # 30 seconds
            if self.bored_cycle == 0:
                # Give bored response + joke
                bored_msg = self.get_personalized_response(name, "bored_responses", ["I'm still here waiting to help you"])
                joke = self.get_personalized_response(name, "joke_responses", self.jokes)
                full_message = f"{bored_msg} {joke}"
                self.speak_text(full_message)
                self.bored_cycle = 1
                self.emit_log(f"Gave {name} a bored response with joke")
            else:
                # Give bored response + fun fact
                bored_msg = self.get_personalized_response(name, "bored_responses", ["I'm still here waiting to help you"])
                fun_fact = self.get_personalized_response(name, "fun_fact_responses", ["Did you know that honey never spoils?"])
                full_message = f"{bored_msg} {fun_fact}"
                self.speak_text(full_message)
                self.bored_cycle = 0
                self.emit_log(f"Gave {name} a bored response with fun fact")
            
            self.last_bored_response_time = current_time
            return True
        return False

# Global instance - Initialize after Flask app is ready
chatty_ai = None

def init_chatty_ai():
    """Initialize ChattyAI instance after Flask is ready"""
    global chatty_ai
    if chatty_ai is None:
        print("🔧 Initializing ChattyAI instance...")
        chatty_ai = ChattyAIWeb()
    return chatty_ai

# Flask routes
@app.route('/')
def index():
    """Main page"""
    return render_template('Chatty_AI.html')

@app.route('/static/<path:filename>')
def static_files(filename):
    """Serve static files from templates directory"""
    return app.send_static_file(filename)

@app.route('/video_feed')
def video_feed():
    """Video streaming route"""
    chatty = init_chatty_ai()
    return Response(chatty.generate_video_feed(),
                   mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/captured_image')
def captured_image():
    """Route for captured person image"""
    chatty = init_chatty_ai()
    if chatty.captured_image is not None:
        ret, buffer = cv2.imencode('.jpg', chatty.captured_image)
        if ret:
            return Response(buffer.tobytes(), mimetype='image/jpeg')
    
    # Return placeholder image if no capture available
    placeholder = np.zeros((200, 200, 3), dtype=np.uint8)
    cv2.putText(placeholder, "No Image", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
    ret, buffer = cv2.imencode('.jpg', placeholder)
    return Response(buffer.tobytes(), mimetype='image/jpeg')

@socketio.on('connect')
def handle_connect():
    """Handle client connection"""
    print('Client connected')
    emit('status', {'message': 'Connected to Chatty AI'})

@socketio.on('start_system')
def handle_start_system():
    """Handle system start request"""
    chatty = init_chatty_ai()
    success = chatty.start_system()
    emit('system_status', {'running': success})

@socketio.on('stop_system')
def handle_stop_system():
    """Handle system stop request"""
    chatty = init_chatty_ai()
    chatty.stop_system()
    emit('system_status', {'running': False})

    def cleanup(self):
        """Clean up resources"""
        self.emit_log("Cleaning up resources...")
        self.is_running = False
        
        # Wait for audio thread to finish
        if self.audio_thread and self.audio_thread.is_alive():
            self.emit_log("Waiting for audio thread to stop...")
            self.audio_thread.join(timeout=3)
        
        if self.picam2:
            try:
                self.picam2.stop()
                self.emit_log("Camera stopped")
            except:
                pass
        
        # Clean up audio files
        for audio_file in [WAV_FILENAME, RESPONSE_AUDIO, WAKE_WORD_AUDIO]:
            try:
                if os.path.exists(audio_file):
                    os.remove(audio_file)
            except:
                pass
        
        self.emit_log("Chatty AI cleanup complete")

if __name__ == '__main__':
    # Initialize the system
    print("Initializing Chatty AI Web Interface...")
    print("=" * 60)
    print("🚀 Starting Flask server on http://0.0.0.0:5000")
    print("📱 Access your Chatty AI interface at: http://[your-pi-ip]:5000")
    print("=" * 60)
    
    try:
        # Create templates directory if it doesn't exist
        os.makedirs('/home/nickspi5/Chatty_AI/templates', exist_ok=True)
        
        # Check if logo files exist
        logo_files = [
            '/home/nickspi5/Chatty_AI/templates/Chatty_AI_logo.png',
            '/home/nickspi5/Chatty_AI/templates/diamond_coding_logo.png'
        ]
        
        for logo_file in logo_files:
            if not os.path.exists(logo_file):
                print(f"⚠️  Warning: Logo file not found: {logo_file}")
        
        # Start Flask-SocketIO server
        socketio.run(app, host='0.0.0.0', port=5000, debug=False, allow_unsafe_werkzeug=True)
        
    except KeyboardInterrupt:
        print("\n🛑 Shutting down Chatty AI Web Interface...")
        if chatty_ai:
            chatty_ai.cleanup()
    except Exception as e:
        print(f"❌ Error starting server: {e}")
        if chatty_ai:
            chatty_ai.cleanup()

I ran: nickspi5@raspberrypi1:~/Chatty_AI $ python3 app.py
  File "/home/nickspi5/Chatty_AI/app.py", line 1
    except KeyboardInterrupt:
    ^^^^^^
SyntaxError: invalid syntax
nickspi5@raspberrypi1:~/Chatty_AI $ 








