

Check faster-whisper Streaming Capability
Let's first check if faster-whisper supports streaming transcription:
cd /home/nickspi5/Chatty_AI
source chatty-venv/bin/activate

python3 << 'EOF'
from faster_whisper import WhisperModel
import inspect

# Check available methods
model = WhisperModel("tiny", device="cpu", compute_type="int8")
print("Available methods:")
for name, method in inspect.getmembers(model, predicate=inspect.ismethod):
    if not name.startswith('_'):
        print(f"  - {name}")

# Check transcribe signature
print("\nTranscribe signature:")
print(inspect.signature(model.transcribe))
EOF

Check llama-cpp-python Streaming Capability
python3 << 'EOF'
from llama_cpp import Llama
import inspect

# Check if streaming is supported
print("Checking Llama streaming capability...")
llm = Llama(
    model_path="tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    n_ctx=512,
    n_threads=4,
    verbose=False
)

# Test streaming
print("\nTesting streaming generation:")
prompt = "What is the capital of France?"
for token in llm(prompt, max_tokens=20, stream=True):
    print(token['choices'][0]['text'], end='', flush=True)
print("\n\nStreaming works!")
EOF

