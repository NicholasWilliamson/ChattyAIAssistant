Thank you Claude!

Before I start working on the wake word detection for my local AI Assistant, I went to first test my local AI_Assistant again.

My Python script for my local AI Assistant - run_chatty_ai_6.py, which is located in my /home/nickspi5/Chatty_AI folder - is as follows:

run_chatty_ai_6.py
!/usr/bin/env python3
"""
run_chatty_ai_6.py
Record voice, transcribe using Whisper, reply with TinyLLaMA, and speak with Piper.
"""

import os
import subprocess
import sounddevice as sd
import soundfile as sf
from faster_whisper import WhisperModel
from llama_cpp import Llama

# -------------------------------
# Config
# -------------------------------
WHISPER_MODEL_SIZE = "base"
LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
VOICE_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx"
CONFIG_PATH = "/home/nickspi5/Chatty_AI/voices/en_US-amy-low/en_US-amy-low.onnx.json"
PIPER_EXECUTABLE = "/home/nickspi5/Chatty_AI/piper/piper"
WAV_FILENAME = "user_input.wav"
RESPONSE_AUDIO = "output.wav"

# -------------------------------
# Record 5 seconds of audio
# -------------------------------
def record_audio(filename=WAV_FILENAME, duration=5, samplerate=16000, channels=1):
    print("üé§ Recording 5s of audio...")
    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=channels, dtype='int16')
    sd.wait()
    sf.write(filename, audio, samplerate)
    print(f"‚úÖ Saved audio to: {filename}")

# -------------------------------
# Transcribe using Whisper
# -------------------------------
def transcribe_audio(filename):
    print("üß† Transcribing with Faster Whisper...")
    model = WhisperModel(WHISPER_MODEL_SIZE, device="cpu", compute_type="int8")
    segments, _ = model.transcribe(filename)
    transcript = " ".join(segment.text for segment in segments).strip()
    print(f"üìù Transcript: {transcript}")
    return transcript

# -------------------------------
# Generate LLM response
# -------------------------------
def query_llama(prompt):
    print("ü§ñ Generating response...")
    try:
        llm = Llama(model_path=LLAMA_MODEL_PATH, n_ctx=2048, temperature=0.7, repeat_penalty=1.1, n_gpu_layers=0, verbose=False)
    except Exception as e:
        print(f"‚ùå Failed to load TinyLLaMA: {e}")
        return "Sorry, I couldn't load the AI model."

    formatted_prompt = f"You are a friendly, helpful assistant.\nUser: {prompt}\nAssistant: "

    try:
        result = llm(formatted_prompt, max_tokens=50)
        if "choices" in result and result["choices"]:
            import re
            reply_text = result["choices"][0]["text"].strip()
            reply_text = re.sub(r"\(.*?\)", "", reply_text) # Remove roleplay like (laughs)
            reply_text = re.sub(r"(User:|Assistant:)", "", reply_text) # Remove speaker labels
            reply_text = reply_text.strip()
            print(f"üí¨ Chatty AI says: {reply_text}")
            speak_text(reply_text)
            return reply_text
        else:
            print("‚ö†Ô∏è No response from model.")
            return "I did not understand that."
    except Exception as e:
        print(f"‚ùå Inference failed: {e}")
        return "An error occurred while generating a reply."

# -------------------------------
# Speak with Piper
# -------------------------------
def speak_text(text):
    print("üîä Speaking with Piper ...")
    try:
        command = [
            PIPER_EXECUTABLE,
            "--model", VOICE_PATH,
            "--config", CONFIG_PATH,
            "--output_file", RESPONSE_AUDIO
        ]
        subprocess.run(command, input=text.encode("utf-8"), check=True)
        subprocess.run(["aplay", RESPONSE_AUDIO])
    except subprocess.CalledProcessError as e:
        print("‚ùå Piper playback failed:", e)

# -------------------------------
# Main Orchestration
# -------------------------------
def main():
    record_audio()
    user_text = transcribe_audio(WAV_FILENAME)
    if not user_text:
        print("‚ùå No voice input detected.")
        return
    query_llama(user_text)

if __name__ == "__main__":
    main()

I ran: nickspi5@raspberrypi1:~ $ cd Chatty_AI
nickspi5@raspberrypi1:~/Chatty_AI $ source chatty-venv/bin/activate
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ python3 run_chatty_ai_6.py
Traceback (most recent call last):
  File "/home/nickspi5/Chatty_AI/run_chatty_ai_6.py", line 11, in <module>
    from faster_whisper import WhisperModel
ModuleNotFoundError: No module named 'faster_whisper'
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ 

How can I fix this?


