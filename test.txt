

# CHATTY AI FIXES - Apply these changes to your chatty_ai.py file

# 1. TURN OFF CAMERA FRAME DEBUGGING
# Line 772: Comment out or remove this line:
# self.emit_log(f"Streaming frame {frame_count}", 'debug')

# Replace line 772 with:
# (Remove or comment out the streaming frame debug message)

# 2. FIX SILENCE THRESHOLD - Line 101
# Current: SILENCE_THRESHOLD = 0.35
# Change to: SILENCE_THRESHOLD = 0.025
# The current threshold of 0.35 is way too high - that's why it never detects silence

# 3. ADD MISSING MIN_SILENCE_DURATION - Add after line 101
MIN_SILENCE_DURATION = 1.5  # 1.5 seconds of silence to stop recording

# 4. ENHANCE WAKE WORD DEBUGGING - Replace record_wake_word_check method (starting line 1544)
def record_wake_word_check(self):
    """Record short audio clip for wake word detection with debugging"""
    try:
        if not self.audio_recording_lock.acquire(blocking=False):
            self.emit_log("Audio system busy, skipping wake word check", 'debug')
            return False
        
        try:
            # Record 4 seconds of audio for wake word detection
            duration = 4.0
            sample_rate = 16000
            
            self.emit_log(f"üé§ Recording {duration}s for wake word detection...", 'debug')
            audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')
            sd.wait()
            
            # Check if audio contains sound above threshold
            rms = np.sqrt(np.mean(audio_data**2))
            wake_word_threshold = SILENCE_THRESHOLD * 2.0  # More sensitive threshold
            
            self.emit_log(f"üìä Wake word audio - RMS: {rms:.4f} | Threshold: {wake_word_threshold:.4f}", 'info')
            
            if rms > wake_word_threshold:
                sf.write(WAKE_WORD_AUDIO, audio_data, sample_rate)
                self.emit_log(f"‚úÖ Wake word audio saved ({duration}s recorded)", 'info')
                return True
            else:
                self.emit_log(f"üîá Audio too quiet for wake word detection", 'debug')
                return False
                
        finally:
            self.audio_recording_lock.release()
            
    except Exception as e:
        self.emit_log(f"‚ùå Wake word recording error: {e}", 'error')
        return False

# 5. IMPROVE SILENCE DETECTION IN RECORDING - Replace the record_with_silence_detection method
def record_with_silence_detection(self):
    """Record audio until silence is detected with enhanced debugging"""
    self.emit_log("üé§ Recording started... (speak now, will stop after 1.5s of silence)", 'info')
    
    audio_data = []
    silence_duration = 0
    recording_duration = 0
    check_interval = 0.1  # Check every 100ms
    samples_per_check = int(SAMPLE_RATE * check_interval)
    
    def audio_callback(indata, frames, time, status):
        if status:
            self.emit_log(f"Audio status: {status}", 'warning')
        audio_data.extend(indata[:, 0])  # Take first channel
    
    # Start recording
    with sd.InputStream(callback=audio_callback, 
                      samplerate=SAMPLE_RATE, 
                      channels=1,
                      dtype='float32'):
        
        while recording_duration < 30:  # Max 30 seconds
            time.sleep(check_interval)
            recording_duration += check_interval
            
            # Check for silence in recent audio
            if len(audio_data) >= samples_per_check:
                recent_audio = np.array(audio_data[-samples_per_check:])
                rms = np.sqrt(np.mean(recent_audio**2))
                
                # Enhanced debug output every second
                if int(recording_duration * 10) % 10 == 0:  # Every 1 second
                    self.emit_log(f"üîä Recording: {recording_duration:.1f}s | Audio level: {rms:.4f} | Silence: {silence_duration:.1f}s", 'debug')
                
                if rms < SILENCE_THRESHOLD:
                    silence_duration += check_interval
                    if silence_duration >= MIN_SILENCE_DURATION:
                        self.emit_log(f"‚úÖ Silence detected! Recorded {recording_duration:.1f}s total", 'info')
                        break
                else:
                    if silence_duration > 0:
                        self.emit_log(f"üó£Ô∏è Speech resumed (was silent for {silence_duration:.1f}s)", 'debug')
                    silence_duration = 0  # Reset silence counter
    
    if recording_duration >= 30:
        self.emit_log("‚è∞ Recording stopped - reached 30 second limit", 'warning')
    
    # Save audio file
    if len(audio_data) > 0:
        audio_array = np.array(audio_data)
        sf.write(USER_AUDIO_FILE, audio_array, SAMPLE_RATE)
        self.emit_log(f"üíæ Audio saved: {len(audio_data)/SAMPLE_RATE:.1f}s of audio", 'info')
        return True
    else:
        self.emit_log("‚ùå No audio data recorded", 'error')
        return False

# 6. ENHANCE WAKE WORD DETECTION DEBUGGING - Update detect_wake_word method
def detect_wake_word(self, transcript):
    """Detect wake words in transcript with enhanced debugging"""
    if not transcript:
        self.emit_log("üîç Empty transcript - no wake word check", 'debug')
        return False
        
    transcript_lower = transcript.lower().strip()
    self.emit_log(f"üîç Checking transcript: '{transcript_lower}'", 'info')
    
    # Check each wake word
    for wake_word in WAKE_WORDS:
        wake_word_lower = wake_word.lower().strip()
        if wake_word_lower in transcript_lower:
            self.emit_log(f"üéØ WAKE WORD DETECTED: '{wake_word}' found in '{transcript}'", 'success')
            return True
    
    self.emit_log(f"‚ùå No wake word found in: '{transcript}'", 'debug')
    return False

# 7. UPDATE TRANSCRIBE_AUDIO TO ADD DEBUGGING
def transcribe_audio(self, audio_file):
    """Transcribe audio file with debugging"""
    try:
        if not os.path.exists(audio_file):
            self.emit_log(f"Audio file not found: {audio_file}", 'error')
            return None
            
        self.emit_log(f"üéß Transcribing audio file: {audio_file}", 'debug')
        result = self.whisper_model.transcribe(audio_file)
        transcript = result.get("text", "").strip()
        
        if transcript:
            self.emit_log(f"üìù Transcription result: '{transcript}'", 'info')
        else:
            self.emit_log("üìù Transcription returned empty text", 'warning')
            
        return transcript
        
    except Exception as e:
        self.emit_log(f"‚ùå Transcription error: {e}", 'error')
        return None


