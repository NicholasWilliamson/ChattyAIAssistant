Now more errors have occured, again ....

I saw:

.... more ....
cd llama.cpp && git checkout -b build cc4a95426d17417d3c83f12bdb514fbe8abe2a88 && git submodule update --init --recursive --depth 1
Switched to a new branch 'build'
Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'kompute'
Cloning into '/home/nickspi5/LocalAI/backend/cpp/llama/llama.cpp/kompute'...
remote: Total 0 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
remote: Enumerating objects: 20, done.
remote: Counting objects: 100% (20/20), done.
remote: Compressing objects: 100% (11/11), done.
remote: Total 11 (delta 8), reused 2 (delta 0), pack-reused 0 (from 0)
Unpacking objects: 100% (11/11), 1.68 KiB | 575.00 KiB/s, done.
From https://github.com/nomic-ai/kompute
 * branch            4565194ed7c32d1d2efa32ceab4d3c6cae006306 -> FETCH_HEAD
Submodule path 'kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'
mkdir -p llama.cpp/examples/grpc-server
cp -r /home/nickspi5/LocalAI/backend/cpp/llama/CMakeLists.txt llama.cpp/examples/grpc-server/
cp -r /home/nickspi5/LocalAI/backend/cpp/llama/grpc-server.cpp llama.cpp/examples/grpc-server/
cp -rfv /home/nickspi5/LocalAI/backend/cpp/llama/json.hpp llama.cpp/examples/grpc-server/
'/home/nickspi5/LocalAI/backend/cpp/llama/json.hpp' -> 'llama.cpp/examples/grpc-server/json.hpp'
cp -rfv /home/nickspi5/LocalAI/backend/cpp/llama/utils.hpp llama.cpp/examples/grpc-server/
'/home/nickspi5/LocalAI/backend/cpp/llama/utils.hpp' -> 'llama.cpp/examples/grpc-server/utils.hpp'
echo "add_subdirectory(grpc-server)" >> llama.cpp/examples/CMakeLists.txt
cp -rfv llama.cpp/examples/llava/clip.h llama.cpp/examples/grpc-server/clip.h
'llama.cpp/examples/llava/clip.h' -> 'llama.cpp/examples/grpc-server/clip.h'
cp -rfv llama.cpp/examples/llava/llava.cpp llama.cpp/examples/grpc-server/llava.cpp
'llama.cpp/examples/llava/llava.cpp' -> 'llama.cpp/examples/grpc-server/llava.cpp'
echo '#include "llama.h"' > llama.cpp/examples/grpc-server/llava.h
cat llama.cpp/examples/llava/llava.h >> llama.cpp/examples/grpc-server/llava.h
cp -rfv llama.cpp/examples/llava/clip.cpp llama.cpp/examples/grpc-server/clip.cpp
'llama.cpp/examples/llava/clip.cpp' -> 'llama.cpp/examples/grpc-server/clip.cpp'
cd llama.cpp && mkdir -p build && cd build && cmake ..  && cmake --build . --config Release
-- The C compiler identification is GNU 12.2.0
-- The CXX compiler identification is GNU 12.2.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Found ZLIB: /usr/lib/aarch64-linux-gnu/libz.so (found version "1.2.13") 
CMake Error at examples/grpc-server/CMakeLists.txt:39 (find_package):
  Could not find a package configuration file provided by "gRPC" with any of
  the following names:

    gRPCConfig.cmake
    grpc-config.cmake

  Add the installation prefix of "gRPC" to CMAKE_PREFIX_PATH or set
  "gRPC_DIR" to a directory containing one of the above files.  If "gRPC"
  provides a separate development package or SDK, be sure it has been
  installed.


-- Configuring incomplete, errors occurred!
See also "/home/nickspi5/LocalAI/backend/cpp/llama/llama.cpp/build/CMakeFiles/CMakeOutput.log".
See also "/home/nickspi5/LocalAI/backend/cpp/llama/llama.cpp/build/CMakeFiles/CMakeError.log".
make[1]: *** [Makefile:75: grpc-server] Error 1
make[1]: Leaving directory '/home/nickspi5/LocalAI/backend/cpp/llama'
make: *** [Makefile:517: backend/cpp/llama/grpc-server] Error 2
nickspi5@raspberrypi1:~/LocalAI $ 

I have now spent over 7 hours wasting my time with this.

I do not want to use a Docker service for this. It is too slow. I want a Python script to run my AI Assistant.

Why do we need to use LocalAI? I want to delete LocalAI and revert back to a simple Python script, such as I used previously when I used vosk and eSpeak, but this time I want to use Whisper and Piper instead.

