

python3 << 'EOF'
import time
from llama_cpp import Llama

print("Loading model...")
llm = Llama(
    model_path="tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=4,
    verbose=False
)
print("Model loaded.\n")

# Test different prompts
prompts = [
    "What is the longest river in the world?",
    "What is the deepest ocean in the world?",
    "What is the longest road in the world?",
    "Tell me about the moon.",
]

for prompt in prompts:
    formatted = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
    
    print(f"Prompt: '{prompt}'")
    
    start = time.perf_counter()
    first_token_time = None
    first_sentence_time = None
    full_response = ""
    token_count = 0
    
    for token_data in llm(formatted, max_tokens=100, stream=True, 
                          stop=["\n", "\n\n", "User:", "Human:", "Assistant:"]):
        token = token_data['choices'][0]['text']
        full_response += token
        token_count += 1
        
        if first_token_time is None:
            first_token_time = time.perf_counter() - start
        
        if first_sentence_time is None and any(full_response.rstrip().endswith(p) for p in ['.', '!', '?']):
            first_sentence_time = time.perf_counter() - start
    
    total_time = time.perf_counter() - start
    
    print(f"  First token: {first_token_time:.2f}s")
    print(f"  First sentence: {first_sentence_time:.2f}s" if first_sentence_time else "  First sentence: N/A")
    print(f"  Total time: {total_time:.2f}s")
    print(f"  Tokens: {token_count}")
    print(f"  Response: {full_response[:80]}...")
    print()
EOF

