No more errors have occured ....

I saw:

.... more ....
Consolidate compiler generated dependencies of target ggml
make[5]: Leaving directory '/home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build'
make  -f ggml/src/CMakeFiles/ggml.dir/build.make ggml/src/CMakeFiles/ggml.dir/build
make[5]: Entering directory '/home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build'
make[5]: Nothing to be done for 'ggml/src/CMakeFiles/ggml.dir/build'.
make[5]: Leaving directory '/home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build'
[100%] Built target ggml
make[4]: Leaving directory '/home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build'
/usr/bin/cmake -E cmake_progress_start /home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build/CMakeFiles 0
make[3]: Leaving directory '/home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build'
make[2]: Leaving directory '/home/nickspi5/LocalAI/sources/go-bert/bert.cpp/build'
ar src libgobert.a gobert.o ggml.o 
make[1]: Leaving directory '/home/nickspi5/LocalAI/sources/go-bert'
CGO_LDFLAGS="" C_INCLUDE_PATH=/home/nickspi5/LocalAI/sources/go-bert LIBRARY_PATH=/home/nickspi5/LocalAI/sources/go-bert \
go build -ldflags "-X "github.com/go-skynet/LocalAI/internal.Version=v2.12.0" -X "github.com/go-skynet/LocalAI/internal.Commit=2bbb221fb18cb119c384f86739c1433cec8a491b"" -tags "" -o backend-assets/grpc/bert-embeddings ./backend/go/llm/bert/
echo "BUILD_GRPC_FOR_BACKEND_LLAMA is not defined."
BUILD_GRPC_FOR_BACKEND_LLAMA is not defined.
LLAMA_VERSION=cc4a95426d17417d3c83f12bdb514fbe8abe2a88 make -C backend/cpp/llama grpc-server
make[1]: Entering directory '/home/nickspi5/LocalAI/backend/cpp/llama'
git clone --recurse-submodules https://github.com/ggerganov/llama.cpp llama.cpp
Cloning into 'llama.cpp'...
remote: Enumerating objects: 56821, done.
remote: Counting objects: 100% (97/97), done.
remote: Compressing objects: 100% (73/73), done.
remote: Total 56821 (delta 63), reused 24 (delta 24), pack-reused 56724 (from 3)
Receiving objects: 100% (56821/56821), 135.65 MiB | 445.00 KiB/s, done.
Resolving deltas: 100% (41147/41147), done.
if [ -z "cc4a95426d17417d3c83f12bdb514fbe8abe2a88" ]; then \
	exit 1; \
fi
cd llama.cpp && git checkout -b build cc4a95426d17417d3c83f12bdb514fbe8abe2a88 && git submodule update --init --recursive --depth 1
Switched to a new branch 'build'
Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'kompute'
Cloning into '/home/nickspi5/LocalAI/backend/cpp/llama/llama.cpp/kompute'...
remote: Total 0 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
remote: Enumerating objects: 20, done.
remote: Counting objects: 100% (20/20), done.
remote: Compressing objects: 100% (11/11), done.
remote: Total 11 (delta 8), reused 2 (delta 0), pack-reused 0 (from 0)
Unpacking objects: 100% (11/11), 1.68 KiB | 575.00 KiB/s, done.
From https://github.com/nomic-ai/kompute
 * branch            4565194ed7c32d1d2efa32ceab4d3c6cae006306 -> FETCH_HEAD
Submodule path 'kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'
mkdir -p llama.cpp/examples/grpc-server
cp -r /home/nickspi5/LocalAI/backend/cpp/llama/CMakeLists.txt llama.cpp/examples/grpc-server/
cp -r /home/nickspi5/LocalAI/backend/cpp/llama/grpc-server.cpp llama.cpp/examples/grpc-server/
cp -rfv /home/nickspi5/LocalAI/backend/cpp/llama/json.hpp llama.cpp/examples/grpc-server/
'/home/nickspi5/LocalAI/backend/cpp/llama/json.hpp' -> 'llama.cpp/examples/grpc-server/json.hpp'
cp -rfv /home/nickspi5/LocalAI/backend/cpp/llama/utils.hpp llama.cpp/examples/grpc-server/
'/home/nickspi5/LocalAI/backend/cpp/llama/utils.hpp' -> 'llama.cpp/examples/grpc-server/utils.hpp'
echo "add_subdirectory(grpc-server)" >> llama.cpp/examples/CMakeLists.txt
cp -rfv llama.cpp/examples/llava/clip.h llama.cpp/examples/grpc-server/clip.h
'llama.cpp/examples/llava/clip.h' -> 'llama.cpp/examples/grpc-server/clip.h'
cp -rfv llama.cpp/examples/llava/llava.cpp llama.cpp/examples/grpc-server/llava.cpp
'llama.cpp/examples/llava/llava.cpp' -> 'llama.cpp/examples/grpc-server/llava.cpp'
echo '#include "llama.h"' > llama.cpp/examples/grpc-server/llava.h
cat llama.cpp/examples/llava/llava.h >> llama.cpp/examples/grpc-server/llava.h
cp -rfv llama.cpp/examples/llava/clip.cpp llama.cpp/examples/grpc-server/clip.cpp
'llama.cpp/examples/llava/clip.cpp' -> 'llama.cpp/examples/grpc-server/clip.cpp'
cd llama.cpp && mkdir -p build && cd build && cmake ..  && cmake --build . --config Release
-- The C compiler identification is GNU 12.2.0
-- The CXX compiler identification is GNU 12.2.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Found ZLIB: /usr/lib/aarch64-linux-gnu/libz.so (found version "1.2.13") 
CMake Error at examples/grpc-server/CMakeLists.txt:39 (find_package):
  Could not find a package configuration file provided by "gRPC" with any of
  the following names:

    gRPCConfig.cmake
    grpc-config.cmake

  Add the installation prefix of "gRPC" to CMAKE_PREFIX_PATH or set
  "gRPC_DIR" to a directory containing one of the above files.  If "gRPC"
  provides a separate development package or SDK, be sure it has been
  installed.


-- Configuring incomplete, errors occurred!
See also "/home/nickspi5/LocalAI/backend/cpp/llama/llama.cpp/build/CMakeFiles/CMakeOutput.log".
See also "/home/nickspi5/LocalAI/backend/cpp/llama/llama.cpp/build/CMakeFiles/CMakeError.log".
make[1]: *** [Makefile:75: grpc-server] Error 1
make[1]: Leaving directory '/home/nickspi5/LocalAI/backend/cpp/llama'
make: *** [Makefile:517: backend/cpp/llama/grpc-server] Error 2
nickspi5@raspberrypi1:~/LocalAI $ 






