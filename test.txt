Thank you, Claude.

I ran: (chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ echo "1. Current model files in tinyllama-models:"
ls -la tinyllama-models/
1. Current model files in tinyllama-models:
total 2425124
drwxr-xr-x  2 nickspi5 nickspi5       4096 Jul 19 19:48 .
drwxr-xr-x 16 nickspi5 nickspi5       4096 Aug  9 13:44 ..
-rwxr-xr-x  1 nickspi5 nickspi5  668788096 Jul 19 18:08 tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
-rwxr-xr-x  1 nickspi5 nickspi5  643728768 Jul 19 18:36 tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf
-rwxr-xr-x  1 nickspi5 nickspi5 1170781568 Jan  1  2024 tinyllama-1.1b-chat-v1.0.Q8_0.gguf
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ echo -e "\nIn app.py:"
grep -n "LLAMA_MODEL_PATH\|tinyllama" app.py || echo "No LLAMA_MODEL_PATH found in app.py"

In app.py:
40:LLAMA_MODEL_PATH = "tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
295:                model_path=LLAMA_MODEL_PATH,
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ echo -e "\n4. Verifying model path configuration..."
echo "Current model path configuration is correct (Q4_K_M.gguf)"
echo "Model file exists and is accessible: $(ls -lh tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf)"

4. Verifying model path configuration...
Current model path configuration is correct (Q4_K_M.gguf)
Model file exists and is accessible: -rwxr-xr-x 1 nickspi5 nickspi5 638M Jul 19 18:08 tinyllama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ python3 debug_app.py
[13:54:55] DEBUG: Importing Flask components...
[13:54:55] DEBUG: Basic imports successful
[13:54:55] DEBUG: Attempting to import ChattyAI...
[13:54:59] DEBUG: ✅ ChattyAI imported successfully
[13:54:59] DEBUG: Creating Flask app...
[13:54:59] DEBUG: Setting up global variables...
[13:54:59] DEBUG: Starting debug version of Chatty AI...
[13:54:59] DEBUG: Web interface will be at: http://192.168.1.16:5000
 * Serving Flask app 'debug_app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.16:5000
Press CTRL+C to quit
[13:55:49] DEBUG: Index route accessed
127.0.0.1 - - [09/Aug/2025 13:55:49] "GET / HTTP/1.1" 200 -
127.0.0.1 - - [09/Aug/2025 13:55:49] "GET /templates/Chatty_AI_logo.png HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:49] "GET /video_feed HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:50] "GET /templates/diamond_coding_logo.png HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:50] "GET /captured_image HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:50] "GET /socket.io/?EIO=4&transport=polling&t=PYC-7Mq HTTP/1.1" 200 -
[13:55:50] DEBUG: Client connected to SocketIO
127.0.0.1 - - [09/Aug/2025 13:55:50] "POST /socket.io/?EIO=4&transport=polling&t=PYC-7Na&sid=eg1pQOFH8EOHbtVrAAAA HTTP/1.1" 200 -
127.0.0.1 - - [09/Aug/2025 13:55:50] "GET /socket.io/?EIO=4&transport=polling&t=PYC-7Nc&sid=eg1pQOFH8EOHbtVrAAAA HTTP/1.1" 200 -
127.0.0.1 - - [09/Aug/2025 13:55:50] "GET /socket.io/?EIO=4&transport=polling&t=PYC-7Nz&sid=eg1pQOFH8EOHbtVrAAAA HTTP/1.1" 200 -
127.0.0.1 - - [09/Aug/2025 13:55:50] "GET /favicon.ico HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:52] "GET /video_feed?t=1754711752109 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:54] "GET /video_feed?t=1754711754119 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:56] "GET /video_feed?t=1754711756127 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:55:58] "GET /video_feed?t=1754711758135 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:00] "GET /video_feed?t=1754711760144 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:02] "GET /video_feed?t=1754711762153 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:04] "GET /video_feed?t=1754711764162 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:06] "GET /video_feed?t=1754711766170 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:08] "GET /video_feed?t=1754711768175 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:10] "GET /video_feed?t=1754711770181 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:12] "GET /video_feed?t=1754711772187 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:14] "GET /video_feed?t=1754711774195 HTTP/1.1" 404 -
[13:56:15] DEBUG: Start system requested...
[13:56:15] DEBUG: Creating ChattyAI instance...
Response files loaded successfully
Personalized responses loaded for 3 people
Loading AI models...
127.0.0.1 - - [09/Aug/2025 13:56:16] "GET /video_feed?t=1754711776204 HTTP/1.1" 404 -
Whisper model loaded
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
LLaMA model loaded
Loaded 37 face encodings
Telegram configuration loaded
[2:15:39.212914549] [3471]  INFO Camera camera_manager.cpp:326 libcamera v0.5.1+100-e53bdf1f
[2:15:39.222159110] [3490]  INFO RPI pisp.cpp:720 libpisp version v1.2.1 981977ff21f3 29-04-2025 (14:13:50)
[2:15:39.234697593] [3490]  INFO RPI pisp.cpp:1179 Registered camera /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 to CFE device /dev/media2 and ISP device /dev/media0 using PiSP variant BCM2712_C0
[2:15:39.238282551] [3471]  INFO Camera camera.cpp:1205 configuring streams: (0) 640x480-XRGB8888/sRGB (1) 640x480-BGGR_PISP_COMP1/RAW
[2:15:39.238408699] [3490]  INFO RPI pisp.cpp:1483 Sensor: /base/axi/pcie@1000120000/rp1/i2c@88000/imx219@10 - Selected sensor format: 640x480-SBGGR10_1X10 - Selected CFE format: 640x480-PC1B
127.0.0.1 - - [09/Aug/2025 13:56:26] "GET /video_feed?t=1754711786091 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:26] "GET /video_feed?t=1754711786242 HTTP/1.1" 404 -
Camera initialized
[13:56:27] DEBUG: ✅ ChattyAI instance created successfully!
[13:56:27] DEBUG: System startup complete
127.0.0.1 - - [09/Aug/2025 13:56:28] "GET /video_feed?t=1754711788099 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:28] "GET /video_feed?t=1754711788249 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:30] "GET /video_feed?t=1754711790109 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:30] "GET /video_feed?t=1754711790258 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:32] "GET /video_feed?t=1754711792117 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:32] "GET /video_feed?t=1754711792267 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:34] "GET /video_feed?t=1754711794125 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:34] "GET /video_feed?t=1754711794275 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:36] "GET /video_feed?t=1754711796132 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:36] "GET /video_feed?t=1754711796283 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:38] "GET /video_feed?t=1754711798140 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:38] "GET /video_feed?t=1754711798290 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:40] "GET /video_feed?t=1754711800149 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:40] "GET /video_feed?t=1754711800297 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:42] "GET /video_feed?t=1754711802159 HTTP/1.1" 404 -
127.0.0.1 - - [09/Aug/2025 13:56:42] "GET /video_feed?t=1754711802307 HTTP/1.1" 404 -
^CException ignored in: <function Llama.__del__ at 0x7ffed1df23e0>
Traceback (most recent call last):
  File "/home/nickspi5/.local/lib/python3.11/site-packages/llama_cpp/llama.py", line 2209, in __del__
  File "/home/nickspi5/.local/lib/python3.11/site-packages/llama_cpp/llama.py", line 2206, in close
  File "/usr/lib/python3.11/contextlib.py", line 597, in close
  File "/usr/lib/python3.11/contextlib.py", line 589, in __exit__
  File "/usr/lib/python3.11/contextlib.py", line 574, in __exit__
  File "/usr/lib/python3.11/contextlib.py", line 348, in __exit__
  File "/home/nickspi5/.local/lib/python3.11/site-packages/llama_cpp/_internals.py", line 83, in close
  File "/usr/lib/python3.11/contextlib.py", line 597, in close
  File "/usr/lib/python3.11/contextlib.py", line 589, in __exit__
  File "/usr/lib/python3.11/contextlib.py", line 574, in __exit__
  File "/usr/lib/python3.11/contextlib.py", line 457, in _exit_wrapper
  File "/home/nickspi5/.local/lib/python3.11/site-packages/llama_cpp/_internals.py", line 72, in free_model
TypeError: 'NoneType' object is not callable
(chatty-venv) nickspi5@raspberrypi1:~/Chatty_AI $ 










