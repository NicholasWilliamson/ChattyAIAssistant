Thank you ChatGPT. You are great.

I ran nickspi5@raspberrypi1:~ $ sudo apt update
Hit:1 http://deb.debian.org/debian bookworm InRelease
Hit:2 http://deb.debian.org/debian-security bookworm-security InRelease
Get:3 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Hit:4 http://archive.raspberrypi.com/debian bookworm InRelease                                    
Fetched 55.4 kB in 1s (55.8 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
All packages are up to date.
nickspi5@raspberrypi1:~ $ sudo apt install -y cmake libabsl-dev
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
cmake is already the newest version (3.25.1-1).
The following packages were automatically installed and are no longer required:
  golang-1.19 golang-1.19-doc golang-1.19-go golang-1.19-src golang-src
Use 'sudo apt autoremove' to remove them.
The following NEW packages will be installed:
  libabsl-dev
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 955 kB of archives.
After this operation, 6,019 kB of additional disk space will be used.
Get:1 http://deb.debian.org/debian bookworm/main arm64 libabsl-dev arm64 20220623.1-1+deb12u2 [955 kB]
Fetched 955 kB in 2s (601 kB/s)      
Selecting previously unselected package libabsl-dev:arm64.
(Reading database ... 187454 files and directories currently installed.)
Preparing to unpack .../libabsl-dev_20220623.1-1+deb12u2_arm64.deb ...
Unpacking libabsl-dev:arm64 (20220623.1-1+deb12u2) ...
Setting up libabsl-dev:arm64 (20220623.1-1+deb12u2) ...
nickspi5@raspberrypi1:~ $ cd LocalAI
nickspi5@raspberrypi1:~/LocalAI $ make build
go mod edit -replace github.com/ggerganov/whisper.cpp=/home/nickspi5/LocalAI/sources/whisper.cpp
go mod edit -replace github.com/ggerganov/whisper.cpp/bindings/go=/home/nickspi5/LocalAI/sources/whisper.cpp/bindings/go
go mod edit -replace github.com/mudler/go-piper=/home/nickspi5/LocalAI/sources/go-piper
go mod download
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@1958fcbe2ca8bd93af633f11e97d44e567e945af
go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.34.2
go install github.com/GeertJohan/go.rice/rice@latest
mkdir -p pkg/grpc/proto
protoc --experimental_allow_proto3_optional -Ibackend/ --go_out=pkg/grpc/proto/ --go_opt=paths=source_relative --go-grpc_out=pkg/grpc/proto/ --go-grpc_opt=paths=source_relative \
    backend/backend.proto
mkdir -p backend-assets/grpc
I llama-cpp build info:avx
cp -rf backend/cpp/llama backend/cpp/llama-avx
make -C backend/cpp/llama-avx purge
make[1]: Entering directory '/home/nickspi5/LocalAI/backend/cpp/llama-avx'
rm -rf llama.cpp/build
rm -rf llama.cpp/tools/grpc-server
rm -rf grpc-server
make[1]: Leaving directory '/home/nickspi5/LocalAI/backend/cpp/llama-avx'
CMAKE_ARGS="-DBUILD_SHARED_LIBS=OFF -DGGML_NATIVE=OFF -DGGML_AVX=on -DGGML_AVX2=off -DGGML_AVX512=off -DGGML_FMA=off -DGGML_F16C=off" make VARIANT="llama-avx" build-llama-cpp-grpc-server
make[1]: Entering directory '/home/nickspi5/LocalAI'
echo "BUILD_GRPC_FOR_BACKEND_LLAMA is not defined."
BUILD_GRPC_FOR_BACKEND_LLAMA is not defined.
LLAMA_VERSION=d6fb3f6b49b27ef1c0f4cf5128e041f7e7dc03af make -C backend/cpp/llama-avx grpc-server
make[2]: Entering directory '/home/nickspi5/LocalAI/backend/cpp/llama-avx'
mkdir -p llama.cpp/tools/grpc-server
bash prepare.sh
Applying patch 01-llava.patch
patching file tools/mtmd/clip.cpp
patch unexpectedly ends in middle of line
Hunk #1 FAILED at 2608.
1 out of 1 hunk FAILED -- saving rejects to file tools/mtmd/clip.cpp.rej
'llama.cpp/vendor/nlohmann/json.hpp' -> 'llama.cpp/tools/grpc-server/json.hpp'
'llama.cpp/tools/server/utils.hpp' -> 'llama.cpp/tools/grpc-server/utils.hpp'
'llama.cpp/vendor/cpp-httplib/httplib.h' -> 'llama.cpp/tools/grpc-server/httplib.h'
grpc-server already added
Building grpc-server with  build type and -DBUILD_SHARED_LIBS=OFF -DGGML_NATIVE=OFF -DGGML_AVX=on -DGGML_AVX2=off -DGGML_AVX512=off -DGGML_FMA=off -DGGML_F16C=off -DGGML_NATIVE=OFF -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=OFF
cd llama.cpp && mkdir -p build && cd build && cmake .. -DBUILD_SHARED_LIBS=OFF -DGGML_NATIVE=OFF -DGGML_AVX=on -DGGML_AVX2=off -DGGML_AVX512=off -DGGML_FMA=off -DGGML_F16C=off -DGGML_NATIVE=OFF -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=OFF && cmake --build . --config Release --target grpc-server
-- The C compiler identification is GNU 12.2.0
-- The CXX compiler identification is GNU 12.2.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- GGML_SYSTEM_ARCH: ARM
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM feature FMA enabled
-- Adding CPU backend variant ggml-cpu:  
-- ggml version: 0.0.5923
-- ggml commit:  d6fb3f6b
CMake Error at tools/grpc-server/CMakeLists.txt:21 (find_package):
  Could not find a package configuration file provided by "Protobuf" with any
  of the following names:

    ProtobufConfig.cmake
    protobuf-config.cmake

  Add the installation prefix of "Protobuf" to CMAKE_PREFIX_PATH or set
  "Protobuf_DIR" to a directory containing one of the above files.  If
  "Protobuf" provides a separate development package or SDK, be sure it has
  been installed.


-- Configuring incomplete, errors occurred!
See also "/home/nickspi5/LocalAI/backend/cpp/llama-avx/llama.cpp/build/CMakeFiles/CMakeOutput.log".
See also "/home/nickspi5/LocalAI/backend/cpp/llama-avx/llama.cpp/build/CMakeFiles/CMakeError.log".
make[2]: *** [Makefile:81: grpc-server] Error 1
make[2]: Leaving directory '/home/nickspi5/LocalAI/backend/cpp/llama-avx'
make[1]: *** [Makefile:713: build-llama-cpp-grpc-server] Error 2
make[1]: Leaving directory '/home/nickspi5/LocalAI'
make: *** [Makefile:742: backend-assets/grpc/llama-cpp-avx] Error 2
nickspi5@raspberrypi1:~/LocalAI $ 


Is everything now correct?



