

Thank you, Claude,

I ran: nickspi5@raspberrypi:~ $ grep -A25 "def query_llama" /home/nickspi5/Chatty_AI/chatty_ai.py | head -30
    def query_llama(self, prompt):
        """Generate LLM response"""
        import time as t
        
        print(f"[TIMING] query_llama called with prompt: '{prompt[:50]}...'")
        start = t.perf_counter()
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            print(f"[TIMING] Starting LLM inference...")
            llm_start = t.perf_counter()
            
            result = self.llama_model(
                formatted_prompt, 
                max_tokens=100,
                stop=["\n", "User:", "Human:", "\n\n"]
            )
            
            llm_time = t.perf_counter() - llm_start
            print(f"[TIMING] LLM inference completed in {llm_time:.2f}s")
            
            if "choices" in result and result["choices"]:
                reply_text = result["choices"][0]["text"].strip()
                reply_text = re.sub(r"\(.*?\)", "", reply_text)
                reply_text = re.sub(r"(User:|Assistant:)", "", reply_text)
nickspi5@raspberrypi:~ $ grep -A20 "def query_llama" /home/nickspi5/Chatty_AI/chatty_ai.py | head -25
    def query_llama(self, prompt):
        """Generate LLM response"""
        import time as t
        
        print(f"[TIMING] query_llama called with prompt: '{prompt[:50]}...'")
        start = t.perf_counter()
        
        formatted_prompt = f"You are a friendly, helpful assistant. Give a brief, conversational answer.\nUser: {prompt}\nAssistant: "
        
        try:
            print(f"[TIMING] Starting LLM inference...")
            llm_start = t.perf_counter()
            
            result = self.llama_model(
                formatted_prompt, 
                max_tokens=100,
                stop=["\n", "User:", "Human:", "\n\n"]
            )
            
            llm_time = t.perf_counter() - llm_start
            print(f"[TIMING] LLM inference completed in {llm_time:.2f}s")
nickspi5@raspberrypi:~ $ journalctl -u chatty-ai.service --since "30 minutes ago" --no-pager | grep -i "timing\|llm\|generat\|process_user\|inference"
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] process_user_input called with: 'What is the longest river in the world?'
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] Calling LLM...
Dec 26 17:28:16 raspberrypi python[1101]: [05:28:16 PM] Generating LLM response
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] query_llama called with prompt: 'What is the longest river in the world?...'
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] Starting LLM inference...
Dec 26 17:28:22 raspberrypi python[1101]: [TIMING] LLM inference completed in 6.43s
Dec 26 17:28:22 raspberrypi python[1101]: [TIMING] query_llama total time: 6.43s, response: '78,408 kilometers! The River Nile, which flows thr...'
Dec 26 17:28:22 raspberrypi python[1101]: [TIMING] process_user_input total time: 6.43s
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] process_user_input called with: 'What is the deepest ocean in the world?'
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] Calling LLM...
Dec 26 17:29:09 raspberrypi python[1101]: [05:29:09 PM] Generating LLM response
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] query_llama called with prompt: 'What is the deepest ocean in the world?...'
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] Starting LLM inference...
Dec 26 17:29:12 raspberrypi python[1101]: [TIMING] LLM inference completed in 3.33s
Dec 26 17:29:12 raspberrypi python[1101]: [TIMING] query_llama total time: 3.33s, response: '12,646 feet deep....'
Dec 26 17:29:12 raspberrypi python[1101]: [TIMING] process_user_input total time: 3.33s
nickspi5@raspberrypi:~ $ journalctl -u chatty-ai.service --since "50 minutes ago" --no-pager | grep -i "timing\|llm\|generat\|process_user\|inference"
Dec 26 17:27:39 raspberrypi python[1101]: [TIMING] process_user_input called with: 'What is the longest road in the world?'
Dec 26 17:27:39 raspberrypi python[1101]: [TIMING] Calling LLM...
Dec 26 17:27:39 raspberrypi python[1101]: [05:27:39 PM] Generating LLM response
Dec 26 17:27:39 raspberrypi python[1101]: [TIMING] query_llama called with prompt: 'What is the longest road in the world?...'
Dec 26 17:27:39 raspberrypi python[1101]: [TIMING] Starting LLM inference...
Dec 26 17:27:46 raspberrypi python[1101]: [TIMING] LLM inference completed in 6.93s
Dec 26 17:27:46 raspberrypi python[1101]: [TIMING] query_llama total time: 6.93s, response: '6,777 kilometers , which is the length of the Grea...'
Dec 26 17:27:46 raspberrypi python[1101]: [TIMING] process_user_input total time: 6.93s
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] process_user_input called with: 'What is the longest river in the world?'
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] Calling LLM...
Dec 26 17:28:16 raspberrypi python[1101]: [05:28:16 PM] Generating LLM response
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] query_llama called with prompt: 'What is the longest river in the world?...'
Dec 26 17:28:16 raspberrypi python[1101]: [TIMING] Starting LLM inference...
Dec 26 17:28:22 raspberrypi python[1101]: [TIMING] LLM inference completed in 6.43s
Dec 26 17:28:22 raspberrypi python[1101]: [TIMING] query_llama total time: 6.43s, response: '78,408 kilometers! The River Nile, which flows thr...'
Dec 26 17:28:22 raspberrypi python[1101]: [TIMING] process_user_input total time: 6.43s
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] process_user_input called with: 'What is the deepest ocean in the world?'
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] Calling LLM...
Dec 26 17:29:09 raspberrypi python[1101]: [05:29:09 PM] Generating LLM response
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] query_llama called with prompt: 'What is the deepest ocean in the world?...'
Dec 26 17:29:09 raspberrypi python[1101]: [TIMING] Starting LLM inference...
Dec 26 17:29:12 raspberrypi python[1101]: [TIMING] LLM inference completed in 3.33s
Dec 26 17:29:12 raspberrypi python[1101]: [TIMING] query_llama total time: 3.33s, response: '12,646 feet deep....'
Dec 26 17:29:12 raspberrypi python[1101]: [TIMING] process_user_input total time: 3.33s
nickspi5@raspberrypi:~ $ 












